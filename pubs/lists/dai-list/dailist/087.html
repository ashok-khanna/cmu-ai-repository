DAI-List Digest         Friday, 14 August 1992         Issue Number 87

Topics:
  Re: DAI Olympics
  Re: DAI Event at AI Olympics
  DAI Competition

Please send submissions to DAI-List@mcc.com.  Send other requests,
such as changes in your e-mail address, to DAI-List-Request@mcc.com.
------------------------------------------------------------------------

Date: Fri, 14 Aug 92 13:59:42 -0400
From: "Van D. Parunak" <van@iti.org>
Subject: Re: DAI Olympics

       A Multiagent Playoff Proposal in Manufacturing Control
                            Van Parunak

Mike Huhns' recent call for suggestions on "The Cooperation Competition"
has already elicited a number of exciting and promising proposals that
would let individual submitted agents demonstrate their coordination
capabilities in interactions with agents they've not previously met, in
relatively schematic domains with a fixed interagent architecture.
Perhaps in addition to such a competition (or as a longer-range goal for
the second year), I suggest a scenario that differs in two ways from
those that have already been described.

	1. Each competitor submits a system, not just an isolated agent,
where the system accomplishes the entire task.  Typically, a system
would be a team of agents.  The competition thus involves comparing the
performance of different systems' architectures rather than just the
performance of individual agents' algorithms.

	2. The application domain is rich rather than schematic, and of
direct commercial interest, thus helping to attract the interest of
industrial sponsors to the important ideas that our field is developing,
and in turn focusing researchers' attention on some urgent application
issues.

In particular, I propose that the competition testbed be a detailed
simulation of a discrete manufacturing facility, populated by a variety
of entities such as machining centers, turning centers, stamping
presses, assembly robots, paint booths, and material transport devices
of various sorts.  There is considerable interest in both industry and
DoD on "agile production systems:" general-purpose equipment and control
architectures that permit rapid, autonomous reconfiguration of the
equipment to produce new products.

Entries compete on the basis of how well they can run this simulated
facility; a test consists of defining the available manufacturing
entities and their individual characteristics, and then submitting
successive process plans for different products that can be manufactured
with those entities, together with the number of copies of each product
that are required.  In keeping with the spirit of "agile production,"
competitors do not know at the outset what products will be required or
when the requests will arrive.  The competitor is responsible for
assigning the tasks in the process plan to various entities in the
facility and controlling their activity in simulated real time.  Entries
are ranked on the basis of the cost and time consumed by the facility to
produce both the first part and the complete batch of each new design.
Depending on the execution environment, we could also rank them on the
communication traffic and machine cycles they consume in performing this
task.

To keep matters simple for the initial run of such a competition, the
domain knowledge required can be kept to a minimum by defining a process
plan as a partially ordered list of operations, and by associating with
each machine a list of the operations it can perform (with associated
costs and time delays).  The domain knowledge relevant to the task is
largely embedded in the parameters of the operations.  For example,
operations would include such items as "relocate machine" that would
permit a competing system to move the machines around on the factory
floor (at appropriate cost, of course) so as to form cells with high
internal coordination and minimal transport costs.  Operations would
also be parameterized to recognize that the first instance of an
operation incurs setup and retooling costs that are not repeated if the
same operation recurs without intervention of other operations.  Thus
competitors do not need to understand the intricacies of machining
centers or stamping presses, but simply need to locate machines that can
perform the operations required by the specified process plans, and
schedule these resources in simulated real time to optimize cost and
time. (Naturally, if a sizable portion of the community finds this
class of problem interesting, we can appeal to even greater commercial
interest in later versions of the test by increasing the amount of
domain knowledge involved in the reasoning.)

The environment for the testbed already exists at ITI (XFaST(tm), the
eXecutable Factory Simulation Tool), and has been used commercially to
design and simulate real manufacturing facilities in the automotive
industry.  Although the competition would be in a simulated world, the
testbed actually permits the simulators to be unhooked from the control
interface and replaced by actual machinery, so that transitioning
promising candidates into test applications in the physical world would
be straightforward.  Under simulation, the testbed provides a graphical
display of the facility, thus providing visual attractiveness to walk-by
observers.

In supporting the competition, the testbed
	
--defines for each such entity a number of basic parameters, such as
mean time to failure; mean time to repair; current loading; available
operations with time and cost for each; and current tooling and other
set-up information;

--provides a low-level control interface to individual entities,
permitting the systems under test to read entity status and issue
operation commands;

--simulates the actual performance of the system, including kinematic
behavior of moving entities and failure of system components;

--maintains the system clock and gathers performance statistics, such as
time and cost to produce first copy and successive copies of each
designated product.

Competitors can map computational agents onto manufacturing entities in
any way they wish, ranging from a single agent that controls the whole
system at one extreme, to a separate agent for each piece of equipment
(and perhaps some "management" agents not assigned to machines) at the
other.  Competitors can run on any hardware they like, interfacing to
the testbed via RS-232.  The only requirement is that the testbed own
the simulated time clock.  If the problem is further constrained to
require competitor systems to run on a standard platform (say, UM's
MICE), that platform can also collect statistics on computational time
and communication traffic among computational agents, providing further
bases for comparing the performance of different systems.

The set-up for such a competition is not trivial, but not overwhelming
either.  The basic components of such a testbed already exist
(XFaST(tm), MICE), and the commercial and government interest in agile
manufacturing might make it possible to obtain support for the
development of a suite of scenarios.  The potential benefits are
considerable.  Not only does this approach let us compare different
system architectures against a common problem, but the results are of
direct commercial interest, an important consideration in a period when
commercial competitiveness is assuming the stature that defense once had
as a motivation for government research funding.

This proposal has the advantage of industrial relevance.  A
corresponding disadvantage is that it is an instance of distributed
problem solving, not full MAS.  A candidate solution need not use
COOPERATIVE DPS; I am personally very interested in solutions to this
problem in which each agent follows a local utility-maximizing approach
and the overall system behavior is emergent. Still, the system
_designer_ has an overall system goal, so this sort of competition does
not exercise some aspects of MAS behavior that would be valuable (for
instance) in a mutual pursuit game.  I'd be delighted if readers can
suggest scenarios that combine these two objectives:  clear relevance to
the commercial community, and openness to the full scope of MAS
techniques.

If there's interest in pursuing such a competition, I'd be happy to
explore funding for the set-up and to coordinate it.

Van Parunak

Dr. H. Van Dyke Parunak            internet: van@iti.org
Scientific Fellow                  voice: (313) 769-4049
Industrial Technology Institute    fax:   (313) 769-4064
PO Box 1485, Ann Arbor, MI 48106
------------------------------------------------------------------------

Date: Thu, 13 Aug 92 14:20:45 EDT
From: chopin!orpheus!gray@uunet.uu.net ( Michael Gray)
Subject: Re:  DAI Event at AI Olympics

I'll participate, too.

I suggest we keep the rules of the first event minimal to see how it
goes.  The first event will give us a lot of guidance on how to run
these events.

Michael Gray
CSIS Dept.
American University
Washington, DC
------------------------------------------------------------------------

Date: Fri, 14 Aug 92 14:13:18 -0400
From: Ed Durfee <durfee@engin.umich.edu>
Subject: DAI Competition

    More on a DAI Tournament and the Mutual Pursuit Task

As has been pointed out by the participants in the dialogue on a
cooperation competition, there are many decisions that need to be made
in coming up with a competition depending on what factors we want to
test for in that competition.  Off the top of my head, here are some
observations on some factors.

Most of the responses, so far, have seemed to emphasize cooperation in
the traditional CDPS (common goal) form.  That is, the agents are thrown
together to accomplish a task that they cannot do without each other's
help (move furniture, design a house), or where they share an overall
goal (such as avoiding collisions in ATC), or where they have identical
goals that can easily be seen to be accomplished faster by pursuing
subproblems in parallel (planetary exploration, maze exploration).  Is
the ability to cooperate given some overarching goal what we want to
explore in the competition, or do we want to include in the competition
some degree of allowing self-interested agents to "discover" benefits of
cooperation?

By "throwing together" agents designed by different researchers, the
hope is that, although the separately designed agents will have some
common (built-in) expectations about the task and how to interact with
others, cooperation could emerge and evolve in different ways.  The
challenge here is that providing too little commonality for
communication protocols and languages might doom agents to perpetual
misunderstanding, while providing too much could eliminate
"individuality" from agent encounters.  Alternatively, if we assume that
the agents have common abilities to sense and act in their environment,
then they can communicate through the environment by observing each
other.  This eliminates the need for defining languages and protocols
for explicit message passing, but can place more of a burden on issues
of plan/goal recognition.

The above assumed that each "contestant" in the competition would
contribute an "individual" who would coexist in the environment with one
or more other individuals.  An alternative is to have each contestant
supply a "team" to the competition.  Because members of the team are
designed by the same contestant, each contestant can make his or her own
decisions as to how rich the communication language/protocol is, how
agents represent individual and collective goals, etc.  Of course, some
choices might be penalized by the environment; for example, depending on
large amounts of communication might backfire if the environment has
slow and unreliable communication channels.

If each contestant supplies a team, then one way of evaluating
performance is to have each team work on performing the task in
isolation.  But this kind of tournament probably wouldn't be very
exciting.  Alternatively, we could populate the same environment with
collections of teams.  If the teams need to cooperate to accomplish the
task, then we face the same issues as before.  Alternatively, the teams
could be somewhat competitive, leading to a REAL competition in the
tournament.

Anyway, it is this line of reasoning that led me to use a mutual pursuit
task for the tournament in my distributed AI course.  For more details
on this task, I'm attaching the assignment writeup to this message.  The
points I want to make about it are:

    - Each contestant submitted a team, and thus could make coordination
   within the team as sophisticated as desired.

    - The environment "charged" agents for time spent thinking and
   talking (computing and communicating), so being too smart could
   backfire as the dumb agents of the other team capture your agents
   while they are standing still, thinking and talking.  Similarly,
   having a chain of command could be costly due to communication
   delays.

    - Perception is limited, so it is seldom that an agent has a global
   view of the current situation.  As a result, depending on making
   identical inferences based on the state of the world is risky.

    - The competition is NOT zero-sum.  That is, in a fair exchange,
   both teams benefit.  This gives the game a "Prisoner's Dilemma"
   flavor because, if two teams could infer a "cooperative" strategy on
   each other's parts, they could orchestrate interactions to "feed"
   each other to their mutual benefit.

    - In short, the environment promotes both more traditional CDPS
   within each team, and MA interactions between teams.

I'm not saying that this is necessarily the right balance of issues to
examine for the cooperation competition, but it is one possible balance.
Mostly, in this message, I wanted to share some thoughts on
characteristics of alternative tasks, on some challenges in designing a
"tournament," and on an example tournament that was successfully run as
part of a class, using portable software (MICE), where "success" was
measured as participants having fun and learning something about
coordination in the process.  (Just like the AAAI robot competition was
fun and provided only some qualitative feedback with respect to
"correct" ways to design mobile robots.)

The drawbacks of mutual pursuit is that it is a bit of a stretch
(although not impossible) to explain why it is relevant to commercial
interests (less of a stretch for military interests :).  To appeal to
the commercial side, perhaps a task like Van's would be more suitable,
even though, from a research perspective, it only exercises the CDPS
side of DAI...

Comments, suggestions, etc are welcome!
                                                - Ed

PS: As I said, mutual pursuit has been done in MICE.  I'd be happy to
help people who have tried MICE to try out this task...
=======================
\documentstyle{article}    
\begin{document}                               
\begin{center}
\Large{Assignment 5: Mutual Pursuit \\
      Due: Monday, December 16, 1991}\\
\bigskip
You may work in teams of 2 for this assignment.
\end{center}

\section{Overview}

The purpose of this assignment is to give you a chance to sift through
the DAI ideas and techniques you have seen in the course and to decide
which of them (one or more) are most appropriate for a particular
problem.  The structure of this assignment is as follows.  The
assignment defines a multiagent MICE environment that extends the
pursuit problem (previously explored).  Specifically, the new
environment is comprised of two types of agents, where each type preys
upon and is preyed upon by the other.  Your goal in the assignment is to
design a type of agent that can perform well in this environment.  At
the outset, I should make clear that the most important part of this
assignment is your {\it design\/}, meaning your description of the
techniques you would employ in an agent and why those are the best
choices.  You are also expected to implement your design (or a subset of
it), and your team of agents will compete against other teams in a
tournament.  While success in the tournament will count for a little
something (both in your grade and your pride), the bulk of the grade on
this assignment will rest on the quality of your design based on your
assessment of the task and of the strengths of the available ideas.

\section{The Mutual Pursuit Task}

In the traditional pursuit task, four agents of one type (predators)
surround an agent of a second type (prey) in order to capture it.  While
the mutual pursuit task still involves two types of agents, the types
are no longer strictly either predators or prey.  Instead, four agents
of either type can capture an agent of the other type by surrounding it.
The agent types are thus symmetric.

In this assignment, we are considering a mutual pursuit task involving
five agents of each type.  Five (rather than four) was chosen for
several reasons, including
\begin{itemize}
\item having an extra agent allows more sophisticated strategies, such
as using a decoy;
\item having an extra agent allows redundancy in a team in case of
the failure (capture) of one of its members.
\end{itemize}

The five agents of the same type comprise a team.  Each of the agents in
a team has a unique authority, ranging from 1 through 5 (corresponding
to the agent name), and higher authority agents can physically displace
lower authority agents (use the authority-collision-function).  Thus, an
agent cannot coexist in a location with another agent (of either team),
and two agents cannot move through each other.  Each agent has a limited
sensory range (10 by 10), can see all types of agents, and requires 0
time to scan.  Each agent requires one time unit to move in any
direction.  If agents on a team communicate, communication delay is also
one time unit.

Finally, in this set of experiments, time spent thinking {\it does\/}
translate into simulated time.  MICE has built in functions for doing
this.  Simply put, the variable {\tt *real-time-knob*} contains a real
number that gets multiplied by the number of seconds (real time) taken
between when an agent's invocation function was called and when it
returns.  The product is rounded off to the nearest integer, and the
agent is charged that much simulated time for reasoning before the
commands generated by the agent are carried out by MICE.  For these
experiments, the {\tt *real-time-knob*} is set to a fairly forgiving
value of 0.1, so an agent is not charged for reasoning if it takes less
than 5 seconds to send commands to MICE, it is charged 1 time unit for
reasoning if it takes 5 or more seconds but less than 15, and so on.
Thus, if being smart takes time, it can backfire.  Also, note that
elapsed time can be nondeterministic because of garbage collecting, etc.

When a team of agents captures a member of the other team, the capturing
team gains points equal to twice the strength of the captured agent,
while the team that loses the agent loses points equal to just the
strength of the captured agent.  Moreover, when an agent is captured, in
this environment, we assume it is replaced by another identical agent
placed randomly in the environment (in actuality, this is accomplished
by simply having MICE move the same agent somewhere else).  Because a
team always has five agents, having two or more agents captured is not
fatal to the team.  Furthermore, because the game is not zero sum (gain
to capturing team is twice loss to other team), two teams that capture
roughly equal numbers of agents on the other team will {\it both\/}
benefit, relative to running into different corners and never
interacting.  Thus, it is to each team's advantage to get in there and
slug it out.

\section{Agent Implementation}

Agent implementation is entirely up to you.  You can make your agents as
aggressive or passive, as projective or reactive, as talkative or quiet,
as you want.  What is most important is that you are able to justify why
you made the design and implementation choices that you did.  For the
purpose of avoiding collisions in code, however, you should preface all
of your functions with a unique sequence of characters, such as your
initials.

\section{Evaluation}

As mentioned above, there are 2 parts to the evaluation.  One is how
well you have designed your agents and justified your design.  This
should be part of a writeup that you submit.

The other is how well your team of agents performs relative to the other
teams.  This will be judged in a tournament, where the class will get
together.  Currently, the model is to use the Macintosh version of MICE,
so that we can use an overhead projector to show everyone what is going
on without crowding around a screen.  The structure of the tournament is
as follows:

Each team of agents will compete in an environment with each other team.
An experiment will run for the usual 200 time units.  At the end of an
experiment, the points collected by each team will be recorded.  After
every pairwise combination of teams has been tested (depending on the
number of teams, we might run multiple experiments for each combination
to get a slightly more statistical valuation), the points for each team
will be totaled and the teams will be ranked according to how many
points they get.

If there is time (and interest) we can also use the relative point
totals to try an evolutionary approach, where more successful teams are
more numerous in future generations.  If we go this route, note that a
team that does well playing against itself is likely to dominate!

In summary, the serious part of the evaluation is based on what you turn
in.  The tournament is for fun!
\end{document}


