 
From mcovingt@aisun3.ai.uga.edu  Fri Dec 18 14:54:16 1992
Return-Path: <mcovingt@aisun3.ai.uga.edu>
Received: from server.uga.edu by aisun1.ai.uga.edu (4.1)
	id AA06823; Fri, 18 Dec 92 14:54:16 EST
Received: by server.uga.edu (5.57/Ultrix3.0-C)
	id AA19210; Fri, 18 Dec 92 14:54:10 -0500
Received: by aisun3.ai.uga.edu (4.1)
	id AA12073; Fri, 18 Dec 92 14:54:09 EST
From: mcovingt@aisun3.ai.uga.edu (Michael Covington)
Message-Id: <9212181954.AA12073@aisun3.ai.uga.edu>
Subject: Test message - Please ignore
To: dg@ai.uga.edu
Date: Fri, 18 Dec 92 14:54:08 EST
X-Mailer: ELM [version 2.3 PL11]

This is a test mailing to just a few of the participants, plus a number of
known invalid addresses.  If no problems arise, DG will be opened for use soon.
------------------------------------------------------------------------
From mcovingt@aisun3.ai.uga.edu  Fri Dec 18 14:59:16 1992
Return-Path: <mcovingt@aisun3.ai.uga.edu>
Received: from server.uga.edu by aisun1.ai.uga.edu (4.1)
	id AA06880; Fri, 18 Dec 92 14:59:16 EST
Received: by server.uga.edu (5.57/Ultrix3.0-C)
	id AA19233; Fri, 18 Dec 92 14:59:15 -0500
Received: by aisun3.ai.uga.edu (4.1)
	id AA12137; Fri, 18 Dec 92 14:59:14 EST
From: mcovingt@aisun3.ai.uga.edu (Michael Covington)
Message-Id: <9212181959.AA12137@aisun3.ai.uga.edu>
Subject: Second test message - Please ignore
To: dg@ai.uga.edu
Date: Fri, 18 Dec 92 14:59:13 EST
X-Mailer: ELM [version 2.3 PL11]

This is the second test message.  DG is not yet open for business.
-- 
:-  Michael A. Covington     internet mcovingt@uga.cc.uga.edu :   Verbum caro
:-  Artificial Intelligence Programs       phone 706 542-0358 :    factum est
:-  The University of Georgia                fax 706 542-0349 :  et habitavit
:-  Athens, Georgia 30602-7415 U.S.A.     amateur radio N4TMI :   in nobis...
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
Log available by anon. FTP from ai.uga.edu, directory /ai.natural.language.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From mcovingt@aisun3.ai.uga.edu  Fri Dec 18 22:17:12 1992
Return-Path: <mcovingt@aisun3.ai.uga.edu>
Received: from server.uga.edu by aisun1.ai.uga.edu (4.1)
	id AA08592; Fri, 18 Dec 92 22:17:12 EST
Received: by server.uga.edu (5.57/Ultrix3.0-C)
	id AA20075; Fri, 18 Dec 92 22:17:11 -0500
Received: by aisun3.ai.uga.edu (4.1)
	id AA12857; Fri, 18 Dec 92 22:17:10 EST
From: mcovingt@aisun3.ai.uga.edu (Michael Covington)
Message-Id: <9212190317.AA12857@aisun3.ai.uga.edu>
Subject: Test mailing - Please ignore
To: dg@ai.uga.edu
Date: Fri, 18 Dec 92 22:17:09 EST
X-Mailer: ELM [version 2.3 PL11]

This is a test of the dependency grammar mailing list.
-- 
:-  Michael A. Covington     internet mcovingt@uga.cc.uga.edu :  

---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
Log available by anon. FTP from ai.uga.edu, directory /ai.natural.language.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From mcovingt@aisun1.ai.uga.edu Sun Dec 20 22:47:33 1992
Received: from aisun1.ai.uga.edu by aisun3.ai.uga.edu (4.1)
	id AA14659; Sun, 20 Dec 92 22:47:32 EST
Received: by aisun1.ai.uga.edu (4.1)
	id AA12411; Sun, 20 Dec 92 22:47:31 EST
Return-Path: <mcovingt>
Received: by aisun1.ai.uga.edu (4.1)
	id AA12407; Sun, 20 Dec 92 22:47:29 EST
From: mcovingt@aisun1.ai.uga.edu (Michael Covington)
Message-Id: <9212210347.AA12407@aisun1.ai.uga.edu>
Subject: ** New dependency grammar mailing list **
To: dg@ai.uga.edu
Date: Sun, 20 Dec 92 22:47:28 EST
X-Mailer: ELM [version 2.3 PL11]
Status: OR

A new mailing list has been set up for discussion of dependency grammar
and related issues.  Basic information follows.  Let me know if I can
assist you in any way.
                                  - Michael Covington, mcovingt@ai.uga.edu

(If you received this message directly, you are already a subscriber.)

------------------------------------------------------------------------
-------------------------------------------------------------------------
Basic information about mailing list DG
-------------------------------------------------------------------------
The mailing list "DG" is for discussion of dependency grammar and related
matters.  It was set up by Michael Covington at the suggestion of Richard
Hudson.

DG is unmoderated.  Any message mailed to DG@AI.UGA.EDU will automatically
be distributed to all participants. 

When a message arrives from DG, you have two choices:
  - You can reply directly to the sender (that's what happens if you
hit "reply" or "r" in most mail-reading programs);
  - You can mail a reply to DG@AI.UGA.EDU to have it distributed to all
participants.  (This is usually preferable.)
  (Every mailing from DG ends with a footnote summarizing these two
options.)

To subscribe to DG, or for other assistance, email MCOVINGT@AI.UGA.EDU.

Copies of all mailings sent through DG are available by anonymous FTP
from AI.UGA.EDU, directory /ai.natural.language, file dg.log.
-------------------------------------------------------------------------
List of subscribers as of December 20, 1992

Michael Covington <mcovingt@aisun3.ai.uga.edu>
Richard Hudson <uclyrah@ucl.ac.uk>
sag@csli.stanford.edu
guy@cogsci.ed.ac.uk
laurie.bauer@vuw.ac.nz
norman@logcam.co.uk
hajicova@cspguk11.bitnet
phopkins@sol.uvic.ca
uclyrah@ucl.ac.uk
rcj@caen.engin.umich.edu
joshi@linc.cis.upenn.edu
duda@zisw.wtza-berlin.dbp.de
"Mark Liberman" <myl@unagi.cis.upenn.edu>
lobin@lili1.uni-bielefeld.de
jmiller@cogsci.ed.ac.uk
davidm@cogsci.edinburgh.ac.uk
"bneving@bbn.com" <bnevin@bbn.com>
kpuba@hujivm1.bitnet
panevova@cspguk11.bitnet
pollard@ling.ohio-state.edu
rambow@unagi.cis.upenn.edu
rohrer@adler.ims.uni-stuttgart.de
A.Rosta@ucl.ac.uk
sgall@cspguk11.bitnet
hls@ccl.umist.ac.uk
t042270@uhccmvs.bitnet
voutilainen@cc.helsinki.fi
Peter Hellwig <c87@dhdurz2.bitnet>
manyman@cc.helsinki.fi
fddeane@ucf1vm.cc.ucf.edu
uclynsg@ucl.ac.uk
"Michael C. McCord (Phone 914-784-7808)" <mccord@watson.ibm.com>
"Dan Maxwell" <maxwell@ltb.bso.nl>
"Richard Sharman, IBM UK" <rsharman@vnet.ibm.com>
"Henning Lobin" <lobin@lili1.uni-bielefeld.de>
"Ted Briscoe" <Ted.Briscoe@computer-lab.cambridge.ac.uk>

---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From mcovingt@aisun1.ai.uga.edu Sun Dec 20 22:51:44 1992
Received: from aisun1.ai.uga.edu by aisun3.ai.uga.edu (4.1)
	id AA14665; Sun, 20 Dec 92 22:51:43 EST
Received: by aisun1.ai.uga.edu (4.1)
	id AA12426; Sun, 20 Dec 92 22:51:41 EST
Return-Path: <mcovingt>
Received: by aisun1.ai.uga.edu (4.1)
	id AA12422; Sun, 20 Dec 92 22:51:40 EST
From: mcovingt@aisun1.ai.uga.edu (Michael Covington)
Message-Id: <9212210351.AA12422@aisun1.ai.uga.edu>
Subject: Dependency grammar - Some questions
To: dg@ai.uga.edu
Date: Sun, 20 Dec 92 22:51:40 EST
X-Mailer: ELM [version 2.3 PL11]
Status: OR

Here are some topics that may help us get the discussion started...

(1) What, if anything, does dependency grammar (DG) have to do with Chomsky's
new "minimalist" theory?

(2) In recent GB theory there seems to be a determination to define all
grammatical relations in terms of tree structure.  To what extent has this
led to the positing of tree structure that is otherwise unmotivated?
(Insofar as this has happened, it's an argument that dependency grammar
-- which defines the grammatical relations directly -- would be simpler.)

(3) Bechraoui has pointed out that there are really two distinctions between
constituency and dependency theories:
 (a) Do you use dependency arcs or constituency trees to represent structure?
 (b) Do you take structure as basic or relations and functions as basic?

(4) What is the dependency analog of ID/LP rules?

(5) What dependency parsing algorithms are there?  (I know that Norman Fraser
is doing research on this now.)

Looking forward to hearing from you...
-- 
:-  Michael A. Covington     internet mcovingt@uga.cc.uga.edu :   Verbum caro
:-  Artificial Intelligence Programs       phone 706 542-0358 :    factum est
:-  The University of Georgia                fax 706 542-0349 :  et habitavit
:-  Athens, Georgia 30602-7415 U.S.A.     amateur radio N4TMI :   in nobis...
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From mcovingt@aisun3.ai.uga.edu  Sun Dec 20 23:49:00 1992
Return-Path: <mcovingt@aisun3.ai.uga.edu>
Received: from server.uga.edu by aisun1.ai.uga.edu (4.1)
	id AA12614; Sun, 20 Dec 92 23:49:00 EST
Received: by server.uga.edu (5.57/Ultrix3.0-C)
	id AA23110; Sun, 20 Dec 92 23:48:59 -0500
Received: by aisun3.ai.uga.edu (4.1)
	id AA14792; Sun, 20 Dec 92 23:48:58 EST
From: mcovingt@aisun3.ai.uga.edu (Michael Covington)
Message-Id: <9212210448.AA14792@aisun3.ai.uga.edu>
Subject: Brief scheduled outage
To: dg@ai.uga.edu
Date: Sun, 20 Dec 92 23:48:57 EST
X-Mailer: ELM [version 2.3 PL11]

The machine that services the DG mailing list will be down for about
8 hours during the evening/night of December 21-22.
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------








From @mail-a.bcc.ac.uk:uclyrah@ucl.ac.uk  Mon Dec 21 04:17:50 1992
Return-Path: <@mail-a.bcc.ac.uk:uclyrah@ucl.ac.uk>
Received: from sun2.nsfnet-relay.ac.uk by aisun1.ai.uga.edu (4.1)
	id AA12940; Mon, 21 Dec 92 04:17:50 EST
Via: uk.ac.bcc.mail-a; Mon, 21 Dec 1992 09:17:32 +0000
Received: from link-1.ts.bcc.ac.uk by mail-a.bcc.ac.uk with SMTP (PP) 
          id <00558-0@mail-a.bcc.ac.uk>; Mon, 21 Dec 1992 09:13:17 +0000
Received: by link-1.ts.bcc.ac.uk (AIX 3.2/UCB 5.64/4.03) id AA28708;
          Mon, 21 Dec 1992 09:13:15 GMT
Message-Id: <9212210913.AA28708@link-1.ts.bcc.ac.uk>
To: dg@ai.uga.edu
Subject: DG and GB
Date: Mon, 21 Dec 92 09:13:15 +0000
From: "R.HudsonIBM-850" <uclyrah@ucl.ac.uk>


21 December 1992

Can I pick up on Michael Covington's second question, about the
possible redundancy of tree structure in GB. I've been struck recently
by the similarities between GB `government' and DG `dependency'.
(Notice that GB government is NOT the same as DG government, because A
can GB-govern B without having any effect on B's inflectional
category, i.e. without DG-governing it.) Roughly speaking, if A
depends on B then B GB-governs A. 

The most important difference between GB-government and dependency
seems to me to involve adjuncts/modifiers, which aren't GB-governed
but do depend on other words. Another difference stems from the role
of phrases in GB, so if A depends on B, then B GB-governs not only B
but also the phrase of which it's head. A third difference is that if
A GB-governs B, then A also GB-governs B's specifier. (This is a very
peculiar claim, in my opinion, which seems to create at least as many
problems as it solves.) Any of these differences between GB and DG
deserves some attention as a possible way of distinguishing between GB
and DG, but they shouldn't obscure the basic similarity between GB-
government and dependency. Since GB-government is an innovation of the
80's in GB, it is clearly a major shift in the direction of DG.

Now if you push behind the notation, one consequence of the
recognition of GB-government is that GB already *generates* something
remarkably similar to a dependency structure along with the various X-
bar structures, a `government-structure'. Admittedly this structure is
derived from the constituent structure, but it's formally available
thanks to the definition of GB-government. And more importantly, it's
referred to directly in a lot of principles of the theory, and seems
to be playing an increasingly important part as the theory develops.

Is it just a matter of time before someone realises that it would be
possible to reverse the relations between government-structure and
constituent-structure? Then the government-structures will be
generated directly (e.g. as projections of lexical valencies), and
constituent structures will be derivable from them. When that point is
reached, GB will in effect have become (a very sophisticated version
of) DG.


Dick Hudson
Dept of Phonetics and Linguistics,
University College London,
Gower Street,
London WC1E 6BT
(071) 387 7050 ext 3152

---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From @uga.cc.uga.edu:PANEVOVA@CSPGUK11.BITNET  Mon Dec 21 08:32:04 1992
Return-Path: <@uga.cc.uga.edu:PANEVOVA@CSPGUK11.BITNET>
Received: from uga.cc.uga.edu by aisun1.ai.uga.edu (4.1)
	id AA13162; Mon, 21 Dec 92 08:32:04 EST
Message-Id: <9212211332.AA13162@aisun1.ai.uga.edu>
Received: from UGA.CC.UGA.EDU by uga.cc.uga.edu (IBM VM SMTP V2R2)
   with BSMTP id 1320; Mon, 21 Dec 92 08:31:37 EST
Received: from CSPGUK11.BITNET by UGA.CC.UGA.EDU (Mailer R2.08 PTF008) with
 BSMTP id 9591; Mon, 21 Dec 92 08:31:37 EST
Received: from CSPGUK11 (PANEVOVA) by CSPGUK11.BITNET (Mailer R2.07) with BSMTP
 id 9020; Mon, 21 Dec 92 12:26:23 CET
Date:         Mon, 21 Dec 92 12:22:58 CET
From: PANEVOVA%CSPGUK11.BITNET@uga.cc.uga.edu
To: dg@ai.uga.edu

The dependency mailing list caused great troubles in our e-mail disks.
Now it seems to be O.K.(after clearing of our disks during last week).
I hope now the connection will function well.
Marry Christmas  Jarmila Panevova
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From ltb!maxwell@relay.nluug.nl  Tue Dec 22 09:36:01 1992
Received: from sun4nl.nluug.nl by aisun1.ai.uga.edu (4.1)
	id AA00877; Tue, 22 Dec 92 09:36:01 EST
Received: from ltb by sun4nl.nluug.nl via EUnet
	id AA17205 (5.65b/CWI-3.3); Tue, 22 Dec 1992 15:35:56 +0100
Return-Path: <ltb!maxwell>
X-Organisation: BSO/Language Technology BV
X-Address: 	P.O. Box 543
X-City: 	NL-3740 AM Baarn
X-Country: 	The Netherlands
X-Phone: 	(+31) 2154 84411
X-Fax: 		(+31) 2154 16781
Received: from lt5 by ltb.ltb.bso.nl (5.65b/LT-1.0)
	id AA14001; Tue, 22 Dec 92 15:11:47 +0100
Received: by lt5.ltb.bso.nl (5.65b/LT-1.0)
	id AA20349; Tue, 22 Dec 92 15:11:45 +0100
From: maxwell@ltb.bso.nl
Message-Id: <9212221411.AA20349@lt5.ltb.bso.nl>
Subject: questions
To: dg@ai.uga.edu
Date: Tue, 22 Dec 92 15:11:43 MET
X-Mailer: ELM [version 2.3 PL11]


Michael raises some questions about dependency grammar. I plead ignorance on
(1), (2), and (5), but feel qualified to provide some answers to (3) and (4).

"(3) Bechraoui has pointed out that there are really two distinctions between
constituency and dependency theories:
(a) Do you use dependency arcs or constituency trees to represent structure?


I would definitely not use constituency trees, since then I think you would be
doing some kind of constituency grammar. Actually some kinds of constituency
grammar are not that far from dependency grammar.  Carl Pollard was quoted as
saying that HPSG is almost a dependency grammar, and this is to some extent
true for any constituent grammar which makes use of X-bar theory.

As for dependency arcs, as used in Dick Hudson's word grammar, for example,
these appear to me to be a notational variant of what I'm used to working
with, namely dependency trees. At least it is not obvious to me what the
empirical differences between them are.

(b) Do you take structure as basic or relations and functions as basic?"  I
think you need a combination of both: arcs or branches to indicate
structure, but usually with some kind of label to indicate function.

There is then less structure than in constituency trees, but somewhat more function marking.

(4) What is the dependency analog of ID/LP rules?

linearization rules which are based either on the governor-dependent
relationship

or the sibling relationship between nodes with the same governor.
--

This is in constrast to ID/LP rules which make exclusive use of sibling
relationships in GPSG, but in HPSG there is also a second kind of LP rule,
based on a grammatical relations hierarchy.

Dan
Maxwell@ltb.bso.nl

---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From @mail-a.bcc.ac.uk:uclyrah@ucl.ac.uk  Tue Dec 22 16:29:31 1992
Return-Path: <@mail-a.bcc.ac.uk:uclyrah@ucl.ac.uk>
Received: from sun2.nsfnet-relay.ac.uk by aisun1.ai.uga.edu (4.1)
	id AA01767; Tue, 22 Dec 92 16:29:31 EST
Via: uk.ac.bcc.mail-a; Tue, 22 Dec 1992 21:29:26 +0000
Received: from link-1.ts.bcc.ac.uk by mail-a.bcc.ac.uk with SMTP (PP) 
          id <06680-0@mail-a.bcc.ac.uk>; Tue, 22 Dec 1992 21:29:23 +0000
Received: by link-1.ts.bcc.ac.uk (AIX 3.2/UCB 5.64/4.03) id AA17024;
          Tue, 22 Dec 1992 21:29:21 GMT
Message-Id: <9212222129.AA17024@link-1.ts.bcc.ac.uk>
To: dg@ai.uga.edu
Subject: Notation
Date: Tue, 22 Dec 92 21:29:21 +0000
From: "R.HudsonIBM-850" <uclyrah@ucl.ac.uk>


Dan Maxwell raises the question, inter alia, of notation. Is there any
empirical difference between dependency arcs and dependency trees?
Since he mentions me as an arc-user, I should react. 

a. I agree that in principle there's no difference, provided we make
the usual assumptions about possible dependency structures (and in
particular, that no word has more than one head). 
E.g.     
        \                       
        : \                               
        :   \                             ------>  -------->
        :     \                         book   about   linguistics
        :       
        :       : \                              or:
        :       :   \              =
        :       :     
        :       :       \                  /---\   /-------
        :       :         \               |     | |         |
        :       :         :               |     V |         V
       book   about   linguistics       book   about   linguistics

(Graphic substance leaves something to be desired in both notations
when you're aiming at a simple DOS file!)

b. However the picture changes (literally) if, like me, you think
there are cases where more complex structures are needed, involving
double-headed words or even inter-dependent words. Then you obviously
can't use the vertical dimension to show dependency because if A and B
are inter-dependent they'd each need to be higher than the other. I
think this is what's needed in relative clauses, where the relative
pronoun or the antecedent noun is both the head of the relative verb,
and also one of its dependents. Arcs or flat arrows can show such
relations, but I don't think dependency trees can.

                                    /----------
                                   | /--------\ | 
             -------->             ||     /--\ ||
             <--------             ||    |    |||
                <-----             |V    V    ||V
          book  I   bought   =    book   I   bought

The empirical question, then, is whether such analyses really are
needed in order to deal with the data (in this case, relative
clauses). If you're convinced they aren't, then you'll prefer
dependency trees, as a more restrictive notation. 

That's why I do believe there's an empirical difference between the
notations.


Dick Hudson
Dept of Phonetics and Linguistics,
University College London,
Gower Street,
London WC1E 6BT
(071) 387 7050 ext 3152

---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From mcovingt@aisun3.ai.uga.edu  Mon Jan  4 11:51:40 1993
Return-Path: <mcovingt@aisun3.ai.uga.edu>
Received: from server.uga.edu by aisun1.ai.uga.edu (4.1)
	id AA24506; Mon, 4 Jan 93 11:51:40 EST
Received: by server.uga.edu (5.57/Ultrix3.0-C)
	id AA13240; Mon, 4 Jan 93 11:51:37 -0500
Received: by aisun3.ai.uga.edu (4.1)
	id AA03968; Mon, 4 Jan 93 11:51:36 EST
From: mcovingt@aisun3.ai.uga.edu (Michael Covington)
Message-Id: <9301041651.AA03968@aisun3.ai.uga.edu>
Subject: New year greetings
To: dg@ai.uga.edu
Date: Mon, 4 Jan 93 11:51:36 EST
X-Mailer: ELM [version 2.3 PL11]

There are now a total of 73 subscribers to DG, the dependency grammar
mailing list.  Now let's get some discussion going...

Happy new year to all. 
-- 
:-  Michael A. Covington     internet mcovingt@uga.cc.uga.edu :    *****
:-  Artificial Intelligence Programs       phone 706 542-0358 :  *********
:-  The University of Georgia                fax 706 542-0349 :   *  *  *
:-  Athens, Georgia 30602-7415 U.S.A.     amateur radio N4TMI :  ** *** **
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From ellalain@nuscc.nus.sg  Tue Jan  5 04:10:18 1993
Return-Path: <ellalain@nuscc.nus.sg>
Received: from nuscc.nus.sg by aisun1.ai.uga.edu (4.1)
	id AA28006; Tue, 5 Jan 93 04:10:18 EST
Received: by nuscc.nus.sg (5.65/1.34)
	id AA01534; Tue, 5 Jan 93 17:09:34 +0800
Date: Tue, 5 Jan 93 17:09:34 +0800
From: ellalain@nuscc.nus.sg (Alain Polguere)
Message-Id: <9301050909.AA01534@nuscc.nus.sg>
To: <dg@ai.uga.edu>
Subject: Translation of Tesniere


Dear DGs,

It seems to me that there is no English translation
of Tesniere's _Elements_de_Syntaxe_Structurale_.
Is that right? I would love to have one (for teaching
purposes).

AP


---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From @mail-a.bcc.ac.uk:uclyrah@ucl.ac.uk  Wed Jan  6 05:38:05 1993
Return-Path: <@mail-a.bcc.ac.uk:uclyrah@ucl.ac.uk>
Received: from sun2.nsfnet-relay.ac.uk by aisun1.ai.uga.edu (4.1)
	id AA02622; Wed, 6 Jan 93 05:38:05 EST
Via: uk.ac.bcc.mail-a; Wed, 6 Jan 1993 10:36:42 +0000
Received: from link-1.ts.bcc.ac.uk by mail-a.bcc.ac.uk with SMTP (PP) 
          id <14146-0@mail-a.bcc.ac.uk>; Wed, 6 Jan 1993 10:36:39 +0000
Received: by link-1.ts.bcc.ac.uk (AIX 3.2/UCB 5.64/4.03) id AA44077;
          Wed, 6 Jan 1993 10:36:37 GMT
Message-Id: <9301061036.AA44077@link-1.ts.bcc.ac.uk>
To: dg@ai.uga.edu
Subject: Translation of Tesniere
Date: Wed, 06 Jan 93 10:36:37 +0000
From: "R.HudsonIBM-850" <uclyrah@ucl.ac.uk>


Alain Polguere seems to me to be right - there is no English translation of
Tesniere, and there ought to be one (perhaps abbreviated?). I have contacts
with publishers. Can anyone suggest a good person to do such a translation?
The ideal person would be an experienced English-French translator who could
also relate Tesniere's work to more recent developments in English-language
syntactic theory. 


Dick Hudson
Dept of Phonetics and Linguistics,
University College London,
Gower Street,
London WC1E 6BT
(071) 387 7050 ext 3152

---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From @mail-a.bcc.ac.uk:uclyrah@ucl.ac.uk  Wed Jan  6 06:09:27 1993
Return-Path: <@mail-a.bcc.ac.uk:uclyrah@ucl.ac.uk>
Received: from sun2.nsfnet-relay.ac.uk by aisun1.ai.uga.edu (4.1)
	id AA02654; Wed, 6 Jan 93 06:09:27 EST
Via: uk.ac.bcc.mail-a; Wed, 6 Jan 1993 11:08:52 +0000
Received: from link-1.ts.bcc.ac.uk by mail-a.bcc.ac.uk with SMTP (PP) 
          id <14621-0@mail-a.bcc.ac.uk>; Wed, 6 Jan 1993 11:08:48 +0000
Received: by link-1.ts.bcc.ac.uk (AIX 3.2/UCB 5.64/4.03) id AA98055;
          Wed, 6 Jan 1993 11:08:47 GMT
Message-Id: <9301061108.AA98055@link-1.ts.bcc.ac.uk>
To: dg@ai.uga.edu
Subject: Dependency statistics
Date: Wed, 06 Jan 93 11:08:46 +0000
From: "R.HudsonIBM-850" <uclyrah@ucl.ac.uk>



I've just been doing some work on the statistics of dependencies in
texts, which turn out to be rather interesting. After analysing 5
different very short texts (each about 180 words +/- 60), I'm prepared
to hazard the following generalisations about any written English text
of at least this length:

- 71% (+/- 8%) of all dependents will follow their heads;
- 59% (+/- 5%) of dependents will be complements; 
- 8% (+/- 4% of dependents will be subjects;
- 67% (+/- 3% !! ) of dependents will be next to their heads;
- 79% (+/- 4% !! ) of dependents will be no more than one word  away
from their heads;
- at no point in the chain of words will there be more than 6 `open'
dependencies, i.e. dependencies for which a head or dependent is
expected but not yet found on a left-right parse (this needs some
explanation, but I think it makes sense; it can actually be made more
interesting by weighting the dependencies as complements, adjuncts,
etc.). This figure varies considerably between texts, according to
their syntactic complexity. For simple texts the upper limit is 4 and
most words have no more than one. 

Notice how precise some of these figures are - and therefore how easy
it should be to refute them. I'm not sure that I really believe them
myself, but that should increase your confidence in them since I was
the analyst who produced them. 

It would be very interesting to have similar figures for other
languages. I have analysed two similar texts in German and found:

- contrary to expectations, a small majority of words follow their
heads;
- the proportion of complement and subject dependencies is within the
same range as for English;
- the proportion of words that are next to their heads or only one
word away from their heads is about 5% less than the minimum for
English - i.e. German dependencies are less closely tied to physical
proximity than English ones are;
- the number of open dependencies is higher than in English - i.e.
processing load for the German reader is greater than for the English
reader. (This is a rather mind-blowing conclusion, and needs to be
checked carefully before being announced as a discovery!)

Needless to say, the analyses reported are based on my own grammar (on
the whole as reported in my "English Word Grammar" 1990), some of
which are controvertial - e.g. I take a determiner as the head of the
accompanying common noun. (I think I have some weak statistical
support for this analysis, in fact.) You have to read the figures with
these assumptions in mind - e.g. if you're sure that determiners
depend on their common noun, then the figures for complements and
head-first have to be reduced by between 10 and 20%.

I'd be very interested to hear from anyone who has similar figures
they'd be willing to share with me, or to send more details to anyone
who'd like to examine other texts. I have a Prolog system for manually
parsing texts and counting the results which may be helpful.


Dick Hudson
Dept of Phonetics and Linguistics,
University College London,
Gower Street,
London WC1E 6BT
(071) 387 7050 ext 3152

---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From Edmund.Grimley-Evans@cl.cam.ac.uk  Wed Jan  6 06:37:47 1993
Return-Path: <Edmund.Grimley-Evans@cl.cam.ac.uk>
Received: from swan.cl.cam.ac.uk by aisun1.ai.uga.edu (4.1)
	id AA02685; Wed, 6 Jan 93 06:37:47 EST
Received: from duncan.cl.cam.ac.uk (user etg10 (rfc931)) by swan.cl.cam.ac.uk 
          with SMTP (PP-6.4) to cl; Wed, 6 Jan 1993 11:37:22 +0000
Received: by duncan.cl.cam.ac.uk (5.57/SMI-3.0DEV3) id AA01571;
          Wed, 6 Jan 93 11:37:09 GMT
Date: Wed, 6 Jan 93 11:37:09 GMT
From: Edmund.Grimley-Evans@cl.cam.ac.uk
Message-Id: <9301061137.AA01571@duncan.cl.cam.ac.uk>
To: dg@ai.uga.edu
Cc: uclyrah@ucl.ac.uk
Subject: Dependency statistics

How frequent were discontinuous dependency trees?

(The strongest and simplest definition of continuity that I know of is
that the set of (indirect) dependents of every word should be a
contiguous set of words in the sentence. I understand that there are
also weaker ways of defining continuity.)

=======================================================================
Edmund GRIMLEY EVANS                  Edmund.Grimley-Evans@cl.cam.ac.uk
=======================================================================
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From @sun.central-services.umist.ac.uk:harold@language-linguistics.umist.ac.uk  Wed Jan  6 06:38:47 1993
Return-Path: <@sun.central-services.umist.ac.uk:harold@language-linguistics.umist.ac.uk>
Received: from sun2.nsfnet-relay.ac.uk by aisun1.ai.uga.edu (4.1)
	id AA02696; Wed, 6 Jan 93 06:38:47 EST
Via: uk.ac.umist.central-services.sun; Wed, 6 Jan 1993 11:38:16 +0000
Received: from honshu.ccl.umist.ac.uk by cclsun.ccl.umist.ac.uk;
          Wed, 6 Jan 93 11:38:07 GMT
From: Harold Somers <harold@language-linguistics.umist.ac.uk>
Message-Id: <4100.9301061138@honshu.ccl.umist.ac.uk>
Subject: Re: Translation of Tesniere
To: uclyrah@ucl.ac.uk (R.HudsonIBM-850)
Date: Wed, 6 Jan 93 11:38:04 GMT
Cc: dg@ai.uga.edu
In-Reply-To: <9301061036.AA44077@link-1.ts.bcc.ac.uk>; from "R.HudsonIBM-850" at Jan 6, 93 10:36 am
X-Mailer: ELM [version 2.3 PL11]

> 
> 
> Alain Polguere seems to me to be right - there is no English translation of
> Tesniere, and there ought to be one (perhaps abbreviated?). I have contacts

I would have thought that it would be better to have a detailed
commentary on Elements, with extensive extracts, rather than a faithful
translation. There's so much in there that is naive, wrong or
out-dated. 10 years ago, when I was doing research for my PhD, I wrote
[2] a short paper (11 pages) discussing in detail the parts of Elements
which I thought werte directly relevant to Valnecy and Case grammar
(the topic of my thesis), 
viz. Book A 'Preamble' Ch1-6 (pp 11-20)
     Book B 'Structure of the verb phrase' Ch 48-62 (pp 102-143)
     Book D 'Valency' (pp 238-282 partim)

I ignored all the stuff on 'translation', which I - and earlier
commentators like Baum [1] - took to be very similar to the early
ideas of Chomsky; this wasnt of interest to me at the time, so I
skipped over it.

Being a fairly thorough young chap (those WERE the days), I chased up
quite a lot of reviews of Elements as well (including one in Hebrew
which I never managed to get translated! - sadly, I dont seem to be
able to lay my hands on my bulging file of Tesniere-related stuff), 
and also managed to find copies of the two non-posthumous publications 
[3,4].

If anyone is interested in this old paper of mine (I wrote it in
1982), please let me know your surface address, and I will send it to
you. 

> with publishers. Can anyone suggest a good person to do such a translation?
> The ideal person would be an experienced English-French translator who could

Better would be a French-English translator! :-)

References

[1] R. Baum. 1976. Dependenzgrammatik: Tesnie`re's Modell der
Sprachbeschreibung in wissenschaftlicher und kritischer Sicht (Beihefte
zur Zeitschrift fu"r Romanische Philologie 151). Tu"bingen: Max
Niemeyer Verlag.

[2] H.L. Somers. 1982. Lucien Tesnie`re: a review of his 'oeuvre'.
Report No. 82/8, Centre for Computational Linguistics, UMIST,
Manchester.

[3] L. Tesnie`re. 1934. Comment construire un syntaxe. Bulletin de la
Faculte' des Lettres de Strasbourg 12, 219-229.

[4] L. Tesnie`re. 1953. Esquisse d'une syntaxe structurale. Paris:
Librairie C. Klincksieck.
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From @sun.central-services.umist.ac.uk:harold@language-linguistics.umist.ac.uk  Wed Jan  6 06:47:24 1993
Return-Path: <@sun.central-services.umist.ac.uk:harold@language-linguistics.umist.ac.uk>
Received: from sun2.nsfnet-relay.ac.uk by aisun1.ai.uga.edu (4.1)
	id AA02708; Wed, 6 Jan 93 06:47:24 EST
Via: uk.ac.umist.central-services.sun; Wed, 6 Jan 1993 11:46:57 +0000
Received: from honshu.ccl.umist.ac.uk by cclsun.ccl.umist.ac.uk;
          Wed, 6 Jan 93 11:46:51 GMT
From: Harold Somers <harold@language-linguistics.umist.ac.uk>
Message-Id: <4143.9301061146@honshu.ccl.umist.ac.uk>
Subject: valency of nouns
To: dg@ai.uga.edu
Date: Wed, 6 Jan 93 11:46:49 GMT
X-Mailer: ELM [version 2.3 PL11]

Can anyone mail me a bibliography on the subject of nominal valency. I
did some work on this a few years ago, and so have a few articles up to
about 7 years ago, the latest being a manuscript by Jane Grimshaw
(Nouns, Arguments and Adjuncts, May 1986). I wonder if that was ever
published anywhere?

A more up-to-date bibliography would be much appreciated.
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From bnevin@ccb.bbn.com  Wed Jan  6 09:15:50 1993
Return-Path: <bnevin@ccb.bbn.com>
Received: from BBN.COM by aisun1.ai.uga.edu (4.1)
	id AA03097; Wed, 6 Jan 93 09:15:50 EST
Message-Id: <9301061415.AA03097@aisun1.ai.uga.edu>
Received: from CCB.BBN.COM by BBN.COM id aa02577; 6 Jan 93 9:11 EST
Date: Wed, 6 Jan 93 09:07:15 EST
From: "Bruce E. Nevin" <bnevin@ccb.bbn.com>
Subject: determiner as head
To: dg%ai.uga.edu@bbn.com
Cc: bn@ccb.bbn.com

I have some independent support for your analysis of the
determiner as the head of the noun phrase.  In operator grammar,
"the" is analyzed as a noun in apposition to the following noun,
by way of a reduction from "that which is":

	The family doctor is fast disappearing.
	That which is a family doctor is fast disappearing.

Compare other appositional constructions, viz.:

	My friend John
	My friend who is John
	My friend the doctor
	My friend who is the doctor

For discussion and supporting argument, see Z. Harris, _A Grammar
of English on Mathematical Principles_, Wiley (1982), 236-243.

> From: "R.HudsonIBM-850" <uclyrah@ucl.ac.uk>
> Date: Wed, 06 Jan 93 11:08:46 +0000

> Needless to say, the analyses reported are based on my own grammar (on
> the whole as reported in my "English Word Grammar" 1990), some of
> which are controvertial - e.g. I take a determiner as the head of the
> accompanying common noun. (I think I have some weak statistical
> support for this analysis, in fact.) You have to read the figures with

	Bruce Nevin
	bn@bbn.com
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From rsharman@vnet.ibm.com  Wed Jan  6 09:53:34 1993
Return-Path: <rsharman@vnet.ibm.com>
Received: from vnet.ibm.com by aisun1.ai.uga.edu (4.1)
	id AA03237; Wed, 6 Jan 93 09:53:34 EST
Message-Id: <9301061453.AA03237@aisun1.ai.uga.edu>
Received: from WINVMD by vnet.ibm.com (IBM VM SMTP V2R2) with BSMTP id 8885;
   Wed, 06 Jan 93 09:51:35 EST
Date: Wed, 6 Jan 93 14:52:37 GMT
From: rsharman@vnet.ibm.com
To: dg@ai.uga.edu
Subject: Dependency Statistics

 After some discussions with Dick Hudson a few weeks ago I became interested
 in the question of how adjacent and non-adjacent words might be related,
 and whether it was possible to shed some light on the (thorny) question
 of whether a phrase-structure or a dependency-graph representation of
 the word associations in a sentence was better. It occurred to me that
 some techniques currently in use in Language Modelling for Speech
 Recognition might come in useful, so I proposed the following problem:

 The WORD PAIR ASSOCIATION problem: what is the average degree of association
   between pairs of words, taken over the language as a whole, when the
   constraint is applied that the pairs must be adjacent words?

 This generalises to:

 The GENERAL WORD ASSOCIATION problem: what is the average degree of
   association between pairs of words, taken over the language as a
   whole, when the pairs are taken at arbitrary distances (one word
   intervening, two words intervening, etc)?

 From experience in predicting words for Speech Recognition I would have
 expected that the further apart the words in a pair are, the less their
 association. But how far do associations go? what is the precise nature
 of the relationship? etc. Dick Hudson's figures on dependency relations
 seem to suggest that local dependencies are very strong, if not the
 dominant type of relationship. But it could be that the figures he
 gives have something to do with the type of grammatical analysis he has
 assumed. So, independently of any grammatical theory, what does one word
 tell you about other words in its local context?

 I decided to try a small experiment calculating the Mutual Information
 between pairs of words as an indicator of this association. (Mutual
 Information between two items, x and y, is defined as the log of the
 ratio of the joint probability of the two items occuring to the
 product of their separate a priori probabilities. Any textbook on
 Information Theory gives the details). Then average mutual information
 for all words taken from a corpus would give some figures for the
 general amount of association in a language. Here are the results
 for a sample corpus of 192,723 words of English Newspaper text, with
 a vocabulary of 22,996 unique lexical items:

 1  Self-information of a word with itself      10.25
 2  adjacent word pairs                          3.00
 3  a pair separated by 1 word                   2.83
 4  a pair separated by 2 words                  2.73
 5  a pair separated by 3 words                  2.68

 The measure is in bits per word. So, knowing a word is worth about 10
 bits, and it tells you about 3 bits worth about the next word, and so on.

 In general, the hypothesis appears correct: the further words are
 away from each other the less related they are, INDEPENDENTLY of any
 assumptions about grammatical theory.

 Any comments?      Richard Sharman
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From @mail-a.bcc.ac.uk:ucleaar@ucl.ac.uk  Wed Jan  6 16:12:25 1993
Return-Path: <@mail-a.bcc.ac.uk:ucleaar@ucl.ac.uk>
Received: from sun2.nsfnet-relay.ac.uk by aisun1.ai.uga.edu (4.1)
	id AA05897; Wed, 6 Jan 93 16:12:25 EST
Via: uk.ac.bcc.mail-a; Wed, 6 Jan 1993 21:09:18 +0000
Received: from link-1.ts.bcc.ac.uk by mail-a.bcc.ac.uk with SMTP (PP) 
          id <22719-0@mail-a.bcc.ac.uk>; Wed, 6 Jan 1993 21:09:16 +0000
Received: by link-1.ts.bcc.ac.uk (AIX 3.2/UCB 5.64/4.03) id AA60538;
          Wed, 6 Jan 1993 21:09:13 GMT
Message-Id: <9301062109.AA60538@link-1.ts.bcc.ac.uk>
To: dg@ai.uga.edu, "Bruce E. Nevin" <bnevin@ccb.bbn.com>
Subject: Re: determiner as head
In-Reply-To: (Your message of Wed, 06 Jan 93 09:07:15 EST.) <9301061415.AA03097@aisun1.ai.uga.edu>
X-Caution: the 'from' field may be garbled, but 'sender' is probably OK.
Date: Wed, 06 Jan 93 21:09:12 +0000
From: And Rosta <ucleaar@ucl.ac.uk>


Bruce Nevin:
> I have some independent support for your analysis of the
> determiner as the head of the noun phrase.  In operator grammar,
> "the" is analyzed as a noun in apposition to the following noun,
> by way of a reduction from "that which is":
> 
> 	The family doctor is fast disappearing.
> 	That which is a family doctor is fast disappearing.
> 
> Compare other appositional constructions, viz.:
> 
> 	My friend John
> 	My friend who is John
> 	My friend the doctor
> 	My friend who is the doctor
> 
> For discussion and supporting argument, see Z. Harris, _A Grammar
> of English on Mathematical Principles_, Wiley (1982), 236-243.

Why is this an argument for the determiner being the head? Rather,
the examples seem to show the attractiveness of treating the determiner
as *coreferential* with the common noun. Or is _that which is_ actually
*present* at some stage & then deleted?

What are the grounds for deciding which phrase's head is the root of
the restrictive appo construction? Is the funny prosody of (1a) relevant?

  1 a.    MY friend eLIza
    b.    my FRIEND the DOCtor

----
And Rosta
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From mcovingt@aisun3.ai.uga.edu  Wed Jan  6 16:36:50 1993
Return-Path: <mcovingt@aisun3.ai.uga.edu>
Received: from server.uga.edu by aisun1.ai.uga.edu (4.1)
	id AA06270; Wed, 6 Jan 93 16:36:50 EST
Received: by server.uga.edu (5.57/Ultrix3.0-C)
	id AA22292; Wed, 6 Jan 93 16:36:49 -0500
Received: by aisun3.ai.uga.edu (4.1)
	id AA08438; Wed, 6 Jan 93 16:36:48 EST
From: mcovingt@aisun3.ai.uga.edu (Michael Covington)
Message-Id: <9301062136.AA08438@aisun3.ai.uga.edu>
Subject: Natural language software registry (announcement)
To: dg@ai.uga.edu
Date: Wed, 6 Jan 93 16:36:47 EST
X-Mailer: ELM [version 2.3 PL11]

Forwarded message:
From registry@dfki.uni-sb.de Wed Jan  6 05:18:03 1993
Date: Wed, 6 Jan 93 11:20:54 +0100
Message-Id: <9301061020.AA07617@disco-sun5.dfki.uni-sb.de>
Organization: DFKI Saarbruecken GmbH, D-W 6600 Saarbruecken
From: registry@dfki.uni-sb.de (The Software Registry)
To: mcovingt@aisun3.ai.uga.edu
Subject: NL Software Registry
Reply-To: registry@dfki.uni-sb.de

(Dear participants: I'm passing along the following, which
seems interesting. -- Michael Covington)


Remark:                  NATURAL LANGUAGE SOFTWARE REGISTRY


The Natural Language Software Registry is a catalogue of software
implementing core natural language processing techniques, whether
available on a commercial or noncommercial basis. The current
version includes 

+ speech signal processors, such as the Computerized Speech Lab 
	(Kay Electronics)
+ morphological analyzers, such as PC-KIMMO 
	(Summer Institute for Linguistics)
+ parsers, such as Alveytools (University of Edinburgh)
+ knowledge representation systems, such as Rhet 
	(University of Rochester)
+ multicomponent systems, such as ELU (ISSCO), PENMAN (ISI), 
	Pundit (UNISYS), SNePS (SUNY Buffalo),
+ applications programs (misc.)

This document is available on-line via anonymous ftp to ftp.dfki.uni-sb.de
(directory:registry), by email to 

                    registry@dfki.uni-sb.de, 

and by physical mail to the address below.


Now our request: we would like to subscribe Dependency Grammar in order
                 to be fully informed about the activities in this area.


        Yours,

        Christoph Jung, Markus Vonerden 


        Natural Language Software Registry
        Deutsches Forschungsinstitut fuer Kuenstliche Intelligenz (DFKI)
        Stuhlsatzenhausweg 3
        D-W-6600 Saarbruecken
        Germany

        phone: +49 (681) 303-5282
        e-mail: registry@dfki.uni-sb.de


-- 
:-  Michael A. Covington     internet mcovingt@uga.cc.uga.edu :    *****
:-  Artificial Intelligence Programs       phone 706 542-0358 :  *********
:-  The University of Georgia                fax 706 542-0349 :   *  *  *
:-  Athens, Georgia 30602-7415 U.S.A.     amateur radio N4TMI :  ** *** **
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From @mail-a.bcc.ac.uk:uclyrah@ucl.ac.uk  Thu Jan  7 03:01:57 1993
Return-Path: <@mail-a.bcc.ac.uk:uclyrah@ucl.ac.uk>
Received: from sun2.nsfnet-relay.ac.uk by aisun1.ai.uga.edu (4.1)
	id AA08067; Thu, 7 Jan 93 03:01:57 EST
Via: uk.ac.bcc.mail-a; Thu, 7 Jan 1993 08:00:16 +0000
Received: from link-1.ts.bcc.ac.uk by mail-a.bcc.ac.uk with SMTP (PP) 
          id <26499-0@mail-a.bcc.ac.uk>; Thu, 7 Jan 1993 08:00:09 +0000
Received: by link-1.ts.bcc.ac.uk (AIX 3.2/UCB 5.64/4.03) id AA67697;
          Thu, 7 Jan 1993 08:00:05 GMT
Message-Id: <9301070800.AA67697@link-1.ts.bcc.ac.uk>
To: dg@ai.uga.edu
Subject: Apposition
Date: Thu, 07 Jan 93 08:00:05 +0000
From: "R.HudsonIBM-850" <uclyrah@ucl.ac.uk>


7 January 1993

Bruce Nevin says that operator grammar recognises the common noun as a
noun in apposition to the determiner; e.g. in "the family doctor",
"doctor" is in apposition to "the". Splendid - just what I say in
"Word Grammar" (1984: 91). But then And Rosta asks what evidence there
is for the next step in the argument, for taking the common noun as a
dependent of the determiner. The evidence that I would use involves
uncontroversial examples of apposition like (1).

(1)a   The village of Trumpington lies next to the M25.
   b   The fact that he was late is beyond dispute.

In (a), "Trumpingon" is clearly in apposition to "village" (i.e.
coreferential with it), but equally clearly "Trumpington" is
subordinated to "village" by the preposition "of". And in (b) it is
quite clear that "that (he was late)" is a dependent of "fact",
because it follows the same rules for extraposition as a relative
clause like "that he reported" in (2); but at the same time it is
clear that it is in apposition to "fact".

(2)    The fact that he reported is beyond dispute.

In both cases the "that" clause can be extraposed:

(3)a   The fact is beyond dispute that he was late.
   b   The fact is beyond dispute that he reported.

Moreover, in both (1b) and (2) the relation between "that" and the
preceding noun would act as a barrier to extraction if the whole noun-
phrase had been in an (otherwise) extractable position:

(4)a   *Who did you discuss the fact that he had contacted?
   b   *Who did you discuss the fact that he had reported to?

       In other words, the argument for determiners as head would run
as follows: 
- there are some cases of apposition in which it is clear that the
second element is dependent on the first;
- the semantic relation between a common noun and its preceding
determiner is the same as in standard examples of apposition;
- there is no clear evidence that the common noun is head of the
determiner;
- therefore we can treat the relation between them as a special case
of the already established apposition relation.



Dick Hudson
Dept of Phonetics and Linguistics,
University College London,
Gower Street,
London WC1E 6BT
(071) 387 7050 ext 3152

---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From @mail-a.bcc.ac.uk:uclyrah@ucl.ac.uk  Thu Jan  7 03:02:02 1993
Return-Path: <@mail-a.bcc.ac.uk:uclyrah@ucl.ac.uk>
Received: from sun2.nsfnet-relay.ac.uk by aisun1.ai.uga.edu (4.1)
	id AB08067; Thu, 7 Jan 93 03:02:02 EST
Via: uk.ac.bcc.mail-a; Thu, 7 Jan 1993 08:01:33 +0000
Received: from link-1.ts.bcc.ac.uk by mail-a.bcc.ac.uk with SMTP (PP) 
          id <26540-0@mail-a.bcc.ac.uk>; Thu, 7 Jan 1993 08:01:26 +0000
Received: by link-1.ts.bcc.ac.uk (AIX 3.2/UCB 5.64/4.03) id AA67746;
          Thu, 7 Jan 1993 08:01:24 GMT
Message-Id: <9301070801.AA67746@link-1.ts.bcc.ac.uk>
To: dg@ai.uga.edu
Re: Discontinuous phrases
Date: Thu, 07 Jan 93 08:01:24 +0000
From: "R.Hudson" <uclyrah@ucl.ac.uk>


6 January 1993

Edmund Grimley-Evans asks how many discontinuous dependency trees I
found in my texts. The brief answer is either (a) that I don't know,
or (b) that I didn't find any, according to what theoretical
assumptions you make. The most obvious place where the difference
arises is in cases of `extraction' like (1), where "who" depends on
"contact", so the phrase rooted in "contact" is discontinuous.

(1) Who did you tell John he should contact?

(a) If you assume that dependencies are by definition `deep', i.e.
closely linked to semantics and valency, and that each word has only
one head, then "who" has just one head, "contact". I don't know how
frequent such phrases are, because I would have recorded "John" as a
dependent of "did". This dependency is essential for the position of
"John" (i.e. "John" takes its position from "did"), but has nothing to
do with the semantic role of "John", so you won't recognise it as a
proper dependency. But then you have to decide what to say about
examples like (2), where "John" is a `raised' subject of "seems", and
has all the characteristics of a syntactic subject (and is therefore a
dependent) of "seems" without having any semantic relation to it.

(2) John seems to like Mary.

(b) If you allow a word to have more than one head (as I do - see for
example my "English Word Grammar" 1990) then "who" can have (at least)
two heads, "did" and "contact". One is responsible for its position,
and the other for its semantic role. If you allow purely surface
dependencies of this kind, then you find that every English sentence
is built around a dependency tree that is totally continuous. The
continuous trees are the only ones I recorded in my analysis, because
there are a lot of other grafted-on (but easily recoverable)
dependencies (of which the relation between "who" and "contact" is
just one example). I felt that it was reasonable to record just the
"basic skeleton" because the other dependencies are reasonably
predictable from it; e.g. if you know that "who" depends on "did", and
that "contact" is subordinate to "did" (i.e. indirectly dependent on
it) then it is easy to work out from the grammar that "who" can also
depend on "contact". This gives the grammar just a bit more power than
a context-free phrase structure grammar, and ought to make it
computationally tractable.

       However it would indeed be interesting to know what the answer
to Edmund's question would be given the more standard assumptions of
(a), as a measure of the difficulty of the parsing task without my
extra assumptions.


Dick Hudson
Dept of Phonetics and Linguistics,
University College London,
Gower Street,
London WC1E 6BT
(071) 387 7050 ext 3152

---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From ellalain@nuscc.nus.sg  Thu Jan  7 04:34:02 1993
Return-Path: <ellalain@nuscc.nus.sg>
Received: from nuscc.nus.sg by aisun1.ai.uga.edu (4.1)
	id AA08170; Thu, 7 Jan 93 04:34:02 EST
Received: by nuscc.nus.sg (5.65/1.34)
	id AA22001; Thu, 7 Jan 93 17:33:56 +0800
Date: Thu, 7 Jan 93 17:33:56 +0800
From: ellalain@nuscc.nus.sg (Alain Polguere)
Message-Id: <9301070933.AA22001@nuscc.nus.sg>
To: <dg@ai.uga.edu>
Subject: Tesniere


Concerning Tesniere (1959), Harold Somers writes:

> I would have thought that it would be better to have a detailed
> commentary on Elements, with extensive extracts, rather than a faithful
> translation. There's so much in there that is naive, wrong or
> out-dated. 10 years ago, when I was doing research for my PhD, I wrote
> [2] a short paper (11 pages) discussing in detail the parts of Elements
> which I thought werte directly relevant to Valnecy and Case grammar
> (the topic of my thesis), 

Thanks for your comment.
You may be right, even though I am not so sure that I necessarily
would agree with you on what to consider "naive, wrong or out-dated"
in Tesniere's work...
Nevertheless, two remarks:

1) It would be nice if all linguistics books which are translated
   could contain at least a quarter of what can be found AND USED
   in Tesniere (1959). In that respect, I think it won't lower the
   standards.
2) I still maintain that a full translation is more desirable;
   I prefer to select myself what is relevant for my own needs.
   By the way, I would be glad to help as a reviewer for the
   translation.

I am very interested in having a copy of your '82
paper. I include the .signature below...

 - Alain POLGUERE --------------------------------------------------------
|                   Department of English Language and Literature         |
|                   National University of Singapore                      |
|                   10 Kent Ridge Crescent -- Republic of Singapore  0511 |
|                   tel. (65) 772-3700 / fax (65) 773-2981                |
|                   ellalain@nuscc.nus.sg / ellalain@nusvm.bitnet         |
 -------------------------------------------------------------------------

---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From bnevin@ccb.bbn.com  Thu Jan  7 11:47:13 1993
Return-Path: <bnevin@ccb.bbn.com>
Received: from BBN.COM by aisun1.ai.uga.edu (4.1)
	id AA08967; Thu, 7 Jan 93 11:47:13 EST
Message-Id: <9301071647.AA08967@aisun1.ai.uga.edu>
Received: from CCB.BBN.COM by BBN.COM id ab28811; 7 Jan 93 11:45 EST
Date: Thu, 7 Jan 93 11:41:44 EST
From: "Bruce E. Nevin" <bnevin@ccb.bbn.com>
Subject: Re: determiner as head
In-Reply-To: Your message of Wed, 06 Jan 93 21:09:12 +0000
To: dg%ai.uga.edu@bbn.com
Cc: bn@ccb.bbn.com

And Rosta:

>Why is this an argument for the determiner being the head? Rather,
>the examples seem to show the attractiveness of treating the determiner
>as *coreferential* with the common noun. Or is _that which is_ actually
>*present* at some stage & then deleted?

Yes, _that which is_ is actually present.  In fact, it never goes
away.  It is present overtly at an earlier stage of the
derivation, and still present in phonemically zero form at the
later stage.  Thus, all dependencies found in the unreduced form
are still present in the reduced form.

>What are the grounds for deciding which phrase's head is the root of
>the restrictive appo construction? 

The noun _friend_ is clearly the head of the construction _my
friend who is the doctor_, and it is still the head when _who is_
is reduced to zero in _my friend the doctor_.

The (indefinite) noun _that_ is clearly the head of the
construction _that which is a family doctor_, and it is still the
head when _that_ is reduced to _the_ in the environment of _which
is_ being reduced to zero, yielding _the family doctor_.

>				     Is the funny prosody of (1a) relevant?
>
>  1 a.    MY friend eLIza
>    b.    my FRIEND the DOCtor

Contrastive stress is another matter, available in a pretty
unrestricted way.  It applies before the reductions, viz.:

  1 a.    MY friend who is eLIza
    b.    my FRIEND who is the DOCtor

For reference, I had said:

>				            In operator grammar,
> "the" is analyzed as a noun in apposition to the following noun,
> by way of a reduction from "that which is":
> 
>     The family doctor is fast disappearing.
>     That which is a family doctor is fast disappearing.
> 
> Compare other appositional constructions, viz.:
> 
>     My friend John
>     My friend who is John
>     My friend the doctor
>     My friend who is the doctor
> 
> For discussion and supporting argument, see Z. Harris, _A Grammar
> of English on Mathematical Principles_, Wiley (1982), 236-243.

Co-reference is an effect of a metalinguistic assertion that 
two occurrences of a word are the same.  In many treatments of
grammar this metalinguistic assertion is in the form of subscript
indices in the written representation.  But the metalanguage for
natural language is demonstrably statable in natural language
itself (see below), and must necessarily be included in language.
(The metalanguage for logic or mathematics depends upon the
background vernacular of language, the metalanguage for language
cannot.  For discussion, see Z. Harris, _A Theory of Language and
Information_ Oxford, Clarendon (1991).)

The metalanguage adequate for asserting sameness of two words
requires very simple vocabulary and syntax.  The assertion of
sameness is one of the requirements for the derivation of a
relative clause from a secondary sentence:

	My friend the doctor arrived
	My friend who is the doctor arrived.
	My friend--a friend (prior same as mentioned) is the
	doctor--arrived.

The intonation of the interrupting secondary sentence is retained
in the relative clause and in the appositional noun phrase.
(Contrastive stress on _doctor_ would be retained also.)

This account clarifies the difference between "ordinary"
dependency of argument words and operators ("predicative" words
like verbs, adjectives, adverbs, etc.) and dependency of adjuncts
(modifiers) on heads.  

We have zero allomorphs in many morphophonemic alternations, viz.
_John has three sheep-0_.  What are commonly called cases of
elision involve zero allomorphs, e.g.:

  I prefer that John should go.         I prefer that I go.
	----------                      I prefer to   0 go.  

What does that do for dependency representations?

  prefer that             prefer to
   / \                     / \          
  /    \                  /    \        
I	  go            I	  go    
	 /              	 /      
        I                       0       

The reduction to zero requires a metalinguistic assertion of
sameness.  The interruption is under a paratactic conjunction
(represented in writing by semicolon or dash, and represented in
speech by secondary intonation, which is preserved in the reduced
form of the adjunct).  The fact that it is an interruption means
that dependency lines (or projections from nodes to the linear
string of words) must cross:

       ;                               0
      /	\                             /	\
     /	 same as                     /	 0
    /	  /   \                     /   / \
    |  prior mentioned              |  0   0
    |                               |       
    |                               |       
  prefer that                     prefer to
   /         \                     /       \
  /            \                  /          \
I            	  go            I             go
             	 /                            /
                I                            0

In this example, the paratactic conjunction _;_ is zeroed (the
entire metalinguistic assertion has zero form, and there is
nothing left to receive reduced intonation).  In the case of
modifiers from relative clauses, it has phonemic shape still (as
subordinated intonation).

		     ;
		   /  \
		 /    same as      
	       /       / \           
	     /	   prior  mentioned
	    ;      
	  /  \_______________________
	/	                     \
     hit                           is red 
    /	\	       /------------/
John     a ball  a ball

John hit a ball; a ball (prior same as mentioned) is red.

In the full relative clause, the paratactic conjunction has
additional phonemic content as the wh- of the relative pronoun:

		    wh-
		   /  \
		 /      0          
	       /       / \           
	     /	      0   0        
	    ;      
	  /  \_______________________
	/	                     \
     hit                          is red
    /	\	          /---------/
John     a ball       -ich

John hit a ball which is red.

I am now going to use | in place of slanted lines for the sake of
compression.  A vertical line does represent a dependency below,
not a projection from the dependency tree to the linear string.

	   0
	  | \
 	  |  0               
	  | / \              
	  ; 0 0             
	/  \   
       /   red 
     hit    |
    /	\   0
John     a .. ball

John hit a red ball.

Graphical representations of word dependencies soon reach the
limit of practicability (even without the limitations of
diagramming with ASCII characters!).  And one does not need them
to implement a grammar.  The point here is to show that adjuncts,
like the adjective "red" in this example and the noun in
apposition in the earlier examples, are not dependent upon their
head noun, they are dependent upon the paratactic conjunction
that makes an interrupting sentence into a secondary aside about
a word in the main sentence.  That conjunction is still present
in zero form.  As is the metalinguistic sameness statement.

Again, for exemplification, argument, and discussion, see Z.
Harris, _A Grammar of English on Mathematical Principles_.

	Bruce Nevin
	bn@bbn.com

---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From vhahn@nats4.informatik.uni-hamburg.de  Wed Jan 13 04:13:11 1993
Return-Path: <vhahn@nats4.informatik.uni-hamburg.de>
Received: from deneb.dfn.de by aisun1.ai.uga.edu (4.1)
	id AA23919; Wed, 13 Jan 93 04:13:11 EST
Received: from fbihh.informatik.uni-hamburg.de by deneb.dfn.de (4.1/SMI-4.2)
	id AA26790; Wed, 13 Jan 93 10:10:52 +0100
Received: from nats4.informatik.uni-hamburg.de
	by fbihh.informatik.uni-hamburg.de (5.65+/FBIHH-2.26) with SMTP;
	id AA06275; Wed, 13 Jan 93 10:10:38 +0100
Message-Id: <9301130908.AA04723@nats4.informatik.uni-hamburg.de>
Received: from [134.100.5.232] (bomac42)
	by nats4.informatik.uni-hamburg.de (4.1/FBIHH-2.15);
	id AA04723; Wed, 13 Jan 93 10:08:04 +0100
Date: Wed, 13 Jan 1993 10:12:41 +0100
To: dg@ai.uga.edu
From: vhahn@nats4.informatik.uni-hamburg.de
Subject: Sgall's answers

Dear Colleagues,
Using the opportunity of a short stay in Hamburg, where I can find more
quiet time than at home, I would like to express my opinion on some of the
questions you have been discussing, applying the advantages of my age, viz.
the training in European structural grammar I went through with my Prague
teachers in the years around 1950, and my owm long experience with
empirical and theoretical research in dependency syntax.
        (i) As for Chomsky's 'minimalist theory', I cannot say anything
concerning the technical shape of his new theory, but at least one point is
important, and that is his emphasis on the two 'interface levels' of
phonetics and of logical form. In our publications on the 'Functional
Generative Description' (see esp. the book The meaning of the sentence,
published with Reidel in 1986, with J. Mey as the editor) we have always
worked with a single underlying level, conjoining (from a viewpoint closw
to Chomsky's "Cartesian Linguistics" from the 1960's) the roles of 'initial
P-markers', D-structure and logical form; in recent writings, in our group
also the possibility to work without a level of surface syntax was pointed
out (see my paper in Wiener Linguistischer Almanach, Sonderband 33
(Festschrift fuer Rozencvejg), Vienna 1992), so that there are clear points
of convergence between the two approaches, as far as they can be compared
at all.
        (ii)It may be claimed that dependency trees are much simpler than
the usual kinds of P-markers, having a lower number of nodes, working just
with a single set of symbols for syntactic relations (all of which, not
only those corresponding to theta roles or arguments, but also free
adverbials or adjuncts, being justified by the valency grids of the head
words, in which parameters of obligatoriness, deletability, role as
controllers and many others can be lexically specified). However, it should
be admitted that coordination (even if not fully symmetric) is a relation
of another kind, so that more than the two dimensions of the dependency
tree are needed for a network that completely describes the sentence
structure. Strong limitations such as R. Hudson's  adjacency (projectivity)
make the whole system simple enough (the unprojective consructions can be
described as exceptional deviations), treatable e.g. in a linearized form
with two kinds of parentheses (for dependency and for cooridnation), see
the paper by Petkevic in Theoretical Linguistics 1987).
        (iii) An analog of the ID/LP rule dichotomy can be seen in our
hypothesis of a basic ('systemic') ordering of the kinds of dependency
relations (of valency slots, or of theta roles and adjuncts), which is
reflected by word order with several 'deviations', the main of which
consists in the elements of the topic standing more to the left than what
the systemic ordering itself would determine (see Sgall and Hajicova,
Ordering principle, in Journal of Pragmatics 1987); other such deviatios
concern 'shallow rules' on the position of the verb, of the adjective, of
the clitics, etc.
        (iv) A dependency based parser of English was published in our
group by Z. Kirschner in two volumes, which are of an internal character,
not accessible on the market, but I can send them to those who are
interested; the two drawbacks there are that he used Colmerauer's Q-systems
for implementation and that the lexicon used there is narrowly restricted.
We are now preparing a more general version of the parser, and the contact
person for this issue would be Alexandr Rosen,
e-mail:rosen@Praha1.ff.cuni.cs.internet.
        Since I am here just for a few days, please, do not send replies on
the address of this message, but rather to Prague (sgall@cspguk11.bitnet).
I hope I'll be able to continue our discussions in February.
                             Cordially                Petr Sgall
=====================================
====       Walther v.Hahn      ======
=     Computer Science Department   =
= Natural Language Systems Division =
=         Bodenstedtstr.16          =
=       D - 2000 HAMBURG 50         =
=       Tel: (X40) 4123 4529        =
=       Fax (X40) 4123 6530         =
-------------------------------------
vhahn@nats2.informatik.uni-hamburg.de
=====================================

---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From mcovingt@aisun3.ai.uga.edu  Thu Jan 14 23:41:48 1993
Return-Path: <mcovingt@aisun3.ai.uga.edu>
Received: from aisun3.ai.uga.edu by aisun1.ai.uga.edu (4.1)
	id AA03622; Thu, 14 Jan 93 23:41:48 EST
Received: by aisun3.ai.uga.edu (4.1)
	id AA22675; Thu, 14 Jan 93 23:41:48 EST
Date: Thu, 14 Jan 93 23:41:48 EST
From: mcovingt@aisun3.ai.uga.edu (Michael Covington)
Message-Id: <9301150441.AA22675@aisun3.ai.uga.edu>
To: dg@ai.uga.edu
Subject: Test message

This is a test message to validate the entire DG mailing list.  We have just
upgraded our mailer to sendmail.mx and should have better success reaching
overseas addresses that had caused difficulty.  If you receive this message,
please ignore it; if you don't receive it, I'll be notified automatically.
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From mcovingt@aisun3.ai.uga.edu  Fri Jan 22 11:24:49 1993
Return-Path: <mcovingt@aisun3.ai.uga.edu>
Received: from aisun3.ai.uga.edu by aisun1.ai.uga.edu (4.1)
	id AA27546; Fri, 22 Jan 93 11:24:49 EST
Received: by aisun3.ai.uga.edu (4.1)
	id AA05757; Fri, 22 Jan 93 11:24:47 EST
From: mcovingt@aisun3.ai.uga.edu (Michael Covington)
Message-Id: <9301221624.AA05757@aisun3.ai.uga.edu>
Subject: Test message
To: dg@ai.uga.edu
Date: Fri, 22 Jan 93 11:24:47 EST
X-Mailer: ELM [version 2.3 PL11]

Test message; please ignore.

---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From mcovingt@aisun3.ai.uga.edu  Sun Jan 24 20:38:57 1993
Return-Path: <mcovingt@aisun3.ai.uga.edu>
Received: from aisun3.ai.uga.edu by aisun1.ai.uga.edu (4.1)
	id AA04520; Sun, 24 Jan 93 20:38:57 EST
Received: by aisun3.ai.uga.edu (4.1)
	id AA09560; Sun, 24 Jan 93 20:38:56 EST
From: mcovingt@aisun3.ai.uga.edu (Michael Covington)
Message-Id: <9301250138.AA09560@aisun3.ai.uga.edu>
Subject: Service upgrades on aisun1 (DG host)
To: dg@ai.uga.edu
Date: Sun, 24 Jan 93 20:38:55 EST
X-Mailer: ELM [version 2.3 PL11]

The mailer on aisun1.ai.uga.edu (the machine that hosts DG) has just been
upgraded, and we should have a much easier time reaching faraway addresses.
(Some of you may be hearing from us now for the first time!)

There will be further upgrades (with concomitant outages and temporary
malfunctions) during the next couple of weeks.
-- 
:-  Michael A. Covington     internet mcovingt@uga.cc.uga.edu :    *****
:-  Artificial Intelligence Programs       phone 706 542-0358 :  *********
:-  The University of Georgia                fax 706 542-0349 :   *  *  *
:-  Athens, Georgia 30602-7415 U.S.A.     amateur radio N4TMI :  ** *** **

---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From mcovingt@aisun3.ai.uga.edu  Tue Jan 26 22:57:52 1993
Return-Path: <mcovingt@aisun3.ai.uga.edu>
Received: from aisun3.ai.uga.edu by aisun1.ai.uga.edu (4.1)
	id AA25186; Tue, 26 Jan 93 22:57:52 EST
Received: by aisun3.ai.uga.edu (4.1)
	id AA13465; Tue, 26 Jan 93 22:57:50 EST
From: mcovingt@aisun3.ai.uga.edu (Michael Covington)
Message-Id: <9301270357.AA13465@aisun3.ai.uga.edu>
Subject: DG parsing vs PSG parsing
To: dg@ai.uga.edu
Date: Tue, 26 Jan 93 22:57:49 EST
X-Mailer: ELM [version 2.3 PL11]

Is there some useful mapping by which dependency grammars and D-trees
can be mapped onto phrase-structure grammars and PS-trees, thereby
generating dependency counterparts of well-known phrase-structure
parsing algorithms?

I can think of some trivial mappings that don't seem to shed any real
light on the issue.  Has anyone delived more deeply into this?
-- 
:-  Michael A. Covington     internet mcovingt@uga.cc.uga.edu :    *****
:-  Artificial Intelligence Programs       phone 706 542-0358 :  *********
:-  The University of Georgia                fax 706 542-0349 :   *  *  *
:-  Athens, Georgia 30602-7415 U.S.A.     amateur radio N4TMI :  ** *** **
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From rsharman@vnet.ibm.com  Wed Jan 27 05:00:39 1993
Return-Path: <rsharman@vnet.ibm.com>
Received: from vnet.ibm.com by aisun1.ai.uga.edu (4.1)
	id AA26561; Wed, 27 Jan 93 05:00:39 EST
Message-Id: <9301271000.AA26561@aisun1.ai.uga.edu>
Received: from WINVMD by vnet.ibm.com (IBM VM SMTP V2R2) with BSMTP id 9547;
   Wed, 27 Jan 93 04:58:35 EST
Date: Wed, 27 Jan 93 09:49:09 GMT
From: rsharman@vnet.ibm.com
To: dg@ai.uga.edu
Subject: Finding dependencies from Phrase Structures

 The correspondence between dependency relations and phrase structure
analyses is one of the most interesting questions in practical
syntactic analysis of real texts.  Avoiding the debate of  whether
dependencies are "fundamental" and phrases "derivative", or vice versa,
for the moment, it would a useful practical task to map one to the
other. In particular, there is a growing amount of text labelled
with phrase structures, which we call a "treebank". However I have
not seen the corresponding "dependency bank" anywhere. If someone has
one, or can annotate a given text with dependencies to create one,
then a preoper study could be sone of the correspondence between
dependencies and phrases IN THE SAME TEXT, and so a model which maps
between the two could be created.

 In default of this, it is necessary to resort to some kind of rule-based
approach, which is unfortunately beset by difficulties of interpretation,
since the underlying assumptions of the phrase structure analysis,
and probably also the assumptions of the dependency labelling, need to be
taken into account in specifying the rules.  It would, I feel, be
preferable to take an existing phrase labelled treebank, and do the
dependency analysis, then compare the two. This way we would learn
the actual mappings, as opposed to hypothetically assumed mappings?

Richard Sharman
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From LARSSON@ntcclu.ntc.nokia.com  Wed Jan 27 06:19:02 1993
Return-Path: <LARSSON@ntcclu.ntc.nokia.com>
Received: from NTC02.TELE.NOKIA.FI by aisun1.ai.uga.edu (4.1)
	id AA26691; Wed, 27 Jan 93 06:19:02 EST
Date:    Wed, 27 Jan 1993 13:19:52 +0300 (EET)
From: LARSSON@ntcclu.ntc.nokia.com
Message-Id: <930127131952.2581fc7d@ntcclu.ntc.nokia.com>
Subject: Mapping PS-structures onto D-structures
To: dg@ai.uga.edu
X-Vmsmail-To: INET::"dg@ai.uga.edu"



	In a recent posting Michael Covington asked:

Is there some useful mapping by which dependency grammars and D-trees
can be mapped onto phrase-structure grammars and PS-trees, thereby
generating dependency counterparts of well-known phrase-structure
parsing algorithms?

	This issue was treated by Robert F. Simmons in the book
	"Computations from the English" (Prentice-Hall; 1984.
	ISBN 0-13-164640-0). In chapter 4 (p. 57) he gives the following
	rules for transforming PS-structures into D-structures:

	"[...]
	VERBS govern preceding and following NPs, auxiliaries, VPs
	adverbials, and PPs that are not more immediately governed.
	NOUNS dominate articles, adjectives, modifying nouns, and
	PPs.
	ADJECTIVES dominate modifying adverbials.
	PREPOSITIONS are dominated by preceding nouns and verbs and
	dominate following nouns.
	[...]"

	In chapter 5, Simmons elaborates the rationale for doing
	these transformations and presents appropriate grammar
	rules (p. 89) to accomplish the task.

	The rules are DCG rules (called 'procedural grammar rules'
	by the author) intended for processing with HCPRVR, a Horn
	clause theorem prover written in Lisp (NOT Common Lisp).
	The grammar is claimed to be symmetric, thus allowing
	both parsing and generation.

I can think of some trivial mappings that don't seem t shed any real
[... additional text deleted]

	Maybe, the above rules are 'trivial' in a certain sense;
	they are also limited to languages, where the position of
	constituents (and their order) plays an important role (e.g.
	the Nordic languages, German, English). In Finnish, for example,
	ordering rules could partly be applied (i.e. WITHIN noun
	phrases), but decisions on which NPs are the 'subject' or
	the 'object' of a sentence must be made using other criteria
	(morphology etc.).

	In my present work concerning corpus-based methods in 
	translation studies (including the practice of translation
	and composition) using bi/multilingual corpora of technical
	texts, I am facing the problem of finding a working way of
	comparing structures in several languages. The requirement
	is that the structures (parse trees, lists or whatever)
	should themselvs be mutually comparable (and somehow
	compatible). D-structures with the finite verb as the root
	node seem to be a 'natural' representation.

	I would very much appreciate additional opinions, hints and
	deeper discussion of this issue on the DG list. Implementational
	details would also be welcome.

*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*
Arne Larsson			Nokia Telecommunications
Translator			Transmission Systems, Customer Services
larsson@ntc02.tele.nokia.fi	P.O. Box 12, SF-02611 Espoo, Finland
				Phone +358 0 5117476, Fax +358 0 51044287
*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*

---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From LARSSON@ntcclu.ntc.nokia.com  Wed Jan 27 06:43:02 1993
Return-Path: <LARSSON@ntcclu.ntc.nokia.com>
Received: from NTC02.TELE.NOKIA.FI by aisun1.ai.uga.edu (4.1)
	id AA26739; Wed, 27 Jan 93 06:43:02 EST
Date:    Wed, 27 Jan 1993 13:44:01 +0300 (EET)
From: LARSSON@ntcclu.ntc.nokia.com
Message-Id: <930127134401.2581fc7d@ntcclu.ntc.nokia.com>
Subject: DG and PSG
To: dg@ai.uga.edu
X-Vmsmail-To: INET::"dg@ai.uga.edu"

As a follow up to Richard Sharman:

Victor Sadler of B.S.O/Research (The Netherlands) has a detailed
treatment of a structured parallel corpus of bilingual text,
called the Bilingual Knowledge Bank (BKB) in his book "Working
with Analogical Semantics (Dordrecht 1989. ISBN 90 6765 428 0.)
The author proposes full alignment of dependency structures
(p. 127 ff).

At the time of writing there was a pilot BKB comprising "some
2,500 sentences" in English, French and Esperanto. It would be
very interesting to receive information about the present status 
of this project (the Distributed Language Translation project), 
provided such information is not company confidential.

*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*
Arne Larsson			Nokia Telecommunications
Translator			Transmission Systems, Customer Services
larsson@ntc02.tele.nokia.fi	P.O. Box 12, SF-02611 Espoo, Finland
				Phone +358 0 5117476, Fax +358 0 51044287
*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*

---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From ellalain@nuscc.nus.sg  Thu Jan 28 00:57:49 1993
Return-Path: <ellalain@nuscc.nus.sg>
Received: from nuscc.nus.sg by aisun1.ai.uga.edu (4.1)
	id AA03800; Thu, 28 Jan 93 00:57:49 EST
Received: by nuscc.nus.sg (5.65/1.34)
	id AA15246; Thu, 28 Jan 93 13:57:43 +0800
Date: Thu, 28 Jan 93 13:57:43 +0800
From: ellalain@nuscc.nus.sg (Alain Polguere)
Message-Id: <9301280557.AA15246@nuscc.nus.sg>
To: <dg@ai.uga.edu>
Subject: DG and Chinese


I have a colleague here who would like to know
if anything has been done on the description
of Chinese in a DG approach.

Any reference, etc. will be welcome. Thanks.

AP

---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From @mail-a.bcc.ac.uk:uclyrah@ucl.ac.uk  Thu Jan 28 03:08:47 1993
Return-Path: <@mail-a.bcc.ac.uk:uclyrah@ucl.ac.uk>
Received: from sun3.nsfnet-relay.ac.uk by aisun1.ai.uga.edu (4.1)
	id AA04099; Thu, 28 Jan 93 03:08:47 EST
Via: uk.ac.bcc.mail-a; Thu, 28 Jan 1993 08:02:17 +0000
Received: from link-1.ts.bcc.ac.uk by mail-a.bcc.ac.uk with SMTP (PP) 
          id <06259-0@mail-a.bcc.ac.uk>; Thu, 28 Jan 1993 08:08:30 +0000
Received: by link-1.ts.bcc.ac.uk (AIX 3.2/UCB 5.64/4.03) id AA71194;
          Thu, 28 Jan 1993 08:08:28 GMT
Message-Id: <9301280808.AA71194@link-1.ts.bcc.ac.uk>
To: dg@ai.uga.edu
Subject: converting PSG to DG
Date: Thu, 28 Jan 93 08:08:28 +0000
From: RichardHudson50 <uclyrah@ucl.ac.uk>


I spent a couple of weeks a few years ago trying to convert a PSG tree
bank into a DG one, 
and found that there were some insuperable problems when
working from the tree-bank itself - i.e. from the list of phrase-types that
were observed to occur in the corpus. The main problem is precisely that
if you know that some larger phrase contains a smaller phrase, you don't
know what kind of word that smaller phrase had as its head. E.g. if you
know that there were some PPs, consisting of a preposition followed by a
noun-phrase, you can't convert this into a dependency analysis in which a
preposition heads some word-class because you don't know what word-class 
headed each of the NPs. Of course you could assume simply that it was a
`noun', including proper nouns, common nouns, pronouns, gerunds, etc, to 
say nothing of coordinated NPs, but most people working on tree banks would
probably want to break down word-classes into more specific classes. It is
precisely because phrase-categories are so much less specific and revealing
than word-classes that a DG tree-bank is likely to be much more useful than
a PSG one, so it would be odd if it were easy to convert the latter into 
the former.

These problems shouldn't arise, however, if one were working from the text
itself, with PSG annotations added. In that case I feel sure it would be
quite easy at least in the vast majority of cases to replace the PSG phrase
tags with tags showing which words depended on which.


Dick Hudson
Dept of Phonetics and Linguistics,
University College London,
Gower Street,
London WC1E 6BT
(071) 387 7050 ext 3152

---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From LARSSON@ntcclu.ntc.nokia.com  Thu Jan 28 11:11:14 1993
Return-Path: <LARSSON@ntcclu.ntc.nokia.com>
Received: from NTC02.TELE.NOKIA.FI by aisun1.ai.uga.edu (4.1)
	id AA05308; Thu, 28 Jan 93 11:11:14 EST
Date:    Thu, 28 Jan 1993 18:12:12 +0300 (EET)
From: LARSSON@ntcclu.ntc.nokia.com
Message-Id: <930128181212.2582261f@ntcclu.ntc.nokia.com>
Subject: cu-Prolog available
To: dg@ai.uga.edu
X-Vmsmail-To: INET::"dg@ai.uga.edu"

Dear Colleagues,

It may be of interest to the members of this group to
know that MAC and MS-DOS versions of cu-Prolog from 
ICOT is now available using ftp from csli.stanford.edu
in the directory pub/MacCup. 

Arne Larsson

---------Here is the readme file for your information:-----------

Files:

MacCupE0.78w.sit.hqx   program, Bin Hexed and Stuff It-ed
djcup.lzh              MS-DOS program, running under DOS-extender
                        (386/486 cpu), compressed by LHA
manual.tex             manual for older version of MacCup
sample.p               sample program a la JPSG
util.p                 utility program

------------------------------------------------------------
Note:
MacCup is Macintosh version of CU-Prolog
DJCup is MS-DOS version of CU-Prolog compiled with DJ's gcc.
CU-Prolog has been developed at ICOT, Japan.
------------------------------------------------------------
Brief Introduction to MacCup:

  MacCup is a Macintosh version of CUP, Constraint Unification
Prolog, whose original version has been developed by ICOT.
  It looks like a usual Prolog system.  However there are
several differences:
  (1) use "<file name>" to load a program from
a file instead of using [<file name>].
  (2) input program directly from keyboard after `_' prompt
instead of using [console].
  (3) use %d* to list defined predicates instead of
:-listing.
  etc. etc.

To get MacCup from MacCupXXX.sit.hqx:
   Load (or FTP) to your Macintosh, use BinHex to
decode it to get a StuffIt archiver form file.
Then use StuffIt to get MacCup program.

To run MacCup:
   Click twice the MacCup icon, and wait about
10 seconds.  You will get MacCup window, and the following
message:

   ********   MacCup Ver. XXX   ********
    All Modular mode (help -> %h)

To quit:
   Input %Q, or :-halt., or Command-Q.

To load MacCup Programs:
   Click FILE menu and OPEN item (or use Command-O), select
the program file, and OPEN it.

To get command lists:
   Input %h.
MacCup has several `commands' which are marked by `%' as in %h.

Note. 
1. There are several features which are not mentioned
in the manual.
As I am writing a new manual, you can get it in September.

2. Thanks to Dr. Emele,  I found a bug in MacCupV0.63, and
made MacCupV0.70.

The difference between them is that  the latter will deal with
PSTs in constraint transformation better than the former.

For example, 
p({c/C}) :- ab(C).
q({d/D,c/{a/D}}).
ab({a/1,b/0}).
ab({a/0,b/1}).

@ p(X),q(X).
will produce different results in V0.63 and V0.70.

HOWEVER, MacCup still returns incorrect answer for the
following:

ab({a/1,b/0}).
ab({a/0,b/1}).
r({c/C});ab(C).
s({d/D,c/X});X={a/D}.

:-r(X),s(X).

Frankly speaking, there is a trade-off between efficiency and
getting correct/strict answers.

MacCup078S takes efficiency, and MacCup078X takes correctness.
(MacCup078X is not distributed for publicity.)

3. You can interrupt MacCup by pressing COMMAND-`.'.
If you click the Abort button, the interrupted process
will be aborted.


I'd appreciate if you give any comments and/or suggestions.
------------------------------------------------------------
Hidetosi SIRAI
sirai@csli.stanford.edu, or sirai@sccs.chukyo-u.ac.jp
Chukyo University


---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From shimizu@hakobera.isct.kyutech.ac.jp  Thu Jan 28 23:15:52 1993
Received: from hiko.isct.kyutech.ac.jp by aisun1.ai.uga.edu (4.1)
	id AA12098; Thu, 28 Jan 93 23:15:52 EST
From: shimizu@hakobera.isct.kyutech.ac.jp
Received: by hiko.isct.kyutech.ac.jp (5.65/6.4J.6)
	id AA07469; Fri, 29 Jan 93 13:17:17 +0900
Received: by hakobera.isct.kyutech.ac.jp (5.65/6.4J.6)
	id AA16379; Fri, 29 Jan 93 13:26:58 +0900
Return-Path: <shimizu@hakobera.isct.kyutech.ac.jp>
Message-Id: <9301290426.AA16379@hakobera.isct.kyutech.ac.jp>
To: dg@ai.uga.edu
Cc: shimizu@hakobera.isct.kyutech.ac.jp
Subject: more than him have
Date: Fri, 29 Jan 93 13:26:57 +0900



The following is the passage I found in a novel called
_The Graduate_;

'How many people have done this?'
'Proposed to me?'
'Yes.'
'I don't know,' she said.
'You mean more than him have?'

Since I am not a native speaker of English, I am not certain
about the grammarticality (or acceptability) of the last
sentence (or utterance).  Will anyone tell me your opinion?
I also would like to know how you would analyse it.

\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
Makoto SHIMIZU
Kyushu Institute of Technology
shimizu@hakobera.isct.kyutech.ac.jp
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
"I guess I've lost another pupil," said the 
professor as his glass eye rolled down the sink.


---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From rambow@unagi.cis.upenn.edu  Sun Jan 31 22:33:18 1993
Received: from linc.cis.upenn.edu by aisun1.ai.uga.edu (4.1)
	id AA29501; Sun, 31 Jan 93 22:33:18 EST
Received: from UNAGI.CIS.UPENN.EDU by linc.cis.upenn.edu
	id AA25689; Sun, 31 Jan 93 22:33:16 -0500
Return-Path: <rambow@unagi.cis.upenn.edu>
Received: by unagi.cis.upenn.edu
	id AA17609; Sun, 31 Jan 93 22:33:15 EST
Date: Sun, 31 Jan 93 22:33:15 EST
From: rambow@unagi.cis.upenn.edu (Owen Rambow)
Posted-Date: Sun, 31 Jan 93 22:33:15 EST
Message-Id: <9302010333.AA17609@unagi.cis.upenn.edu>
To: mcovingt@aisun3.ai.uga.edu
Cc: dg@ai.uga.edu
In-Reply-To: Michael Covington's message of Tue, 26 Jan 93 22:57:49 EST <9301270357.AA13465@aisun3.ai.uga.edu>
Subject: DG parsing vs PSG parsing


Michael Covington some days ago asked about mapping DGs onto PSGs in order
to profit from parsing algorithms for PSGs.  Several answers, if I have
understood them correctly, have addressed the inverse problem, of mapping
PSGs (and PS-trees) onto DGs (and D-trees).  Thus, the question of how to
map a DG onto a PSG (not simply a D-tree onto a PS-tree) remains open.

One of the "trivial mappings" that Michael Covington mentions is the
following, I assume: each lexical head L is mapped to a phrase-structure
rule of the form LP -> (X1)P ... (Xn)P L (Y1)P ... (Yn)P, where the Xi and
Yi are the lexical categories of the dependents of L, in the right order.
Optional dependents require additional rules.  The result is a CFG, which
can be parsed using the usual instrumentarium for CFG-parsing (CYK, Early,
LR etc.).

The problem is that in the case of adjuncts, there may be an unbounded
number of dependents, and thus we would need an unbounded number of PS
rules, which would not yield a CFG.  Evidence from English shows that it is
impossible, in general, to write a linguistically plausible CFG with rules
of the form given above for a (necessarily finite) CF grammar.  The problem
is the issue of "lexicalization": in the above form, every rule of the CF
grammar is associated with 1 lexical item (thus allowing the easy mapping
between DG and PSG).  This is not the case for the common CFGs for English.
However, it has been shown (Schabes 1989) that in general, CFGs cannot be
transferred into a lexicalized form.

One way to lexicalize a CFG is to increase the formal power of the
underlying mathematical system, and to go to Tree Adjoining Grammars (TAG).
In TAG, grammars consist of trees, which are combined to yield derived
structures.  In a recent paper, Aravind Joshi and I suggest that the fact
that TAGs can be lexicalized makes them an ideal interface between PS-based
grammars and DGs.  So Michael Covington's question could be answered as
follows: to apply parsing algorithms for PSGs to DGs, first devise a TAG
for the language.  A TAG parse can then easily be converted into a DG
analysis.  Much research has been done on TAG parsing; Early-type
algorithms run in O(n^6) time worst case.  

Writing a TAG for a language is not, alas, a trivial matter.  If done in a
principled manner, it amounts to devising a theory of syntax.

The connection between TAG and DG can also be exploited in the inverse: in
a different paper, we propose to use a dependency parser to derive
PS-trees.  The dependency parser can be "compiled down" from a LR(0)
version of a TAG parser.

Owen Rambow
rambow@unagi.cis.upenn.edu
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From rambow@unagi.cis.upenn.edu  Sun Jan 31 22:43:49 1993
Received: from linc.cis.upenn.edu by aisun1.ai.uga.edu (4.1)
	id AA29530; Sun, 31 Jan 93 22:43:49 EST
Received: from UNAGI.CIS.UPENN.EDU by linc.cis.upenn.edu
	id AA25912; Sun, 31 Jan 93 22:43:45 -0500
Return-Path: <rambow@unagi.cis.upenn.edu>
Received: by unagi.cis.upenn.edu
	id AA17703; Sun, 31 Jan 93 22:43:44 EST
Date: Sun, 31 Jan 93 22:43:44 EST
From: rambow@unagi.cis.upenn.edu (Owen Rambow)
Posted-Date: Sun, 31 Jan 93 22:43:44 EST
Message-Id: <9302010343.AA17703@unagi.cis.upenn.edu>
To: uclyrah@ucl.ac.uk
Cc: dg@ai.uga.edu
In-Reply-To: RichardHudson50's message of Thu, 28 Jan 93 08:08:28 +0000 <9301280808.AA71194@link-1.ts.bcc.ac.uk>
Subject: converting PS corpora to D-corpora


Automatically converting the Penn Tree Bank, a PS-corpus, to include
D-annotations would be impossible.  This is because in sentences like
"Yesterday, John ate", "yesterday" is tagged as an NP, and the automatic
converter would not be able to determine if it is a temporal adjunct or the
direct object.  I am not quite sure if this is the same problem Dick Hudson
mentioned.

Tony Kroch has a project at the U. of Penn to annotate Old and Middle
English texts.  Though I don't think the representation will actually be a
D-notation, it will include sufficient information to unambiguously derive
a D-representation (as far as I know).

Owen Rambow
rambow@unagi.cis.upenn.edu
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From Daniel.Sleator@SPADE.PC.CS.CMU.EDU  Sun Jan 31 23:57:40 1993
Return-Path: <Daniel.Sleator@SPADE.PC.CS.CMU.EDU>
Received: from SPADE.PC.CS.CMU.EDU by aisun1.ai.uga.edu (4.1)
	id AA00104; Sun, 31 Jan 93 23:57:40 EST
Message-Id: <9302010457.AA00104@aisun1.ai.uga.edu>
From: Daniel Sleator <sleator@SPADE.PC.CS.CMU.EDU>
Date: Sun, 31 Jan 93 23:56:34 EST
To: dg@ai.uga.edu
Cc: Davy Temperley <dt3@cunixa.cc.columbia.edu>
Subject: DG parsing, PSG parsing, and link parsing

My question to Michael Covington is: Why not parse directly in the
dependency formalism?  This is what Davy Temperley have done in our
link grammar system.

Link grammar is a lexically-based dependency-type formalism in which
we have written a large and wide coverage English grammar, and for
which we have invented and implemented efficient parsing algorithms.
Not only that, but the entire system is available via anonymous ftp.

 (The anonymous ftp site is "spade.pc.cs.cmu.edu", and the directory is
 "usr/sleator/public", to which you must move with one "cd" command
 after entering ftp anonymously.)

Over 500 people have taken the system.  Link grammars are being
developed for other languages, as is a commercial grammar checker.
Link grammars have also been proposed as a grammatical alternative to
the trigram model.

For the moment, I'll leave a detailed description of our work to our
tech reports (also available via anonymous ftp), and just give some
examples.

As a very superficial illustration, when our system is given the
following NY Times sentence: "An increased supply of civilian goods
can restrain our inflation, increase employment, improve public
budgets and enhance exports."  it constructs the following parsing in
three seconds on a decstation 3100.

                                                  +------------------
                     +--------------S-------------+        +---------
   +--------D--------+     +-------J-------+      |        +-------O-
   |      +-----A----+--M--+      +----A---+      |        |      +--
   |      |          |     |      |        |      |        |      |  
  an increased.v supply.n of civilian.a goods.n can.v restrain.v our 


  --------------------------I-------------------------------------------+  
  ------------+                         +-------------------------------+  
  -----+      +-------------------------+     +---------O--------+      |  
  -D---+      +------+-----O-----+      +-----+         +----A---+      +--
       |      |      |           |      |     |         |        |      |  
  inflation.n , increase.v employment.n , improve.v public.a budgets.n and 


  ----+----O----+
      |         |
  enhance.v exports.n 

Every word in our system has a formula that describes the requirements
which that word imposes on any sentence containing it (actually, it
dictates what the combination of links which are required to connect
to that word in order to satisfy it.)  For example, the word
"civilian" (as an adjective) has the following definition in our
system:

   {Ea- or Eb+} & (A+ or ((AI- or IX+ or Ma- or AA+) & {@EV+}));

Each word has a similar definition that completely describes the way
that word can be used.

                         Daniel Sleator
                         Carnegie Mellon University
                         School of Computer Science
                         412-268-7563
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From mcovingt@aisun3.ai.uga.edu  Mon Feb  1 00:23:42 1993
Return-Path: <mcovingt@aisun3.ai.uga.edu>
Received: from aisun3.ai.uga.edu by aisun1.ai.uga.edu (4.1)
	id AA00209; Mon, 1 Feb 93 00:23:42 EST
Received: by aisun3.ai.uga.edu (4.1)
	id AA21720; Mon, 1 Feb 93 00:23:38 EST
From: mcovingt@aisun3.ai.uga.edu (Michael Covington)
Message-Id: <9302010523.AA21720@aisun3.ai.uga.edu>
Subject: Dependency parsing
To: dg@ai.uga.edu
Date: Mon, 1 Feb 93 0:23:38 EST
X-Mailer: ELM [version 2.3 PL11]

With regard to Daniel Sleator's challenge, "Why not parse directly
in the dependency system?" my answer is -- I do!  The reason I asked
about mappings of dependency grammars onto PS-grammars was as one
strategy for doing complexity analysis.  As someone else pointed out,
it is not necessarily a very promising one, because a PS-rule handles
a head and all its arguments (at a particular X-bar level) at once,
whereas a D-rule merely says there is *one* argument and doesn't say
how many others there may be.

Thanks to all for the interesting discussion.  I'll be working through
people's postings and replying in greater depth soon.

(And apologies in advance for any outages that our machine may experience
in the next month.  A major operating system upgrade is coming.)
-- 
:-  Michael A. Covington     internet mcovingt@uga.cc.uga.edu :    *****
:-  Artificial Intelligence Programs       phone 706 542-0358 :  *********
:-  The University of Georgia                fax 706 542-0349 :   *  *  *
:-  Athens, Georgia 30602-7415 U.S.A.     amateur radio N4TMI :  ** *** **
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From @mail-a.bcc.ac.uk:uclyrah@ucl.ac.uk  Mon Feb  1 03:34:59 1993
Return-Path: <@mail-a.bcc.ac.uk:uclyrah@ucl.ac.uk>
Received: from sun3.nsfnet-relay.ac.uk by aisun1.ai.uga.edu (4.1)
	id AA01114; Mon, 1 Feb 93 03:34:59 EST
Via: uk.ac.bcc.mail-a; Mon, 1 Feb 1993 08:23:02 +0000
Received: from link-1.ts.bcc.ac.uk by mail-a.bcc.ac.uk with SMTP (PP) 
          id <00468-0@mail-a.bcc.ac.uk>; Mon, 1 Feb 1993 08:34:01 +0000
Received: by link-1.ts.bcc.ac.uk (AIX 3.2/UCB 5.64/4.03) id AA20410;
          Mon, 1 Feb 1993 08:33:59 GMT
Message-Id: <9302010833.AA20410@link-1.ts.bcc.ac.uk>
To: dg@ai.uga.edu
Subject: converting PS corpora to D-corpora
Date: Mon, 01 Feb 93 08:33:58 +0000
From: RichardHudson50 <uclyrah@ucl.ac.uk>


Owen Rambow says that sentences like (1) can't be automatically reananlysed
from a Penn-style PS analysis into a D-analysis.
(1) Yesterday John ate.
This is because "yesterday" is just labelled as NP, so it would have the same
analysis, presumably, as (2).
(2) Beans John ate.
If you're only looking for semantically-interpretable dependencies, Owen is
right; but it's possible to aim in the first instance at least only at the
most superficial "skeleton" of dependencies, on which the rest of the depend-
ency structure is built. At this level of abstraction, (1) and (2) are the
same - they both contain some kind of "extractee" which has a (surface, but
not semantically relevant) dependency on "ate". 

Interesting to hear about Tony Kroch's Old and Middle English corpus. 



Dick Hudson
Dept of Phonetics and Linguistics,
University College London,
Gower Street,
London WC1E 6BT
(071) 387 7050 ext 3152

---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From shimizu@hakobera.isct.kyutech.ac.jp  Thu Feb  4 00:01:38 1993
Received: from hiko.isct.kyutech.ac.jp ([150.69.2.6]) by aisun1.ai.uga.edu (4.1)
	id AA25281; Thu, 4 Feb 93 00:01:38 EST
From: shimizu@hakobera.isct.kyutech.ac.jp
Received: by hiko.isct.kyutech.ac.jp (5.65/6.4J.6)
	id AA02750; Thu, 4 Feb 93 14:03:21 +0900
Received: by hakobera.isct.kyutech.ac.jp (5.65/6.4J.6)
	id AA14876; Thu, 4 Feb 93 14:01:24 +0900
Return-Path: <shimizu@hakobera.isct.kyutech.ac.jp>
Message-Id: <9302040501.AA14876@hakobera.isct.kyutech.ac.jp>
To: dg@ai.uga.edu
Cc: shimizu@hakobera.isct.kyutech.ac.jp
Subject: Category Conversion
Date: Thu, 04 Feb 93 14:01:23 +0900


I posted to sci.lang a writing two years ago,
but I didn't receive many responses, so I would like
to ask you the same question.

================================================
Hi, everybody.

I've been interested in the discrepancy between the form
and the function of linguistic expressions lately, such as 
the examples in (1) noted by Quirk et. al.(1985:658):

     (1)  a. A: When are we going to have the next meeting?
             B: {On Tuesday /In March/ During the vacation/
                Between 6 and 7} {will be fine/ suits me/
                is what we decided/ may be convenient}.
          b. He picked up the gun {from under the table/
             from behind the curtain}.
          c. We didn't meet until after the show.
          d. Food has been scarce since before the war.
          e. The weather has been fine except in the north.

In (1a), for example, the distribution of the PP 'on Tuesday',
is in the position of an NP.  

I have collected some examples myself.

     (2) a. Now that most of them have first-hand
           experience with computers, they approach computer
           applications without fear or superstition and
           with considerable understanding of how computers
           can serve man kind.
          b. At that time, a lot of American companies were
           looking for clever young scientists from abroad.

In (2b), we see the preposition 'from', which usually requires 
an NP as its object, takes  the lexical item 'abroad', which I 
would presume no dictionary.  When we interpret this sentence, 
I suppose we anyhow regard 'abroad' as functioning as an NP.

=====================================================

A few people responded to me.  Rick Wojcik (rwojcik@bcsaic.UUCP)
gave me interesting examples;

	(3) Between 45 minutes and an hour will elapse before
	    he turns.
	(4) The plan requires from six inches to a foot (of
	    space) here.

In (3) a PP is treated as a subject,and in (4) as an object.

As far as I know, not many people have discussed this phenomenon.

Jackendoff(1973) deals with the issue throughly, but I don't think 
most of us would regard his treatment as satisfactory because
the PS rule in the paper is highly idiosyncratic.  I suppose
he would resort to his correspondence rule now,but I'm not
certain exactly how.

In Grimshaw(1982), if I understand her correctly, she changes functional 
schmata and assign SUBJ and OBJ to French clitics (If my memory is
correct.  Sorry, I don't have it here right now.)  But she does not 
(does she?) discuss the English language, I'm not sure what to do with  
the examples I put above. 

I would like to ask you how you would analyse these examples.
Any comments and opinions are welcome.  I will summarize
the responses.

I've got another question to ask.  I'm thinking of the 
possibility to rewrite, say, an PP into an NP with a rule 
similar to Meta Rule in GPSG, but someone told me Meta Rule 
is not popular these days.  But I'm not sure why.  Could
anyone direct me to the relevant literature, please?

\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
Makoto SHIMIZU
Kyushu Institute of Technology
shimizu@hakobera.isct.kyutech.ac.jp
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
What did the cannibal say to the captured missionary?
"We would like very much to have you for dinner."


---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From mcovingt@aisun3.ai.uga.edu  Thu Feb  4 11:20:43 1993
Return-Path: <mcovingt@aisun3.ai.uga.edu>
Received: from aisun3.ai.uga.edu by aisun1.ai.uga.edu (4.1)
	id AA27424; Thu, 4 Feb 93 11:20:43 EST
Received: by aisun3.ai.uga.edu (4.1)
	id AA28156; Thu, 4 Feb 93 11:20:39 EST
From: mcovingt@aisun3.ai.uga.edu (Michael Covington)
Message-Id: <9302041620.AA28156@aisun3.ai.uga.edu>
Subject: Address needed
To: dg@ai.uga.edu
Date: Thu, 4 Feb 93 11:20:39 EST
X-Mailer: ELM [version 2.3 PL11]

Does anyone have the email address of Michel Eytan (Strasbourg, France)?
The address that I have on the DG list is eytan@dpt-info.u-strasbrg.fr,
and isn't working.  Thanks.
-- 
:-  Michael A. Covington     internet mcovingt@uga.cc.uga.edu :    *****
:-  Artificial Intelligence Programs       phone 706 542-0358 :  *********
:-  The University of Georgia                fax 706 542-0349 :   *  *  *
:-  Athens, Georgia 30602-7415 U.S.A.     amateur radio N4TMI :  ** *** **
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From @mail-a.bcc.ac.uk:ucleaar@ucl.ac.uk  Thu Feb  4 16:48:52 1993
Return-Path: <@mail-a.bcc.ac.uk:ucleaar@ucl.ac.uk>
Received: from sun2.nsfnet-relay.ac.uk by aisun1.ai.uga.edu (4.1)
	id AA00465; Thu, 4 Feb 93 16:48:52 EST
Via: uk.ac.bcc.mail-a; Thu, 4 Feb 1993 21:39:14 +0000
Received: from link-1.ts.bcc.ac.uk by mail-a.bcc.ac.uk with SMTP (PP) 
          id <21484-0@mail-a.bcc.ac.uk>; Thu, 4 Feb 1993 21:39:03 +0000
Received: by link-1.ts.bcc.ac.uk (AIX 3.2/UCB 5.64/4.03) id AA11542;
          Thu, 4 Feb 1993 21:39:01 GMT
Message-Id: <9302042139.AA11542@link-1.ts.bcc.ac.uk>
To: dg@ai.uga.edu, Daniel Sleator <sleator@spade.pc.cs.cmu.edu>
X-Ungarbled_Sender: And Rosta <ucleaar@ucl.ac.uk>
Subject: coordination
Date: Thu, 04 Feb 93 21:38:59 +0000
From: And RostaBM-850 <ucleaar@ucl.ac.uk>


From Daniel Sleator's Link Grammar diagram I see that in 
coordinate structures the conjunction is the head of the
conjuncts (or, at least, all conjuncts are subordinate
to (indirectly depend on) the conjunction).
This approach contrasts with that of Word Grammar, where
coordination is the only area of syntax that involves
constituency. An example:

       <--s--<
     Sophy swam

           <-------------<----s----<          <<< this is a shared
     { [ Sophy ] and [ Edgar ] } swam             dependency

I find both approaches appealing. Has anyone done any work on
weighing up the pros and cons of each?

----
And Rosta.

---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From mahue@linguistik.uni-erlangen.de  Sun Feb 14 12:26:47 1993
Return-Path: <mahue@linguistik.uni-erlangen.de>
Received: from faui45.informatik.uni-erlangen.de by aisun1.ai.uga.edu (4.1)
	id AA10130; Sun, 14 Feb 93 12:26:47 EST
Received: from uranus.linguistik.uni-erlangen.de by uni-erlangen.de with SMTP;
	id AA21931 (5.65c-5/7.3r-FAU); Sun, 14 Feb 1993 18:26:39 +0100
Received: by linguistik.uni-erlangen.de  (16.7/7.3h-FAU)
	id AA22152; Sun, 14 Feb 93 18:26:37 +0100
Date: Sun, 14 Feb 93 18:26:37 +0100
From: Marc Huesken <mahue@linguistik.uni-erlangen.de>
Message-Id: <9302141726.AA22152@linguistik.uni-erlangen.de>
To: dg@ai.uga.edu
Subject: Subscribe

Please add me to the list.

----------------------------------+-----------------------------------
Marc Huesken             Ei joh!  | CLD University of Erlangen Germany
mahue@linguistik.uni-erlangen.de  | 09131/85-9252   priv.: 09131/67209
======================================================================
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From mcovingt@aisun3.ai.uga.edu  Tue Feb 23 00:13:35 1993
Return-Path: <mcovingt@aisun3.ai.uga.edu>
Received: from aisun3.ai.uga.edu by aisun1.ai.uga.edu (4.1)
	id AA05512; Tue, 23 Feb 93 00:13:35 EST
Received: by aisun3.ai.uga.edu (4.1)
	id AA12906; Tue, 23 Feb 93 00:13:33 EST
From: mcovingt@aisun3.ai.uga.edu (Michael Covington)
Message-Id: <9302230513.AA12906@aisun3.ai.uga.edu>
Subject: Possible service interruption
To: dg@ai.uga.edu
Date: Tue, 23 Feb 1993 00:13:33 -0500 (EST)
X-Mailer: ELM [version 2.4 PL20]
Mime-Version: 1.0
Content-Type: text/plain; charset=US-ASCII
Content-Transfer-Encoding: 7bit
Content-Length: 537       

The machine that runs the DG mailing list will be undergoing major
upgrades this week, and there will be periods when messages posted to
DG may be substantially delayed.  Please let me know of any problems,
especially if they persist.
-- 
:-  Michael A. Covington     internet mcovingt@uga.cc.uga.edu :    *****
:-  Artificial Intelligence Programs       phone 706 542-0358 :  *********
:-  The University of Georgia                fax 706 542-0349 :   *  *  *
:-  Athens, Georgia 30602-7415 U.S.A.     amateur radio N4TMI :  ** *** **

---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From murzaku@ux1sns.sns.it  Tue Feb 23 07:19:39 1993
Return-Path: <murzaku@ux1sns.sns.it>
Received: from ux1sns.sns.it by aisun1.ai.uga.edu (4.1)
	id AA06343; Tue, 23 Feb 93 07:19:39 EST
Received: by ux1sns.sns.it (5.57/Ultrix3.0-C)
	id AA14354; Tue, 23 Feb 93 13:09:04 GMT
Date: Tue, 23 Feb 93 13:09:04 GMT
From: murzaku@ux1sns.sns.it (Aleksander Murzaku)
Message-Id: <9302231309.AA14354@ux1sns.sns.it>
To: dg@ai.uga.edu
Subject: Syntax checkers


Do you know anything about some approach made by someone and concerning the
use of DG in the construction of syntax checkers. This can be very useful,
I think, especially for the free word order languages such as Russian, Albanian
etc. But these languages, have also a very reach flectional system. So there
must be added a special morphological parser. 
Is there anyone working on such languages and such problems?

Thanks s in advance,
Aleksander Murzaku

Scuola Normale Superiore				      tel.+39/50/597111
Piazza dei Cavalieri 7				          murzaku@ux1sns.sns.it
56126 PISA - Italy					    murzaku at ipisnsib

Home adress:    Viale Amelia 15  //  00181 ROMA - Italy  //  Tel. +39/6/7806295
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From mcovingt@aisun3.ai.uga.edu  Wed Feb 24 19:09:21 1993
Return-Path: <mcovingt@aisun3.ai.uga.edu>
Received: from aisun3.ai.uga.edu by aisun1.ai.uga.edu (4.1)
	id AA00556; Wed, 24 Feb 93 19:09:21 EST
Received: by aisun3.ai.uga.edu (4.1)
	id AA00361; Wed, 24 Feb 93 19:09:21 EST
From: mcovingt@aisun3.ai.uga.edu (Michael Covington)
Message-Id: <9302250009.AA00361@aisun3.ai.uga.edu>
Subject: Czech dependency grammar (Resent by M. Covington)
To: dg@ai.uga.edu
Date: Wed, 24 Feb 1993 19:09:20 -0500 (EST)
X-Mailer: ELM [version 2.4 PL20]
Mime-Version: 1.0
Content-Type: text/plain; charset=US-ASCII
Content-Transfer-Encoding: 7bit
Content-Length: 2757      

Forwarded message:
From @UGA.CC.UGA.EDU:SGALL@CSPGUK11.BITNET Wed Feb 24 11:26:57 1993
Message-Id: <9302241626.AA00894@aisun3.ai.uga.edu>
Date:         Wed, 24 Feb 93 17:11:19 MET
From: Petr Sgall <SGALL%CSPGUK11.BITNET@uga.cc.uga.edu>
Subject:      Re: Possible service interruption
To: Michael Covington <mcovingt@aisun3.ai.uga.edu>
In-Reply-To:  Your message of Tue, 23 Feb 1993 00:13:33 -0500 (EST)

Dear Colleague,
                 I was not successful in using the shorter address,
but I would be grateful if you can distribute this message in the
group.
      I have just a few remarks to the ongoing discussion(s):
In Czech linguistics the experience with dependency syntax is rather
rich, since a highly detailed and consistentversion has been used here
even in school grammars for decades, with success, and has been reformulated
in our research group in the shape of a formal framework - a synthetic
discussion was published by Sgall, Hajicova and Panevova: The meaning of
the sentence in its semantic and pragmatic aspects, ed. by J.Mey, Dordrecht,
Holl.:Reidel, 1986. There you find, among other issues, a brief analysis
of valency of verbs and nouns.
        Function words (articles, prepositions, conjunctions, aux.verb forms)
are handled here, in accordance with the older tradition, just as indices
or parts of the lexical occurrences (node labels), since there is no need
to include them as labels of specific nodes; e.g. such groups as 'from the
window' or 'since...has been written' are equivalent to word forms with affixes
(endings) in many languages; neither the gramatical affixes nor function words
are syntactically free enough to require specific nodes in dependency trees.
        Our group now also works on a dependency-based grammar checker.
        Be so kind and include the following two membres of our group (i.e
the research group of formal and computational linguistics at Charles Univ.,
Prague) into your e-mail net:
Petkevic@Praha1.ff.cuni.cs (internet) - V.Petkevic, specialist in issues of
             underlying representations in DG;
Rosen@Praha1.ff.cuni.cs - A. Rosen, working in parsing English for MT etc.
                            Best thanks and regards       Petr Sgall
>To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
>---------------------------------------------------------------------------
>---------------------------------------------------------------------------


-- 
:-  Michael A. Covington     internet mcovingt@uga.cc.uga.edu :    *****
:-  Artificial Intelligence Programs       phone 706 542-0358 :  *********
:-  The University of Georgia                fax 706 542-0349 :   *  *  *
:-  Athens, Georgia 30602-7415 U.S.A.     amateur radio N4TMI :  ** *** **
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From mjkim@hyowon.pusan.ac.kr Wed Mar 10 16:41:04 1993
Received: from garam.kreonet.re.kr by aisun1.ai.uga.edu with SMTP id AA21918
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Wed, 10 Mar 1993 02:44:11 -0500
Received: from hyowon.pusan.ac.kr by garam.kreonet.re.kr (4.1/GARAM-MX-1.0)
	id AA10444; Wed, 10 Mar 93 16:54:06 KST
Received: by hyowon.pusan.ac.kr (AIX 3.2/UCB 5.64/hyowon-1.0)
          id AA19363; Wed, 10 Mar 1993 16:41:04 GMT
Date: Wed, 10 Mar 1993 16:41:04 GMT
From: mjkim@hyowon.pusan.ac.kr (Minjung Kim )
Message-Id: <9303101641.AA19363@hyowon.pusan.ac.kr>
To: dg@ai.uga.edu
Subject: subscribe


subscribe dg mjkim@hyowon.pusan.ac.kr
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From shimizu@hakobera.isct.kyutech.ac.jp Sat Mar 13 03:06:06 1993
Received: from hiko.isct.kyutech.ac.jp ([150.69.2.6]) by aisun1.ai.uga.edu with SMTP id AA17303
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Sat, 13 Mar 1993 03:06:06 -0500
Received: by hiko.isct.kyutech.ac.jp (5.65/6.4J.6)
	id AA16390; Sat, 13 Mar 93 17:06:40 +0900
From: <shimizu@hakobera.isct.kyutech.ac.jp>
Received: by hakobera.isct.kyutech.ac.jp (5.65/6.4J.6)
	id AA12327; Sat, 13 Mar 93 17:05:54 +0900
Return-Path: <shimizu@hakobera.isct.kyutech.ac.jp>
Message-Id: <9303130805.AA12327@hakobera.isct.kyutech.ac.jp>
To: dg@ai.uga.edu
Cc: shimizu@hakobera.isct.kyutech.ac.jp
Subject: Summary:Category Conversion
Date: Sat, 13 Mar 93 17:05:53 +0900


Sorry, it's kind of late, but here's the summary
of the responses I got to the question I asked you 
about cateory conversion.  Many thanks to those who
responded.

Ron Kuzar <SOUKR@HUJIVM1.BITNET> points out that the number 
of prepositions which can function as noun modifiers without
changing its status as NP is limited.  I suppose it is an
important observation, but what I'm interested in right now
is PP functioning like NP rather than PP modifying NP.

Laurie.Bauer@vuw.ac.nz asks me if what I'm interested in is
Halliday's rank shift.  Partially, if my understanding is 
correct.  I understand his rank shift means embedding such
as occurring as a sentential subject.  I am certainly 
interested in such a case.  However, it's not the only
type I'm interested in.  PP functioning as NP, for instance,
if I am correct, is not treated in Hallidayan grammar.
Laurie also asks if I am concerned with instances like

A:  Is Lyons's lecture today?
B: No, Lyons is Wednesday.

At first, I thought it was an example of metonymy and it was
not relevant here, but come to think of it, it does seem
related to the phenomena I'm interested.

Bruce E. Nevin" <bnevin@ccb.bbn.com> was so nice to write
me and told me in detail how he would treat the examples.
He regards the head of the PP is present as a zero
allomorph, and according to him, there is
not discrepancy between form and function.  The following 
is an example of his analysis;

>     (1)  a. A: When are we going to have the next meeting?
>             B: {On Tuesday /In March/ During the vacation/
>                Between 6 and 7} {will be fine/ suits me/
>                is what we decided/ may be convenient}.

[[Our] having the next meeting] on Tuesday (etc.) will be fine (etc.)

Although I wonder exactly how one could recover the expression from
a sentence with a zero allomorph, it is a very interesting approach.

Thank you again, Ron, Laurie, Bruce.  I really appreciate your
cooperation.

\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
Makoto SHIMIZU
Kyushu Institute of Technology
shimizu@hakobera.isct.kyutech.ac.jp
\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\
Well, I'm runnin' down the road
Tryin' to loosen my load
I've got seven women on my mind
Four that wanna own me
Two that wanna stone me
One says she's a friend of mine
Eagles   Take it Easy






---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From MCOVINGT@UGA.CC.UGA.EDU Sun Mar 14 12:27:48 1993
Received: from uga.cc.uga.edu by aisun1.ai.uga.edu with SMTP id AA27194
  (5.65c/IDA-1.5 for <dg@AI.UGA.EDU>); Sun, 14 Mar 1993 20:15:17 -0500
Message-Id: <199303150115.AA27194@aisun1.ai.uga.edu>
Received: from UGA.CC.UGA.EDU by uga.cc.uga.edu (IBM VM SMTP V2R2)
   with BSMTP id 5097; Sun, 14 Mar 93 20:13:13 EST
Received: from UGA (NJE origin MCOVINGT@UGA) by UGA.CC.UGA.EDU (LMail V1.1d/1.7f) with BSMTP id 3769; Sun, 14 Mar 1993 20:13:13 -0500
Resent-Date:  Sun, 14 Mar 93 20:13:03 EST
Resent-From: "Michael A. Covington" <MCOVINGT@uga.cc.uga.edu>
Resent-To: dg@ai.uga.edu
Return-Path: <@UGA.CC.UGA.EDU:mccord@WATSON.IBM.COM>
Received: from UGA (NJE origin SMTP@UGA) by UGA.CC.UGA.EDU (LMail V1.1d/1.7f)
          with BSMTP id 9710; Sun, 14 Mar 1993 17:43:40 -0500
Received: from watson.ibm.com by uga.cc.uga.edu (IBM VM SMTP V2R2) with TCP;
   Sun, 14 Mar 93 17:43:39 EST
Received: from YKTVMH by watson.ibm.com (IBM VM SMTP V2R3) with BSMTP id 7907;
   Sun, 14 Mar 93 17:45:31 EST
Date: Sun, 14 Mar 93 17:27:48 EST
From: "Michael C. McCord (Phone 914-784-7808;)" <mccord@watson.ibm.com>
To: mcovingt@uga.cc.uga.edu
Cc: uclyrah@ucl.ac.uk
Subject: Tree drawing in TeX


-------------------------------------------------------------
- Michael A. Covington     internet mcovingt@uga.cc.uga.edu -
- Asst. Director / Lab Manager          bitnet MCOVINGT@UGA -
- Artificial Intelligence Programs       phone 706 542-0359 -
- The University of Georgia                fax 706 542-0349 -
- Athens, Georgia 30602                 mci mail MCOVINGTON -
- U.S.A.                                amateur radio N4TMI -
-------------------------------------------------------------


----------------------------Original message----------------------------
Michael,

   You asked:

      "Has anyone written a program to draw dependency trees
      as LaTeX pictures?  I'm working on one and will post
      it if there is sufficient interest."

I need something like this.  My dependency trees for Slot
Grammar look like the following, roughly:

-------------------------------------------------------------------

   Who did Alice say Bob wanted to find and talk with?

,--------- obj(n)        who1(1)         noun
o--------- top           do1(2,3,4)      verb
`--------- subj(n)       Alice1(3)       noun
`--------- auxcomp(binf) say1(4,3,6,u)   verb
  | ,----- subj(n)       Bob1(5)         noun
  `-+----- obj(fin)      want2(6,5,5,9)  verb
    | ,--- preinf        to2(7)          preinf
    | ,--- lconj         find1(8,5,1,u)  verb
    `-+--- comp(inf)     and(9,8,10)     verb
      `--- rconj         talk1(10,5,u,1) verb
        `- comp(p(with)) with1(11,1)     prep

-------------------------------------------------------------------

   In this display, there is one line per word/node, and it has
the nice property that you can get trees for very large sentences in
a small amount of space, with a lot of information for each node.

   On each line you see: (1) a tree line, (2) the slot filled by the
node, (3) the word sense predication, and (4) the feature structure.
The word sense predicate is a sense name for the word.  Its first
argument is just the word number, which serves as an index for the node.
Other arguments are the indices for the actual arguments
(the first of these for a verb is the logical subject, etc.).
The feature structure (like 'verb') is _abbreviated_ here, by a
display option; but it could be larger, and could easily extend
as far to the right as you want, a nice thing about this notation.

  I usually use SCRIPT for formatting and there I can make those
tree lines neat -- with nice box characters.  So far I've been
unable to reproduce this in LaTeX, though I'm sure that with
enough work you can get LaTeX to _draw_ them.  It would be much
easier if there were just box _characters_ of the sort I can get in
other systems.

   I didn't know how to send this note to the whole mailing
list.  Could you post it?

Best regards,
   Michael
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From mccord@watson.ibm.com Mon Mar 15 02:11:03 1993
Received: from watson.ibm.com by aisun1.ai.uga.edu with SMTP id AA00825
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Mon, 15 Mar 1993 07:12:31 -0500
Message-Id: <199303151212.AA00825@aisun1.ai.uga.edu>
Received: from YKTVMH by watson.ibm.com (IBM VM SMTP V2R3) with BSMTP id 9973;
   Mon, 15 Mar 93 07:12:26 EST
Date: Mon, 15 Mar 93 07:11:03 EST
From: "Michael C. McCord (Phone 914-784-7808;)" <mccord@watson.ibm.com>
To: dg@ai.uga.edu

Michael,

   Thanks for your replies.  I did try 'verbatim' and with that
I can get a display in LaTeX pretty much like what appeared in
my note.  My main problem though was to find a monospace font
that has better tree-line characters than the ones in my note.
The characters I use normally (with SCRIPT) are "box
characters"; they are like parts of a square -- upper left
corner, top edge, etc.  It makes a very nice display.  My parser
produces the characters automatically, so there's no work in
making the displays.

   Did you yet forward my note to the mailing list?  I didn't
receive it, and I thought I would if it were sent to the whole
mailing list.  Thanks for reminding me how to send to the list.
I could do it now for that note, but I didn't want to duplicate
it if you have.

   When notes are sent to that mailing list userid, do they
automatically and immediately go out, or do you have to do
something with them?

Best regards,
   Michael
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From ucleaar@ucl.ac.uk Wed Mar 17 14:54:14 1993
Received: from sun2.nsfnet-relay.ac.uk by aisun1.ai.uga.edu with SMTP id AA18904
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Wed, 17 Mar 1993 09:55:41 -0500
Via: uk.ac.bcc.mail-a; Wed, 17 Mar 1993 14:54:25 +0000
Received: from link-1.ts.bcc.ac.uk by mail-a.bcc.ac.uk with SMTP (PP) 
          id <12890-0@mail-a.bcc.ac.uk>; Wed, 17 Mar 1993 14:54:18 +0000
Received: by link-1.ts.bcc.ac.uk (AIX 3.2/UCB 5.64/4.03) id AA35265;
          Wed, 17 Mar 1993 14:54:16 GMT
From: ucleaar@ucl.ac.uk (Mr Andrew Rosta)
Message-Id: <9303171454.AA35265@link-1.ts.bcc.ac.uk>
To: dg@ai.uga.edu
X-Ungarbled_Sender: And Rosta <ucleaar@ucl.ac.uk>
Subject: raising-to-object in a monostratal DG
Date: Wed, 17 Mar 93 14:54:14 +0000
Sender: ucleaar@ucl.ac.uk


I have been exploring a (monostratal) dependency analysis 
that distinguishes (1) & (2).

1. He persuaded her to be a syntactician.
2. He believed her to be a syntactician.

I would like to know of any other work that has distinguished
these two and that uses analyses (i) based on dependency, and 
(ii) lacking transformations.

I will summarize any replies sent personally to me, though
since the traffic on this list is so low I don't suppose
anyone would be too put out if replies were sent to the 
whole list instead.

----
And Rosta
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From ellalain@nuscc.nus.sg Fri Apr  2 21:26:32 1993
Received: from nuscc.nus.sg by aisun1.ai.uga.edu with SMTP id AA18950
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Fri, 2 Apr 1993 00:26:36 -0500
Received: by nuscc.nus.sg (5.65/1.34)
	id AA04764; Fri, 2 Apr 93 13:26:32 +0800
Date: Fri, 2 Apr 93 13:26:32 +0800
From: ellalain@nuscc.nus.sg (Alain Polguere)
Message-Id: <9304020526.AA04764@nuscc.nus.sg>
To: <dg@ai.uga.edu>
Subject: A question about Mandarin


I don't know whether there is any "Mandarinophone" on
this list... With a colleague here, Gan Kok Wee, we try
to find as many arguments as possible for determining
whether a syntactic dependendy holds between TA and BENREN
or SHENG and BENREN in a sentence like:

   "Ta benren sheng-le san ge haizi."

We are particularly interested in arguments which would
be based on actual characteristic properties of the Chinese
grammar. (No UG please :-) ).


Thanks for you help.

AP

---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From rsharman@vnet.IBM.COM Fri Apr  2 03:00:37 1993
Received: from vnet.IBM.COM ([192.239.48.4]) by aisun1.ai.uga.edu with SMTP id AA19317
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Fri, 2 Apr 1993 03:00:37 -0500
Message-Id: <199304020800.AA19317@aisun1.ai.uga.edu>
Received: from WINVMD by vnet.IBM.COM (IBM VM SMTP V2R2) with BSMTP id 7242;
   Fri, 02 Apr 93 02:59:25 EST
Date: Fri, 2 Apr 93 09:00:27 BST
From: rsharman@vnet.IBM.COM
To: dg@ai.uga.edu
Subject: A Computer-readable notation for DG

What is the universally accepted, computer readable, notation for
dependency grammar parses?

The notation used in publications, with beautiful arrows, labellings etc
(e.g. Hudson's Word Grammar, and Sleator's Link Grammar) is fine for
description and education, but it's not computer readable. Definitions of
dependency graphs in PROLOG clauses, or LISP notation are dependent on
the details of particular implementations, and so not readily interchangeable.

What I need is a simple notation, using only ASCII characters, that
can encode the dependency relationships of any naturally occurring sentence.
This "interchange format" should also have the following characteristics:
1. enable most useful dependency information to be encoded
2. allow for extension for new types of annotation
3. allow different languages (at least in principle)
4. be acceptable to most people working with DG's.

The model I would like to emulate is that of the "tree bank" of phrase
structure analyses used by the Lancaster Univ. UCREL group, and by the
U.Penn corpus annotators, but specifically adapted to DG's.

Richard Sharman
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From eytan@dpt-info.u-strasbg.fr Fri Apr  2 17:49:45 1993
Received: from isis.u-strasbg.fr by aisun1.ai.uga.edu with SMTP id AA21388
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Fri, 2 Apr 1993 10:49:06 -0500
Received: from monza.u-strasbg.fr by isis.u-strasbg.fr with SMTP id AA06547
  (5.65c8/IDA-1.4.4 for <dg@ai.uga.edu>); Fri, 2 Apr 1993 17:48:46 +0200
Received: from [130.79.160.96] (buren.u-strasbg.fr) by monza.u-strasbg.fr (4.1/SMI-3.2-jjp/4/6/92)
	id AA01012; Fri, 2 Apr 93 17:49:22 +0100
Message-Id: <9304021649.AA01012@monza.u-strasbg.fr>
Date: Fri, 2 Apr 1993 16:49:45 +0100
To: dg@ai.uga.edu, rsharman@vnet.IBM.COM
From: eytan@dpt-info.u-strasbg.fr (Michel Eytan, LILoL)
X-Sender: me@ushs.u-strasbg.fr
Subject: Re: A Computer-readable notation for DG

At  9:00 2/04/93 -0800, rsharman@vnet.IBM.COM wrote:
[deleted]
>The model I would like to emulate is that of the "tree bank" of phrase
>structure analyses used by the Lancaster Univ. UCREL group, and by the
>U.Penn corpus annotators, but specifically adapted to DG's.
>
>Richard Sharman
>---------------------------------------------------------------------------
>Distributed through DG, the international dependency grammar mailing list.
>To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
>---------------------------------------------------------------------------
>If you REPLY to this mail, your response will go to the person who sent it.
>To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
>---------------------------------------------------------------------------
>---------------------------------------------------------------------------

Richard,

I believe it would be of interest for several of us ignoramuses if you
could provide some more info about "tree banks" or better to tell us how to
get the info by e-means.

Thank you, cheers
--
Michel Eytan, Lab Info, Log & Lang              eytan@dpt-info.u-strasbg.fr
Dpt Info, U Strasbourg II                       V: +33 88 41 74 29
22 rue Descartes, 67084 Strasbourg FR           F: +33 88 41 74 40

---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From mccord@watson.ibm.com Fri Apr  2 06:29:50 1993
Received: from watson.ibm.com by aisun1.ai.uga.edu with SMTP id AA21875
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Fri, 2 Apr 1993 11:30:01 -0500
Message-Id: <199304021630.AA21875@aisun1.ai.uga.edu>
Received: from YKTVMH by watson.ibm.com (IBM VM SMTP V2R3) with BSMTP id 2027;
   Fri, 02 Apr 93 11:29:58 EST
Date: Fri, 2 Apr 93 11:29:50 EST
From: "Michael C. McCord (Phone 914-784-7808;)" <mccord@watson.ibm.com>
To: dg@ai.uga.edu
Cc: lappin@watson.ibm.com
Subject: Notation for dependency grammar parses

Folks,

   Richard Sharman asked for a good notation for dependency
grammar parses.  It should be computer-readable, simple, and should
use only ASCII characters -- having this much in common with the
notation of the UPenn treebank.

   At the UPenn DARPA workshop on grammar evaluation held a few
months ago, we had a subsession on the representation of
predicate argument structure.  This was of interest since the
existing UPenn treebank doesn't show predicate argument
structure.  In this session I proposed a notation I called "slot graphs"
which have the properties Richard asked for.  Slot graphs
can be output by the Slot Grammar parser, as an option.
And I think they would be a reasonable basis for a parsebank for
dependency grammar parses.

   Slot graph analyses are dependency-oriented and assume that
each phrase has a unique head word (token).  Normally, there is
a 1-1 correspondence between these head words and the original
words (tokens) of the sentence.  In the following, I'll just use
"word" to include real words as well as other tokens such as
punctuation symbols.

   To produce a slot graph for a sentence, you number the words
with integers 1, 2, 3, ... .  It's convenient to write down the
words vertically, with their integer indices in front of them,
so that there is one word (phrase/node) per line.  Then on each
line you show all the information you want about that node.

    Here are two examples, where we have just elected to show
slot-filler information about each node.

Alice loves Bob.

----------------------------------------------------------------
 1  Alice
 2  loves  subj:1 obj:3
 3  Bob
----------------------------------------------------------------



Who does Bob want Alice to love?

----------------------------------------------------------------
 1  who
 2  does   subj:3 auxcomp:4
 3  Bob
 4  want   subj:3 obj:5 comp:7
 5  Alice
 6  to
 7  love   subj:5 obj:1 preinf:6
----------------------------------------------------------------


   Here,  for  each  each word J, I've shown all the slot-filler pairs

    Slot : I

where word I (and the phrase it heads) fills slot Slot in phrase J.
That's  the  _basic_  idea  of  "slot  graph" -- to show the
slot-filler relationships.

   Although  I've  shown only slot-filler relationships in these
examples, the basic idea of the notation is flexible because you
can add whatever information you want about  each  node  on  the
line  for  that  node.    For  instance  you could show parts of
speech, as in


Alice loves Bob.

----------------------------------------------------------------
 1  Alice                  noun
 2  loves  subj:1 obj:3    verb
 3  Bob                    noun
----------------------------------------------------------------


or more complete feature information.  You could also show lemma
forms  of  words    and/or  _word  senses_  instead  of just the
original words.   We discussed several  variants  at  the  UPenn
workshop, and there has been more discussion since.)

   In  most  of  the  following  examples,  I'll  just  show the
slot-filler  relationships,  but  keep  in  mind  that the basic
notation allows more information about each node.

   In the case of passive past participles, the  logical  (deep)
slots could be shown, as in:


Alice was awarded a prize by the committee.

----------------------------------------------------------------
 1  Alice
 2  was        subj:1 pred:3
 3  awarded    subj:8 obj:5 iobj:1
 4  a
 5  prize      ndet:4
 6  by         objprep:8
 7  the
 8  committee  ndet:7
----------------------------------------------------------------



The following example illustrates gapping.


France reported 50 cases; Germany, 48.

----------------------------------------------------------------
 1  France
 2  reported  subj:1  obj:4  ref:7
 3  50
 4  cases     ndet:3
 5  ;         lconj:2  rconj:7
 6  Germany
 7  ,         subj:6  obj:8
 8  48
----------------------------------------------------------------



In certain rare cases, in particular for gapping, abstract nodes
are needed, as the following example illustrates:


Alice sent Bob a letter and Charles a card.

----------------------------------------------------------------
 1  Alice
 2  sent      subj:1  obj:5  iobj:3  ref:x
 3  Bob
 4  a
 5  letter    ndet:4
 6  and       lconj:2  rconj:x
 x  ref:2     subj:1  obj:9  iobj:7
 7  Charles
 8  a
 9  card      ndet:8
----------------------------------------------------------------




   As a mathematical graph,  the  nodes of a slot graph are of course
the node indices (sentence token numbers) I have been talking about
(plus any abstract nodes).  A labeled edge is of the form

    (I, Slot, J),

where  node I fills slot Slot in node J.  In graphic form, these
triples would correspond to labeled arcs.

  Alternatively, a slot graph may be viewed in predicate logic  as
a set of predications

    Slot(I,J).

It  is  probably  easiest  to write down slot graphs in the form
illustrated in  the  examples  above,  which  we  can  call  the
_filler_ notation.  One just takes a word at a time and
writes  down,  next  to word J, all the slot-fillers for J, i.e.
all pairs Slot:I where (I,Slot,J) is in the graph (I fills  slot
Slot in J).

   Another  possibility  would be the _role_ notation:
One writes down, next to word I, all pairs Slot:J such that I  fills
slot  Slot  in J.  (These pairs are the _roles_ of I in the sentence.)
For example:


Who does Bob want Alice to love?

----------------------------------------------------------------
 1  who    obj:7
 2  does   top:0
 3  Bob    subj:2 subj:4
 4  want   auxcomp:2
 5  Alice  obj:4 subj:7
 6  to     preinf:7
 7  love   comp:4
----------------------------------------------------------------


   The  information  content is the same for the filler notation
and  the  role notation; but perhaps the filler notation is more
readable, especially when coordination is involved.

   A surface dependency tree can easily be represented as a slot
graph.  The tree can be specified by showing the  unique  mother
node  of each word.  For each word I, we can just write down the
single role

    mother : J

where  J  is  the  mother  of I.   Or the role could have a more
informative name, like

    subj : J.

An  arbitrary  slot graph has an associated tree.  The main idea
in obtaining it is to compute the _major role_ of  each
word  and  build the tree by considering this as pointing to the
mother.    I have developed procedures for computing major roles
and associated trees, but I won't describe them here.

   If a parsebank is given in  slot  graph  form,  and  a  given
grammar  produces only surface trees, one could always test that
grammar against the trees associated with the slot graphs in the
way just indicated.  In other words, one could always throw away
the information given by non-major roles.


   Some  display  software  could  make  it easier for humans to
write down slot graphs.   For instance,  the  numbering  of  the
words  could  be done by a program, and a screen interface would
allow the human parser to enter the slot-fillers or the roles in
the above sorts of display.   One could  imagine  simple  linear
notations too.


Best regards,
   Michael McCord
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From rsharman@vnet.IBM.COM Fri Apr  2 12:01:59 1993
Received: from vnet.IBM.COM ([192.239.48.4]) by aisun1.ai.uga.edu with SMTP id AA22072
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Fri, 2 Apr 1993 12:01:59 -0500
Message-Id: <199304021701.AA22072@aisun1.ai.uga.edu>
Received: from WINVMD by vnet.IBM.COM (IBM VM SMTP V2R2) with BSMTP id 9354;
   Fri, 02 Apr 93 12:00:45 EST
Date: Fri, 2 Apr 93 18:01:41 BST
From: rsharman@vnet.IBM.COM
To: dg@ai.uga.edu
Subject: Treebanks

A short note for all those who are unfamiliar with "treebanks", and
apologies to those who know: a treebank is a collection of trees, each
of which is a phrase-structure parse of a sentence, e.g.

  [S [N John N] [V loves [N mary N] V] . S]

The notation can be turned into a tree in your implementation when you read it
in. Brackets must be matched, should have labels, etc.  Words can be tagged
with parts-of-speech e.g. John_NP, and other tags can be added to show
anaphora, co-reference, etc, as needed. It is a sort of markup language
a bit like GML. It is, of course, theory specific. But the data can
be used for all sorts of language analysis tasks.

Now, assuming that one wanted to do the same, but using the concepts of
dependency grammar, how would you do the annotations?

regards, Richard Sharman
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From uclyrah@ucl.ac.uk Fri Apr  2 21:53:42 1993
Received: from sun2.nsfnet-relay.ac.uk by aisun1.ai.uga.edu with SMTP id AA25761
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Fri, 2 Apr 1993 15:01:40 -0500
Via: uk.ac.bcc.mail-a; Fri, 2 Apr 1993 20:53:48 +0100
Received: from link-1.ts.bcc.ac.uk by mail-a.bcc.ac.uk with SMTP (PP) 
          id <06127-0@mail-a.bcc.ac.uk>; Fri, 2 Apr 1993 20:53:44 +0100
Received: by link-1.ts.bcc.ac.uk (AIX 3.2/UCB 5.64/4.03) id AA21323;
          Fri, 2 Apr 1993 20:53:43 +0100
Message-Id: <9304021953.AA21323@link-1.ts.bcc.ac.uk>
To: dg@ai.uga.edu
Subject: A computer-readable notation
Date: Fri, 02 Apr 93 20:53:42 +0100
From: RichardHudson50 <uclyrah@ucl.ac.uk>


I'd personally be very happy to accept Michael McCord's notation. Presumably
one objection to it might be that you can't view much of a text on a vdu
screen, in contrast with the linear notation that Richard Sharman 
illustrated for PSG tree-banks. But Michael's notation could presumably
be converted very easily into a linear notation something like the 
following, without loss of information:

	1:N:Alice[2,subject], 2:V:loves[0], 3:N:Bob[2,object].

I.e. word 1, a noun, namely "Alice", depends on word 2, as its subject; etc.



Dick Hudson
Dept of Phonetics and Linguistics,
University College London,
Gower Street,
London WC1E 6BT
(071) 387 7050 ext 3152

---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From stanley@uhunix.uhcc.Hawaii.Edu Sat Apr  3 15:18:18 1993
Received: from uhunix.uhcc.Hawaii.Edu by aisun1.ai.uga.edu with SMTP id AA05474
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Sat, 3 Apr 1993 15:18:18 -0500
Received: by uhunix.uhcc.Hawaii.Edu (4.1/Sun690)
	id AA27980; Sat, 3 Apr 93 10:17:28 HST
Date: Sat, 3 Apr 93 10:17:28 HST
From: Stanley Starosta  <stanley@uhunix.uhcc.Hawaii.Edu>
Message-Id: <9304032017.AA27980@uhunix.uhcc.Hawaii.Edu>
To: dg@ai.uga.edu
Subject: dependency notation
Cc: stanley@uhunix.uhcc.Hawaii.Edu

I gave a paper at the 2nd Japan-Australia joint symposium on natural 
language processing (October 1991) called `Dependency grammar and monostratal
transfer' in which I proposed the following:

"I suggested at the beginning of this paper that using a constrainted 
dependency framework might facilitate machine translation by monostratal 
transfer.  Now I would like to make that proposal a bit more concrete.  
The basic idea is that grammatical structure is encoded in words, that the 
structure of a sentence (including dependency counterparts of `immediate 
dominance' and `linear precedence' in the GPSG sense) can be fully 
represented in a string of words, and that (as a first approximation) the 
grammatical phase of machine translation (mt) might be reduced to a 
straightforward process of substring-to-substring lexical substitution.  No 
additional processing would be necessary to generate a derived structure, 
since the derived structure would be built into the output string of words."

Like McCord's notation, my notation uses indices, but it is a simple list
rather than a graph, and does not refer to any abstract level of 'logical 
form'.  The full paper is available as transfer.snd through anonymous login
on host ftp.ims.uni-stuttgart.de, directory pub/lexicase.
Stan Starosta
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From bnevin@ccb.bbn.com Mon Apr  5 05:52:12 1993
Received: from BBN.COM by aisun1.ai.uga.edu with SMTP id AA19791
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Mon, 5 Apr 1993 09:57:04 -0400
Message-Id: <199304051357.AA19791@aisun1.ai.uga.edu>
Received: from CCB.BBN.COM by BBN.COM id aa12855; 5 Apr 93 9:54 EDT
Date: Mon, 5 Apr 93 09:52:12 EDT
From: "Bruce E. Nevin" <bnevin@ccb.bbn.com>
Subject: Starosta on notation
To: dg@ai.uga.edu
Cc: bn@ccb.bbn.com

> The basic idea is that grammatical structure is encoded in words,
> that the structure of a sentence [...] can be fully represented in
> a string of words, and that (as a first approximation) the
> grammatical phase of machine translation (mt) might be reduced to
> a straightforward process of substring-to-substring lexical
> substitution.  No additional processing would be necessary to
> generate a derived structure, since the derived structure would
> be built into the output string of words."
>   
>Stan Starosta

This supports a claim that the metalanguage that is observably
available as a sublanguage of natural language is sufficient to
describe a natural language.  For development of the notion that
the metalanguage within language is not only sufficient but also
necessary, and that any seeming alternative is based upon or
derived from language, see Harris (1968, 1988, 1991), Harris et
al.(1989), Ryckman (1986), Nevin (1993).  Against this is the
familiar claim that notations such as category labels and
subscripts, or the metalanguage expressions that they represent,
are in some sense innate.  Even without the claim of necessity,
the observation of sufficiency argues against this on grounds of
economy (Ockham's razor).  Notational expressions are compact
graphical representations of the natural language expressions
that are used to express them orally, and the information that is
thought to be in them is precisely the linguistic information
that is constituted by those natural language expressions that
they represent.

Harris shows that not only language structure but also operations
deriving and constituting language structure are stateable in the
intrinsic metalanguage, without recourse to metalanguage
constructs supposedly external to and prior to language.  The
word dependencies of operator grammar are virtually identical
between languages.  The conditions for applying reductions are
present in these dependencies.  The language differences are
almost entirely in the metalanguage statements (rules) deriving
reduced constructions at each point at which an operator enters
on its arguments.  The application to MT is, as you say, obvious,
but I think much more extensive than you had supposed..

Whereas certain constraints are placed on the grammar of the
language as a whole due to absence of an external metalanguage,
the metalanguage for the whole language is in fact external to
the sublanguage, removing those constraints from sublanguage
grammar (Harris 1991 chapter 10).  For example, in sublanguage
grammar, especially for a technical or scientific domain,
information structures common to discourses of the sublanguage
(i.e. in the subject matter domain) become available as elements
of the grammar identified in the metalanguage, vastly simplifying
many computational tasks.  And whereas utterance acceptability
judgements for the language as a whole are a graded property, and
language is in consequence not well defined, sublanguage grammar
can in many cases distinguish and reject nonsense, and science
sublanguages may be well defined.  This too has computational
implications that have barely begun to be explored.

Harris, Zellig S.  1968.  _Mathematical Structures of Language_
__________.  1982.  _A Grammar of English on Mathematical
	Principles_.  New York: Wiley.
__________.  1988 _Language and Information_.  New York:
	Columbia University Press.
__________.  1991.  _Language and Information: A mathematical approach.
	Oxford: Clarendon Press.
__________, Michael J. Gottfried, Thomas A. Ryckman, et. al.  1989.
	_The Form of Information in Science:  Analysis of an
	immunology sublanguage.  Boston Studies in the Philosophy
	of Science 104.  Boston: Kluwer Academic Publishers.
Nevin, Bruce E.  1993.  A minimalist program for linguistics:
	Zellig Harris' work on meaning and information.
	_Historiographia Linguistica_ XX 2/3 (forthcoming).
Ryckman, Thomas A.  1986.  _Grammar and Information: An
	investigation in linguistic metatheory.  PhD.
	dissertation.  New York: Columbia University.

Bruce Nevin
bn@bbn.com

---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From sak@uhunix.uhcc.Hawaii.Edu Wed Apr  7 03:25:36 1993
Received: from uhunix.uhcc.Hawaii.Edu by aisun1.ai.uga.edu with SMTP id AA09353
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Wed, 7 Apr 1993 03:25:36 -0400
Received: by uhunix.uhcc.Hawaii.Edu (4.1/Sun690)
	id AA22114; Tue, 6 Apr 93 21:25:34 HST
Date: Tue, 6 Apr 93 21:25:34 HST
From: Chhany Sak-Humphry  <sak@uhunix.uhcc.Hawaii.Edu>
Message-Id: <9304070725.AA22114@uhunix.uhcc.Hawaii.Edu>
To: dg@ai.uga.edu
Subject: R

Yes, I heard about you from Professor Starosta.  I am very much interested in
joining your group, and would like to know what your activities.  Thanks.
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From stanley@uhunix.uhcc.Hawaii.Edu Thu Apr  8 17:40:41 1993
Received: from uhunix.uhcc.Hawaii.Edu by aisun1.ai.uga.edu with SMTP id AA11072
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Thu, 8 Apr 1993 17:40:41 -0400
Received: by uhunix.uhcc.Hawaii.Edu (4.1/Sun690)
	id AA16850; Thu, 8 Apr 93 11:40:28 HST
Date: Thu, 8 Apr 93 11:40:28 HST
From: Stanley Starosta  <stanley@uhunix.uhcc.Hawaii.Edu>
Message-Id: <9304082140.AA16850@uhunix.uhcc.Hawaii.Edu>
To: dg@ai.uga.edu
Subject: DG notation
Cc: stanley@uhunix.uhcc.Hawaii.Edu, yngve@cerberus.uchicago.edu

Thanks to Bruce Nevin for the response to my posting on dependency notation.
I think I didn't express myself clearly enough, though.  The `words' I was 
referring to in my `string of words' suggestion for mt are words in a 
linguistic sense, that is, complexes of phonological, distributional, and 
distinctive or prototype-based referential information, and I would not try
to represent this kind of information in a subset of natural language (a la 
Wierzbicka?) any more than I would try in physics to represent information 
about mass and energy in a subset of natural language.  Based on BN's 
posting, this sounds to me like what my first linguistics teacher, Martin 
Joos, referred to as 'a fire in a wooden stove'.
I also would not subscribe to the statement that `...whereas utterance 
acceptability judgements for the language as a whole are a graded property, and
language is in consequence not well defined...'.  This seems to me compara-
ble to claiming that since real-world falling objects do not conform exactly
to the physical law s = s0 - 1/2gt2, therefore the law itself is somehow not
well-defined.  I would rather say that informant judgements reflect a variety
of interacting factors, grammaticality being only one, and that a linguistic
scientist's job is to sort out these factors and design experiments to 
control the variables.
Stan S.
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From rsharman@vnet.IBM.COM Mon May 24 12:14:38 1993
Received: from vnet.ibm.com by aisun1.ai.uga.edu with SMTP id AA09398
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Mon, 24 May 1993 12:14:38 -0400
Message-Id: <199305241614.AA09398@aisun1.ai.uga.edu>
Received: from WINVMD by vnet.IBM.COM (IBM VM SMTP V2R2) with BSMTP id 3189;
   Mon, 24 May 93 12:13:23 EDT
Date: Mon, 24 May 93 16:14:58 BST
From: rsharman@vnet.IBM.COM
To: dg@ai.uga.edu
Subject: Classifying Parsers

Dear DG-ers,
 Like Michael Covington I was wondering if anything had happened recently!

 I though Michael's classification a useful step to telling others
what you are talking about. Some time ago I developed a statistically
based phrase structure parser, which the classification would make:

(1) Constituency
(2) Continuous Phrases
(3) Completely Ordered
(4) Mixed, bottom up and top down.
(5) All-at-once
(6) Whole sentence available
(7) Concurrent
(8) Unlimited
(9) Acive Chart
(10) Likelihood maximisation to find "best" parse.

I don't know if this helps to understand what it is like, and it does
miss one important point: the systen is TRAINED on pre-existing data.
Instead of inventing the rules "by hand" the rules are "discovered"
by analysing (automatically) a large treebank. All this prompts me
to ask if another section could be added to the classification
covering these ideas?

I am now working on a more dependency-based parser which would look like
this:

(1) Dependency
(2) Discontinuous
(3) Free linear order
(4) Bottom up
(5) Incremental
(6) Left-to-right
(7) concurrent
(8) Unlimited
(9) Table (like chart)
(10) Likelihood maximisation

It would also be trained. Does this make it clear?

regards, RIchard
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From rsharman@vnet.IBM.COM Mon May 24 13:11:54 1993
Received: from vnet.ibm.com by aisun1.ai.uga.edu with SMTP id AA11679
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Mon, 24 May 1993 13:11:54 -0400
Message-Id: <199305241711.AA11679@aisun1.ai.uga.edu>
Received: from WINVMD by vnet.IBM.COM (IBM VM SMTP V2R2) with BSMTP id 3635;
   Mon, 24 May 93 13:10:38 EDT
Date: Mon, 24 May 93 17:59:58 BST
From: rsharman@vnet.IBM.COM
To: dg@ai.uga.edu
Subject: Classification of Parsers

A classification is a good thing, but  of course, some criteria
are more useful than others. Often the useful criteria are the
hardest to evaluate, and the the easy criteria are not much use.
But that's life! There is one property which I think ought to be included
in a classification, however, and that is what the parser can be used
for. Does the parser just create some structure from the given sentence,
or is it a powerful enough model of language (theory, even) that can
predict other properties? If so, then the parser would have applications
in the gray area of ungrammaticallity. To my mind an ideal parser would
be able to parse good sentences, and also bad ones.  An essential task
in the real world is to find the offending word/phrase/construction that
makes a sentence odd, and suggest the best correction. Another task is
to finish off an unfinished sentence. So I would like to add another
item to Michael Covington's list which covered these points.

 A parse tree, or a dependency graph, seems to me to be an abstract
property of a sentence (you can't actually observe one in nature).
Since there are many abstractions, we have many theories of grammar
and therefore many discussions on which is "best". A practical measure
of goodness can be related to how well the abstraction helps to solve
the sort of problems I indicated above. I don't think conflicts with
other measures, like simplicity, plausibility, elegance, and so on.

Richard
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From uclyrah@ucl.ac.uk Tue May 25 08:55:25 1993
Received: from sun2.nsfnet-relay.ac.uk by aisun1.ai.uga.edu with SMTP id AA18444
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Tue, 25 May 1993 02:55:40 -0400
Via: uk.ac.bcc.mail-b; Tue, 25 May 1993 07:55:28 +0100
Received: from ucl.ac.uk by mail-b.bcc.ac.uk with SMTP (PP) 
          id <07765-0@mail-b.bcc.ac.uk>; Tue, 25 May 1993 07:55:28 +0100
Received: by link-1.ts.bcc.ac.uk (AIX 3.2/UCB 5.64/4.03) id AA20572;
          Tue, 25 May 1993 07:55:25 +0100
Message-Id: <9305250655.AA20572@link-1.ts.bcc.ac.uk>
To: dg@ai.uga.edu
Subject: Theory and practice
Date: Tue, 25 May 93 07:55:25 +0100
From: RichardHudson <uclyrah@ucl.ac.uk>


I have a theoretical question for working dependency grammarians. 


I know a lot of dependency grammarians make a distinction between form-words
and content-words, and include only the latter in their dependency analyses.
Apart from the fact that Tesniere did this, is there any evidence
that it's correct?  In my own work, I have been impressed by the similarities
between e.g. auxiliary verbs (form-words) and non-auxiliary verbs (content-
words), and don't see how a grammar would work smoothly if they had to be
distinguished. 

Dick Hudson
Dept of Phonetics and Linguistics,
University College London,
Gower Street,
London WC1E 6BT
(071) 387 7050 ext 3152

---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From mccord@watson.ibm.com Tue May 25 10:39:24 1993
Received: from watson.ibm.com by aisun1.ai.uga.edu with SMTP id AA22685
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Tue, 25 May 1993 14:41:34 -0400
Message-Id: <199305251841.AA22685@aisun1.ai.uga.edu>
Received: from YKTVMH by watson.ibm.com (IBM VM SMTP V2R3) with BSMTP id 4749;
   Tue, 25 May 93 14:41:37 EDT
Date: Tue, 25 May 93 14:39:24 EDT
From: "Michael C. McCord (Phone 914-784-7808;)" <mccord@watson.ibm.com>
To: dg@ai.uga.edu

Dick,

   Read your note about form-words vs. content-words.  I agree
with you, and don't make any real distinction in Slot Grammar.
For instance, like you, I analyze "auxiliary" verbs as higher
verbs.  And I don't e.g. convert what some people call
form-words into features; rather I keep a 1-1 correspondence
between analysis nodes and words of the sentence as much as possible.

All best,
   Michael
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From stanley@uhunix.uhcc.Hawaii.Edu Thu May 27 09:40:50 1993
Received: from uhunix.uhcc.Hawaii.Edu by aisun1.ai.uga.edu with SMTP id AA00966
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Thu, 27 May 1993 09:40:50 -0400
Received: by uhunix.uhcc.Hawaii.Edu (4.1/Sun690)
	id AA18502; Wed, 26 May 93 15:58:38 HST
Date: Wed, 26 May 93 15:58:38 HST
From: Stanley Starosta  <stanley@uhunix.uhcc.Hawaii.Edu>
Message-Id: <9305270158.AA18502@uhunix.uhcc.Hawaii.Edu>
To: dg@ai.uga.edu
Subject: neglectable words
Cc: stanley@uhunix.uhcc.Hawaii.Edu

I agree with Dick.  I don't see the slightest linguistic justification for
leaving `function' words out of a dependency representation.  This practice
seems to me to be based on a prelinguistic notional grammar-influenced 
reverence for `content' words and contempt for words whose meanings are
hard to describe in terms of `actions' or `things'. It results in a 
major loss in language-specific and cross-linguistic generalizations, not 
to mention the introduction of an otherwise spurious and unmotivated 
distinction between deep and surface structures.
Stan Starosta
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From ucleaar@ucl.ac.uk Fri May 28 18:00:44 1993
Received: from sun2.nsfnet-relay.ac.uk by aisun1.ai.uga.edu with SMTP id AA15939
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Fri, 28 May 1993 12:02:13 -0400
Via: uk.ac.bcc.mail-a; Fri, 28 May 1993 17:01:21 +0100
Received: from link-1.ts.bcc.ac.uk by mail-a.bcc.ac.uk with SMTP (PP) 
          id <09642-0@mail-a.bcc.ac.uk>; Fri, 28 May 1993 17:00:56 +0100
Received: by link-1.ts.bcc.ac.uk (AIX 3.2/UCB 5.64/4.03) id AA92196;
          Fri, 28 May 1993 17:00:45 +0100
From: ucleaar@ucl.ac.uk (Mr Andrew Rosta)
Message-Id: <9305281600.AA92196@link-1.ts.bcc.ac.uk>
To: dg@ai.uga.edu
X-Ungarbled_Sender: And Rosta <ucleaar@ucl.ac.uk>
Subject: a literary dog's breakfast
Date: Fri, 28 May 93 17:00:44 +0100
Sender: ucleaar@ucl.ac.uk


In Word Grammar the dependency relations in the phrase _a dog's breakfast_
(in its literal meaning) are as in (1a-b).

     <------<
     >->    >->
1a.  a dog 's breakfast [literal]
 
     <------------<
     >------->
           <-<    >->
 b.  a hairy dog 's breakfast [literal]

More archaic (;-)) analyses might reverse the directions of some of
these dependencies, but agree on most of the the pairings.

_Dog's breakfast(s)_ can also mean "mess". When it has this idiomatic
reading, one can say things like (2a). (2b-d) are similar.

2a. This is a literary [ dog's breakfast ]
 b. She went on a culinary [ Cook's tour ]
 c. I don't give a bloody [ tinker's cuss ]
 d. I found a green [ man's shoe ]

Should the structure be as in (3)?

     >--------------->
              <------< 
                  <-<>->
3.   a literary dog 's breakfast
     a green    man 's shoe

If this is possible, one ought to be able to say _three man's shoes_.
Can one? (This ought also to be possible if the bracketed phrases
in (2a-d) are compounds.) How should the (2a-d) structure be restricted
to these idiomatic examples?

My preferred analysis would be to treat the bracketed phrases in
(2a-d) as single words, whose internal structure, however, is
syntactic rather than morphological. Other cases where this would
also apply are:

4a. She said "What time is it?"
 b. Pigs Might Fly is coming up to the fourth furlong.
 c. Have you read _For whom the bell tolls_?
 d. ALICE IN WONDERLAND by Lewis Carrol.

Comments?

Unless anyone objects (& feel free to do so), I shan't summarize
any messages sent to me privately. Since the traffic on this
list is so minimal, a bit of on-list discussion might be welcome.

--------
And Rosta
---------------------------------------------------------------------------
Distributed through DG, the international dependency grammar mailing list.
To subscribe or unsubscribe, or for assistance, email mcovingt@ai.uga.edu.
---------------------------------------------------------------------------
If you REPLY to this mail, your response will go to the person who sent it.
To reach the ENTIRE discussion group, MAIL your response to: dg@ai.uga.edu.
---------------------------------------------------------------------------
---------------------------------------------------------------------------
From stanley@uhunix.uhcc.Hawaii.Edu Sun Jul 18 09:33:34 1993
Received: from uhunix.uhcc.Hawaii.Edu by aisun1.ai.uga.edu with SMTP id AA21313
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Sun, 18 Jul 1993 09:33:34 -0400
Received: by uhunix.uhcc.Hawaii.Edu (4.1/Sun690)
	id AA04660; Sun, 18 Jul 93 03:33:31 HST
Date: Sun, 18 Jul 93 03:33:31 HST
From: Stanley Starosta  <stanley@uhunix.uhcc.Hawaii.Edu>
Message-Id: <9307181333.AA04660@uhunix.uhcc.Hawaii.Edu>
To: dg@ai.uga.edu
Subject: How abstract is DG, and what is it good for?

Last November, at the functional dg conference in Prague which was held in 
honor of Petr Sgall, Hans Uszkoreit made two statements during his presenta-
tion which are of potential interest to dependency grammarians.  He said 
1) that DG is only useful for German and Slavic languages, and 
2) that there is no workable monostratal version of DG.  
I wonder how the people on this mailing list would react to these statements?  
I have just given a talk [`Empty categories in constrained dependency 
grammar'] at the Institut f|r maschinelle Sprachverarbeitung, Universitdt 
Stuttgart, in which I attempted to refute 2) by demonstrating that the 
lexicase version of DG is both monostratal and workable, and another 
[`Dependency grammar meets Asian and Pacific languages'] at the Institute 
for Applied Mathematics, Charles University, Prague, in which I talked about 
the more than 70 Asian, Pacific, Native American, African, and Indoeuropean 
languages that had been worked on to varying degrees in the lexicase 
dependency framework and some of the insights that had been derived from work
on these languages.  (I can upload my list (language names, authors and
dates) if there is general interest.)  Maybe dependency grammarians need to 
do a little more `public relations' work to counteract uninformed and 
theoretically naive statements like Uszkoreit's.
Stan Starosta
From hansu@coli.uni-sb.de Mon Jul 19 23:10:13 1993
Received: from iraun1.ira.uka.de by aisun1.ai.uga.edu with SMTP id AA27987
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Mon, 19 Jul 1993 15:10:53 -0400
Received: from sbusol.rz.uni-sb.de by iraun1.ira.uka.de with SMTP (PP) 
          id <05673-0@iraun1.ira.uka.de>; Mon, 19 Jul 1993 21:10:19 +0200
Received: from coli-gate.coli.uni-sb.de by sbusol.rz.uni-sb.de (5.65+/v1.0) 
          id AA19072; Mon, 19 Jul 93 21:10:19 +0200
Received: by coli-gate.coli.uni-sb.de (4.1/SMI-4.1/CL930118) id AA04739;
          Mon, 19 Jul 93 21:10:13 +0200
Date: Mon, 19 Jul 93 21:10:13 +0200
From: hansu@coli.uni-sb.de (Hans Uszkoreit)
Message-Id: <9307191910.AA04739@coli-gate.coli.uni-sb.de>
To: dg@ai.uga.edu
Subject: How abstract is DG, and what...

Let me defend Stanley Starosta's way of getting the ball rolling again
in the DG discussion forum. I concede, the provocative, slightly
slanderous conversation starter typical for scientific flaming in some
parts of the world is not everyone's cup of tea. But it is very
effective in getting people to react. It worked with me although
supported by the fact that I am sitting at home immobilized by a cast
around my left leg this summer instead of travelling to conferences
and summer schools. 

What did I really say in Prague? I will summarize it here although I
admit it is more boring than Starosta's rendering of my claims. I
tried to state some reasons why the three basic grammar models of PSG,
DG, and CG (and derivatives) have coexisted in linguistics for many
decades although proponents of the different frameworks have tried
every method to shoot down the competing approaches. 

I claimed that it was certainly not because scientists were uninformed
about the virtues of the other models or theoretically too naive to
see the light of truth. I claimed that the three basic grammar models
each had to offer something to certain groups of researchers that the
others could not provide. 

In this context, I pointed out that it is no coincidence... 

...that dependency grammar models have traditionally been stronger
among linguists working on Slavic or Germanic languages than among
linguists working on English, 

...that phrase structure based models have been stronger in
Anglocentric parts of linguistics than in--let's say--Slavic
linguistics circles, and 

...that categorial grammar models have been strongest in circles more
concerned with formal semantics than with syntax. 

I also pointed out that contemporary unification grammar frameworks
usually mix strategies from the three basic models. They do this in
quite different ways. The most widespread unification grammar models
agree in that they all base their notion of derivation on recursion in
the phrase structure. In some cases this decision is built into the
formal apparatus in others it is a deliberate choice of usage. (There
is, for instance, a version of FUG using variables in the feature
PATTERN that could be used for doing recursion on dependency structure
by letting the value of CSET be the set of dependents.) 

If you look at newer brands of CG such as Michael Mortgaat's model we
see that here also strategies from different models are combined-
-this time under the primacy of the categorial approach. 

I think that most contemporary grammar models combine strategies from
at least two of the basic models. One might combine for instance: 

- phrase structure constrained by dependency structure (functional
structure) as a control structure 

- phrase structure constrained by a categorial semantics 

- dependency structure extended by means for functional composition or
some other form of argument raising. 

The reason for working with combinations is obvious, it's the large
class of mismatches between functor-argument structure, functional
structure, and surface order found in language. 

I showed that in HPSG, especially in newly emerging versions, we find
a combination of strategies from all three basic models. 

I did not claim that monostratal versions of DG could not be
constructed; quite the opposite, I indicated how a framework such as
the functional dependency model of the New Prague School could be
based on monostratal representations utilizing the same formal
descriptive tools that are used to define the grammatical framework of
HPSG. I also pointed out the most problematic part of such a move,
i.e., the modular formulation of principles that relationally
constrain the mapping between dependency structure and phonological
form. 

There might be other, much more elegant monostratal versions of DG. If
there exists a true DG model (a model with derivation by recursion on
the dependency structure) that is monostratal and that does neither
require a very complex notion of mapping from  dependency structure to
phonology nor several CG-like mechanisms for argument raising (raising
of dependents) in the lexicon or in the  derivation, please let me
know immediately! 

I think Starosta is right when he says that DG grammarians need to
improve their PR. But then isn't this true for the proponents of all
non-GB grammar theories? 

A constructive final remark: It would be nice if DG grammarians  would
join in the very promising dialogue between CG and HPSG that  has
recently been emerging. 

Hans Uszkoreit
From rambow@unagi.cis.upenn.edu Tue Jul 20 07:25:16 1993
Received: from LINC.CIS.UPENN.EDU by aisun1.ai.uga.edu with SMTP id AA03874
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Tue, 20 Jul 1993 11:25:19 -0400
Received: from UNAGI.CIS.UPENN.EDU by linc.cis.upenn.edu
	id AA15468; Tue, 20 Jul 93 11:25:17 -0400
Return-Path: <rambow@unagi.cis.upenn.edu>
Received: by unagi.cis.upenn.edu
	id AA09251; Tue, 20 Jul 93 11:25:16 EDT
Date: Tue, 20 Jul 93 11:25:16 EDT
From: rambow@unagi.cis.upenn.edu (Owen Rambow)
Posted-Date: Tue, 20 Jul 93 11:25:16 EDT
Message-Id: <9307201525.AA09251@unagi.cis.upenn.edu>
To: dg@ai.uga.edu
Subject: DG / PSG


As a quick follow-up to Stan Starosta and Hans Uszkoreit's very interesting
postings: in a paper presented at the workshop on Meaning-text Theory (MTT)
in Darmstadt last summer, Aravind Joshi and I argue that, in the past, the
main dividing line between the linguistic traditions has been the issue of
the lexicon.  DG linguists are naturally interested in the lexicon, while
in a 70s-style Chomskyan theory, the lexicon is relegated to a secondary
and tenuous position.  There is now a general appreciation of the role of
the lexicon beyong DG linguists, as witnessed by theories such as HPSG,
LFG, TAG-based linguistic theories, and GB and Minimalism (Chomsky's
newest).  But none of these systems are formally context-free grammars;
rather, they are formally more powerful (or, as in the case of Chomsky's
theories, not formalized at all any more).  The use of CFGs as the
prototype of PSGs in the 60s introduced the notion that PS-based approaches
are incompatible with lexicon-based approaches, but this need not be true
if more powerful PS systems are used.

So why have PS at all?  Let me suggest the following (in Hans Uszkoreit's
spirit of conversation starters): it seems to me that a theory of syntax
(taken very narrowly as a theory of surface word order) cannot be
formulated (in a principles-and-parameters type methodological framework)
in a DG.  Take for example verb-second in German.  In a DG, can we do
anything more than state the facts in German?  The complimentary
distribution of the overt complementizer and the V2 effect in subordinate
clauses is then a mere observation.  In a PSG, we can suggest that the
complementizer and the finite verb in second position must be in the same
position (there being no other possibilities for the finite verb).  The
complementary distribution then follows as a verified prediction.  What is
crucial in this explanation is the notion that not only can we position
dependents with respect to their head, but we can also, independently,
position heads with respect to some other structure -- in this case PS.  (I
would be interested in hearing of any DG-based analyses of V2.)

Of course, while this very narrow area of syntax is exactly what Chomsky
considers "syntax" to be, MTT linguists (I have been told) consider it a
very small and rather unintersting problem, tackled prematurely.  This may
well be true. 

Owen Rambow
From stanley@uhunix.uhcc.Hawaii.Edu Thu Jul 22 08:33:25 1993
Received: from uhunix.uhcc.Hawaii.Edu by aisun1.ai.uga.edu with SMTP id AA18510
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Thu, 22 Jul 1993 08:33:25 -0400
Received: by uhunix.uhcc.Hawaii.Edu (4.1/Sun690)
	id AA02952; Thu, 22 Jul 93 02:33:23 HST
Date: Thu, 22 Jul 93 02:33:23 HST
From: Stanley Starosta  <stanley@uhunix.uhcc.Hawaii.Edu>
Message-Id: <9307221233.AA02952@uhunix.uhcc.Hawaii.Edu>
To: dg@ai.uga.edu
Subject: The abstractness and typology-specificity of dg again

     It  seems   that  my   conversation-starter  has  started  a
conversation.   I'll limit  my current  turn  to  addressing  the
following four points.

1. History:

     What did Hans Uszkoreit (hereafter `HU') `really' say at the
Prague conference?   My quotes of HU were taken from my notes and
recollection, and  the objections I raised in my posting were the
same ones I raised at the end of HU's conference presentation: as
I had  stated in  my own  presentation a  day or  so before,  the
lexicase version  of DG  has been  applied to  many languages (at
that time  I though  the figure was only about 45) and it is both
monostratal and  workable.  His reply, as I recall, was only that
he had  heard my  paper.  I took this to imply that he had simply
dismissed my claims.

     This question of attribution is of course not something that
can be  meaningfully debated.  Those on this net who attended the
conference know  more or  less who  said what,  and the rest will
have to  take their  pick from the two versions now on the table.

2. The inadequacy of DG

     "I claimed that the three basic grammar models each had
     to offer  something to  certain groups  of  researchers
     that the  others could  not provide."  [HU, 21  June DG
     email]

     So just  what is  it that  DG can't provide?  This is a very
important  claim,   one  that   I  have  heard  before  at  other
conferences, and  I would  very much like to respond to it, but I
don't  quite  see  what  it  is  that  DG is supposedly unable to
provide.

3. The typology-specificity of DG

     "I pointed out that it is no coincidence...

     ...that dependency  grammar models  have  traditionally
     been stronger  among linguists  working  on  Slavic  or
     Germanic languages  than  among  linguists  working  on
     English,

     ...that  phrase   structure  based   models  have  been
     stronger in Anglocentric parts of linguistics than in--
     let's say--Slavic linguistics circles, and

     ...that categorial  grammar models  have been strongest
     in circles  more concerned  with formal  semantics than
     with syntax."  [HU, 21 June DG email]

     `pointed out'   seems  to be an inappropriate turn of phrase
in this  context; an  unsupported assertion is not the same thing
as `pointing  out'.   When something is pointed out, the hearer's
attention is  called to  something which  thereupon is seen to be
clearly true.  When something is asserted, on the other hand, the
hearer waits  for the  supporting evidence  or argumentation.   I
don't see  that evidence  or argumentation here, and I didn't see
it in Prague in November.

     The phrase  `it is  no coincidence'  also seems infelicitous
here.   This phrase  is a  rhetorical device by which the speaker
either 1)  tells the  audience that they should be able to deduce
from the  information given  so far *why* it is no coincidence (I
can't), or 2) alerts the audience to the fact that he is about to
*tell* them  why it is no coincidence.  (As far as I can tell, he
hasn't.)

     Finally, if  `categorial grammar  models have been strongest
in  circles  more  concerned  with  formal  semantics  than  with
syntax', and  if that  means, as I infer from this statment, that
these circles  have ignored syntactic facts and problems in their
work, is  there any  reason  why  syntacticians  should  pay  any
attention to them?

4. The virtues of unification grammar

     Maybe this  is my  chance to  ask a  question  I  have  been
longing to  ask  for  several  years:  what  are  the  linguistic
advantages  of  UG?   Which  outstanding  problem   of  syntactic
description does  UG solve  that PSG  and DG  can't solve?  Which
construction in which language can be described more economically
and more  insightfully in UG than in other frameworks?  HU states
that:

     "The reason  for working  with combinations is obvious,
     it's the  large class  of mismatches  between  functor-
     argument structure,  functional structure,  and surface
     order found in language." [HU, 21 July DG email]

Once again,  what is  obvious to  HU and perhaps to others is not
obvious to  me.   If `functor-argument structure' and `functional
structure' are  separate abstract  levels of  structure which are
distinct from  `surface order',  then the  suspicion arises  that
the  `mismatches'  are merely  an artifact  of assuming  a multi-
stratal analysis.  Concrete examples would help here.

     Possibly HU  feels that  he has already answered my concerns
in statements such as:

     "There  is,  for  instance,  a  version  of  FUG  using
     variables in the feature PATTERN that could be used for
     doing recursion  on dependency structure by letting the
     value of CSET be the set of dependents."

     "I also pointed out the most problematic part of such a
     move, i.e.,  the modular formulation of principles that
     relationally constrain  the mapping  between dependency
     structure and  phonological  form."  [HU,  21  June  DG
     email]

     Unfortunately, these  statement mean nothing to me; I am not
familiar with  the literature  in this  area.  To become familiar
with it  would require  a major  effort on  my part, and before I
expend that much effort, I would like to have a reason to believe
it is  worth my  while.  One thing that might convince me that it
is worth  my while  would be  clear and  concrete answers  to the
points I have raised above:

     i) Exactly  which syntactic problems is DG unable to handle,
     and how can UG or CG solve these problems ?
     ii) Precisely what syntactic properties are common to German
     and Slavic  languages which  are not  shared by English, and
     how do  these shared  syntactic properties explain why it is
     not a concidence that DG has been associated with the former
     and not the latter?

An attempt  by a  UG or  CG' adherent to address these questions,
citing  real   data  from  real  languages  and  providing  full,
detailed, and  explicit syntax-to-phonology  analyses might be as
`effective    in    getting    people    to    react'    as    my
inflammatory/defamatory rhetoric has been.

     "It would  be nice if DG grammarians  would join in the
     very promising  dialogue between  CG and HPSG that  has
     recently been emerging. [HU, 21 July email]

     If this  dialogue involves  real language  data and explicit
analyses, count me in.

     Stan Starosta
From Bert=Peeters%GST%LW@cc3.kuleuven.ac.be Fri Jul 23 02:46:51 1993
Received: from cc3.kuleuven.ac.be by aisun1.ai.uga.edu with SMTP id AA27878
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Fri, 23 Jul 1993 02:46:51 -0400
Message-Id: <199307230646.AA27878@aisun1.ai.uga.edu>
Received: by cc3.kuleuven.ac.be with VINES ; Fri, 23 Jul 93 08:47:42 CET
Date: Fri, 23 Jul 93 08:43:31 CET
From: Bert=Peeters%GST%LW@cc3.kuleuven.ac.be
Subject: DG in French?
To: dg@ai.uga.edu
Cc: 

Could anyone on the list come up with a French name for DG (grammaire des 
dependances??) and possibly some literature in French? Thanks. Please reply 
privately - I fear there is not enough interest to pollute the net. My 
(temporary) account is as follows (until end November)

bp%gst%lw@cc3.kuleuven.ac.be

Bert Peeters

From stanley@uhunix.uhcc.Hawaii.Edu Sat Jul 24 08:39:08 1993
Received: from uhunix.uhcc.Hawaii.Edu by aisun1.ai.uga.edu with SMTP id AA05777
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Sat, 24 Jul 1993 08:39:08 -0400
Received: by uhunix.uhcc.Hawaii.Edu (4.1/Sun690)
	id AA23794; Sat, 24 Jul 93 02:39:00 HST
Date: Sat, 24 Jul 93 02:39:00 HST
From: Stanley Starosta  <stanley@uhunix.uhcc.Hawaii.Edu>
Message-Id: <9307241239.AA23794@uhunix.uhcc.Hawaii.Edu>
To: dg@ai.uga.edu
Subject: Unification grammar vs.(?) dependency grammar?

If unification is just `a way of handling arguments' (Michael Covington's
last posting), then I should revise one of the questions I asked of Hans 
Uszkoreit, and ask a further question of those on this net who use 
unification notation.
Revised question:
i) Exactly  which syntactic problems is DG unable to handle,
     and how can PSG or CG solve these problems ?
Further question:
iii) Does the use of unification notation entail any claims about the nature
of human language, or could it be replaced by some other mechanism for
`handling arguments on nodes' (e.g. the index copying used in lexicase DG)
without affecting the level of descriptive or explanatory adequacy achieved
by the grammatical theory in which the notation is embedded?
Stan Starosta
From ucleaar@ucl.ac.uk Sat Jul 24 16:57:29 1993
Received: from mail-a.bcc.ac.uk by aisun1.ai.uga.edu with SMTP id AA05939
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Sat, 24 Jul 1993 10:57:38 -0400
Received: from link-1.ts.bcc.ac.uk by mail-a.bcc.ac.uk with SMTP (PP) 
          id <23523-0@mail-a.bcc.ac.uk>; Sat, 24 Jul 1993 15:57:33 +0100
Received: by link-1.ts.bcc.ac.uk (AIX 3.2/UCB 5.64/4.03) id AA130269;
          Sat, 24 Jul 1993 15:57:30 +0100
From: ucleaar@ucl.ac.uk (Mr Andrew Rosta)
Message-Id: <9307241457.AA130269@link-1.ts.bcc.ac.uk>
To: dg@ai.uga.edu, Stanley Starosta <stanley@uhunix.uhcc.Hawaii.Edu>
Subject: Re: Unification grammar vs.(?) dependency grammar?
In-Reply-To: (Your message of Sat, 24 Jul 93 02:39:00 K.) <9307241239.AA23794@uhunix.uhcc.Hawaii.Edu>
Date: Sat, 24 Jul 93 15:57:29 +0100


Stan Starosta:
> i) Exactly  which syntactic problems is DG unable to handle,
>      and how can PSG or CG solve these problems ?

What about  gerunds, as in _Him teasing her upsets her_. It's
like a noun with respect to its head, and like a verb with
respect to its dependents. A PSG could handle this with:
  NP -> VP
So the gerund looks like: [NP [VP him teasing her]] upset her.

(Not that DG can't handle this: according to Dick Hudson the
_ing_ is a nominal clitic on which the verb _teas(e)_ depends;
according to me, _teasing_ is both a noun and a verb. But
both analyses have their problems, whereas PSG can manage it
easily.)
------
And
From stanley@uhunix.uhcc.Hawaii.Edu Tue Jul 27 08:21:17 1993
Received: from uhunix.uhcc.Hawaii.Edu by aisun1.ai.uga.edu with SMTP id AA24884
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Tue, 27 Jul 1993 08:21:17 -0400
Received: by uhunix.uhcc.Hawaii.Edu (4.1/Sun690)
	id AA21404; Tue, 27 Jul 93 02:19:43 HST
Date: Tue, 27 Jul 93 02:19:43 HST
From: Stanley Starosta  <stanley@uhunix.uhcc.Hawaii.Edu>
Message-Id: <9307271219.AA21404@uhunix.uhcc.Hawaii.Edu>
To: dg@ai.uga.edu
Subject: Gerunds vs.(?) DG

Gerunds are a nice example of the difference between PSG and DG.  First of
all, they clearly demonstrate that PSG in its original form is more powerful 
than (unaugmented) DG. As shown in And Rosta's rule NP --> VP, PSG 
allows an NP which does not contain a head N, and in general allows a whole
class of such `headless' constructions, but DG does not.  This PSG analysis
is also impossible in X-bar PSG imitations of DG, since it violates the 
basic X-bar configuration of Xn --> YP Xm, m<=n.  (Of course if the X-bar 
system is augmented with empty categories, anything becomes possible and the 
analysis becomes devoid of theoretical interest.)

Dick Hudson's solution of treating the _-ing_ of gerunds such as `teasing' 
in `Him teasing her upsets her' as 'a nominal clitic on which the verb 
_teas(e)_  depends' solves this formal problem, but raises further problems:
1) jury-rigging a `nominal clitic' for English just to solve this formal 
difficulty when there is no other such creature in English NPs and no 
evidence that _teasing_ is a sequence of two words seems quite ad hoc.  
(The possessive _'s_ might be a candidate for such an analysis, but of curse 
_'s_ attaches to a whole NP, not an N.)
2) _ing_ must be a suffix rather than a syntax-level clitic, since it carries 
over in  derivation:
Gerund
	a) Snapping twigs alerted the hunter to the approach of the bear.
`Mixed nominalizations'
	b) The snapping of twigs alerted the hunter to the approach of the 
           bear.
	c) The campers were kept awake by the snappings of twigs and other 
           sounds made by the nocturnal warthogs.
        d) The shootings of the hunters were claimed to be justified.
(Cf. meeting, wedding, christening, opening, ending.)
3) If the _-ing_ is said to be a nominal clitic at some more abstract level, 
then DG's great theoretical advantage of limited expressive power is 
forfeited.

I think And Rosta's suggestion that gerunds are both nouns and verbs is 
closer to a viable solution.  However, from a DG point of view, they have
to be more noun than verb, since they have a crucial distributional property
of nouns: they function as heads of noun phrases, with all the privileges 
and responsibilities accruing thereto.  I analyze them as abstract mass nouns 
which acquire some verbal valence slots in the process of lexical derivation:
                          |
                          |
        |                 upsets    |
        |                 |4ndex  | |
|       teasing | |       |+V     | her
|       |2ndex  | |       |2[+Nom]| |5ndex|
His     |+N     | her     |5[+Acc]| |+N   |
|1ndex| |+mass  | |3ndex|           |+Acc |
|+Det | |1[+Det]| |+N   |
        |3[+Acc]|

As mass nouns, they can't take the indefinite count determiner _a(n)_ or be 
pluralized (though they can when they are further derived into the Chomsky's 
class of `mixed nominalizations').  Note that they can take definite 
determiners, though:
     e) His teasing her upsets her.
     f) This teasing your sister has got to stop.
which is not accounted for by Rosta's NP --> VP rule, though it is by 
Schachter's version: NP --> (DET) VP, as I recall.

My analysis however so far fails to account for the following facts:
1) Why can't gerunds take a _the_ determiner?
     g) *The teasing her upsets her.
2) If gerunds are mass nouns, why can't they take _some_?
     h) *Some teasing her upsets her.
3) Why can they be preceded by an accusative NP?
     i) Him teasing her upsets her.
A partial explanation for this point comes from something which I would 
claim, based on looking at a lot of languages, to be universal: nouns do not 
allow nominative dependents, period.  However, I still have no way of 
explaining a pre-head slot for an accusative NP in English; it doesn't 
happen for other NPs, and it doesn't carry over from verbs (unless we want
to posit an NP-internal topic slot, which would have some other bad 
consequences).  Any suggestions which don't increase the expressive power of 
simple DG would be welcome.
--
Stan Starosta
From ucleaar@ucl.ac.uk Wed Jul 28 03:09:28 1993
Received: from mail-a.bcc.ac.uk by aisun1.ai.uga.edu with SMTP id AA29886
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Tue, 27 Jul 1993 21:09:37 -0400
Received: from link-1.ts.bcc.ac.uk by mail-a.bcc.ac.uk with SMTP (PP) 
          id <18205-0@mail-a.bcc.ac.uk>; Wed, 28 Jul 1993 02:09:32 +0100
Received: by link-1.ts.bcc.ac.uk (AIX 3.2/UCB 5.64/4.03) id AA106347;
          Wed, 28 Jul 1993 02:09:28 +0100
From: ucleaar@ucl.ac.uk (Mr Andrew Rosta)
Message-Id: <9307280109.AA106347@link-1.ts.bcc.ac.uk>
To: dg@ai.uga.edu, Stanley Starosta <stanley@uhunix.uhcc.Hawaii.Edu>
Subject: Re: Gerunds vs.(?) DG
In-Reply-To: (Your message of Tue, 27 Jul 93 02:19:43 K.) <9307271219.AA21404@uhunix.uhcc.Hawaii.Edu>
Date: Wed, 28 Jul 93 02:09:28 +0100


Stan Starosta writes:
> Dick Hudson's solution of treating the _-ing_ of gerunds such as `teasing' 
> in `Him teasing her upsets her' as 'a nominal clitic on which the verb 
> _teas(e)_  depends' solves this formal problem, but raises further problems:
> 1) jury-rigging a `nominal clitic' for English just to solve this formal 
> difficulty when there is no other such creature in English NPs and no 
> evidence that _teasing_ is a sequence of two words seems quite ad hoc.  
> (The possessive _'s_ might be a candidate for such an analysis, but of curse 
> _'s_ attaches to a whole NP, not an N.)

[The silence from Dick is due to him/his being on holiday.]

The Word Grammar view is that 'S is a determiner. It's only phonologically
a clitic; in other respects. it's like any other word. Dick's gerund
ing clitic, by contrast must be attached to its verbal complement.
So indeed there is nothing else in English like this gerund ing, 
as far as I am aware. (But the analysis is nonetheless ingenious.)

> 2) _ing_ must be a suffix rather than a syntax-level clitic, since it carries 
> over in  derivation:
> Gerund
> 	a) Snapping twigs alerted the hunter to the approach of the bear.
> `Mixed nominalizations'
> 	b) The snapping of twigs alerted the hunter to the approach of the 
>            bear.
> 	c) The campers were kept awake by the snappings of twigs and other 
>            sounds made by the nocturnal warthogs.
>         d) The shootings of the hunters were claimed to be justified.
> (Cf. meeting, wedding, christening, opening, ending.)

Two different ings, in Dick's view (I may be wrong on this); it's
coincidental that the nominalizing morpheme is phonologically
identical to the gerund morpheme.

> 3) If the _-ing_ is said to be a nominal clitic at some more abstract level, 
> then DG's great theoretical advantage of limited expressive power is 
> forfeited.

It's not a nominal clitic at some more abstract level - it's
a nominal clitic at the only level there is (which, roughly
speaking, is what is there on the page).
 
> I think And Rosta's suggestion that gerunds are both nouns and verbs is 
> closer to a viable solution.  

Not surprisingly I think Stan is correct in thinking this...

> However, from a DG point of view, they have
> to be more noun than verb, since they have a crucial distributional property
> of nouns: they function as heads of noun phrases, with all the privileges 
> and responsibilities accruing thereto.  

In my view, 'noun' is a category useful only for determining its
members' behaviour as dependents. For example, the object of some
verbs must be a noun, but can be any kind of noun. But as heads
there are no common properties that all nouns have. All common
nouns share certain properties as heads (e.g. modification
by relative clause) but as this property shows, gerunds are not
common nouns:
    *him teasing her that upset her surprised me

> I analyze them as abstract mass nouns 
> which acquire some verbal valence slots in the process of lexical derivation:
>                           |
>                           |
>         |                 upsets    |
>         |                 |4ndex  | |
> |       teasing | |       |+V     | her
> |       |2ndex  | |       |2[+Nom]| |5ndex|
> His     |+N     | her     |5[+Acc]| |+N   |
> |1ndex| |+mass  | |3ndex|           |+Acc |
> |+Det | |1[+Det]| |+N   |
>         |3[+Acc]|
> 
> As mass nouns, they can't take the indefinite count determiner _a(n)_ or be 
> pluralized (though they can when they are further derived into the Chomsky's 
> class of `mixed nominalizations').  Note that they can take definite 
> determiners, though:
>      e) His teasing her upsets her.
>      f) This teasing your sister has got to stop.
> which is not accounted for by Rosta's NP --> VP rule, though it is by 
> Schachter's version: NP --> (DET) VP, as I recall.
> 
> My analysis however so far fails to account for the following facts:
> 1) Why can't gerunds take a _the_ determiner?
>      g) *The teasing her upsets her.
> 2) If gerunds are mass nouns, why can't they take _some_?
>      h) *Some teasing her upsets her.
> 3) Why can they be preceded by an accusative NP?
>      i) Him teasing her upsets her.
> A partial explanation for this point comes from something which I would 
> claim, based on looking at a lot of languages, to be universal: nouns do not 
> allow nominative dependents, period.  However, I still have no way of 
> explaining a pre-head slot for an accusative NP in English; it doesn't 
> happen for other NPs, and it doesn't carry over from verbs (unless we want
> to posit an NP-internal topic slot, which would have some other bad 
> consequences).  Any suggestions which don't increase the expressive power of 
> simple DG would be welcome.

Okay:
(1) Determiners are heads in NPs. Most determiners subcategorize
for a common noun as complement. Exceptionally, possessive
determiners also select the gerund clitic _ing_ (Hudson's
analysis) or an ing-participle (my analysis). _The_ selects
only common nouns.
(2) Same reason as (1).
(3) In (i) _him_ is just the subject. It's not just a characteristic
of gerunds - cf:
  (ii) Him teasing her terribly, she got upset.
Verbs in general can have subjects,  and gerunds/ing-participles
are no exception. Note that as a head, the gerund behaves as 
a verb (provided we take determiners as heads of the following
noun).  As for 'him' rather than 'he', 'he' is only
ever subject of a tensed/finite verb. The evidence for saying
that English has nominative/accusative case, as opposed to
five personal pronouns that take a different form when subject
of a tensed verb, is pretty slender (- to be more precise, I'm
not aware of any evidence at all).

[It's good to be talking to people who aren't baffled by the
very notion of dependency grammar.]

-------
And Rosta
From stanley@uhunix.uhcc.Hawaii.Edu Thu Jul 29 08:38:32 1993
Received: from uhunix.uhcc.Hawaii.Edu by aisun1.ai.uga.edu with SMTP id AA12546
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Thu, 29 Jul 1993 08:38:32 -0400
Received: by uhunix.uhcc.Hawaii.Edu (4.1/Sun690)
	id AA05633; Thu, 29 Jul 93 02:37:30 HST
Date: Thu, 29 Jul 93 02:37:30 HST
From: Stanley Starosta  <stanley@uhunix.uhcc.Hawaii.Edu>
Message-Id: <9307291237.AA05633@uhunix.uhcc.Hawaii.Edu>
To: dg@ai.uga.edu
Subject: more English gerunds
Cc: .@uhunix.uhcc.Hawaii.Edu

Key: 
> Stan Starosta's previous mailing
un-angled and unindented: And Rosta's reactions
     {Indented: Stan Starosta's re-actions}
--
> 2) _ing_ must be a suffix rather than a syntax-level clitic, since it carries
> over in  derivation:
> Gerund
>       a) Snapping twigs alerted the hunter to the approach of the bear.
> `Mixed nominalizations'
>       b) The snapping of twigs alerted the hunter to the approach of the
>            bear.
>       c) The campers were kept awake by the snappings of twigs and other
>            sounds made by the nocturnal warthogs.
>         d) The shootings of the hunters were claimed to be justified.
> (Cf. meeting, wedding, christening, opening, ending.)

Two different ings, in Dick's view (I may be wrong on this); it's
coincidental that the nominalizing morpheme is phonologically
identical to the gerund morpheme.
     {But if the similarity is coincidental, then it is unexplained.  The
     analysis which claims that the gerundival -_ing_ is an affix then has the
     advantage that it explains the similarity: the -_ing_'s are identical in
     form because they are etymologically the same; the -_ing_ in the mixed 
     nominalizations and in _wedding_, etc.,  is the gerundival -_ing_ which 
     was part of an abstract noun which underwent a subsequent syntactic and 
     semantic lexical derivation process.}

> 3) If the _-ing_ is said to be a nominal clitic at some more abstract level,
> then DG's great theoretical advantage of limited expressive power is
> forfeited.

It's not a nominal clitic at some more abstract level - it's
a nominal clitic at the only level there is (which, roughly
speaking, is what is there on the page).

     {I'm delighted to see that Word Grammar is resisting the temptation to
     `go underlying'.}

> I think And Rosta's suggestion that gerunds are both nouns and verbs is
> closer to a viable solution.

Not surprisingly I think Stan is correct in thinking this...

> However, from a DG point of view, they have
> to be more noun than verb, since they have a crucial distributional property
> of nouns: they function as heads of noun phrases, with all the privileges
> and responsibilities accruing thereto.

In my view, 'noun' is a category useful only for determining its
members' behaviour as dependents. 
     {Of course; what more could a dependency grammarian want?  So then any
     word which a regent (`head') word regards as a noun is a noun, right?
     Some other properties shared by nouns as dependents:
     i) all nouns, including gerunds, have case forms (Nominative, Accusative,
     etc.) which may or may not be morphologically marked.  When they *are* 
     morphologically marked for case (would that be true of Latin gerunds?), 
     then this is more than just `behaviour as a dependent'.
     ii) All nouns, including gerunds, either bear a case relation (`thematic
     relation' in Chomskyan terms) or function as predicates; no non-noun 
     bears a case relation.
     iii) All English nouns are grammatically either singular or plural, and
     either first, second, or third person.  (Again, to the extent that this
     category is morphologically marked (as in Latin?), this is more than 
     `mere' behavior as a dependent.)  Note in this connection that 
     agreement facts are unambiguous in revealing English gerunds to be 
     third person singular:
          c) Mildred's behaving like an idiot shocks/*shock her mother.
          d) This/*These teasing your sister must stop instantly!}

For example, the object of some verbs must be a noun, but can be any kind of 
noun. But as heads there are no common properties that all nouns have. 
     {This may be right; I wonder if the situation is any different for 
     verbs, especially if we consider auxiliary verbs, which I believe count 
     as verbs in Word Grammar (as they should).}

All common nouns share certain properties as heads (e.g. modification
by relative clause) but as this property shows, gerunds are not
common nouns:
    *him teasing her that upset her surprised me

     {I can use them with non-restrictive clauses, I think:
          e) His teasing her, which was getting on everyone's nerves, 
          finally went to far.
          f) I finally tried teasing her, which I hadn't attempted before.
     The fact that they don't take restrictive relative clauses indicates 
     something about their semantics (maybe they are lexically specific or
     or something like that) rat thabout their syntax, I suspect.}
 
> I analyze them as abstract mass nouns which acquire some verbal valence 
slots in the process of lexical derivation:
From stanley@uhunix.uhcc.Hawaii.Edu Thu Jul 29 08:48:15 1993
Received: from uhunix.uhcc.Hawaii.Edu by aisun1.ai.uga.edu with SMTP id AA12576
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Thu, 29 Jul 1993 08:48:15 -0400
Received: by uhunix.uhcc.Hawaii.Edu (4.1/Sun690)
	id AA06161; Thu, 29 Jul 93 02:47:17 HST
Date: Thu, 29 Jul 93 02:47:17 HST
From: Stanley Starosta  <stanley@uhunix.uhcc.Hawaii.Edu>
Message-Id: <9307291247.AA06161@uhunix.uhcc.Hawaii.Edu>
To: dg@ai.uga.edu
Subject: more English gerunds
Cc: .@uhunix.uhcc.Hawaii.Edu

Key: 
> Stan Starosta's previous mailing
un-angled and unindented: And Rosta's reactions
     {Indented: Stan Starosta's re-actions}
--
> 2) _ing_ must be a suffix rather than a syntax-level clitic, since it carries
> over in  derivation:
> Gerund
>       a) Snapping twigs alerted the hunter to the approach of the bear.
> `Mixed nominalizations'
>       b) The snapping of twigs alerted the hunter to the approach of the
>            bear.
>       c) The campers were kept awake by the snappings of twigs and other
>            sounds made by the nocturnal warthogs.
>         d) The shootings of the hunters were claimed to be justified.
> (Cf. meeting, wedding, christening, opening, ending.)

Two different ings, in Dick's view (I may be wrong on this); it's
coincidental that the nominalizing morpheme is phonologically
identical to the gerund morpheme.
     {But if the similarity is coincidental, then it is unexplained.  The
     analysis which claims that the gerundival -_ing_ is an affix then has the
     advantage that it explains the similarity: the -_ing_'s are identical in
     form because they are etymologically the same; the -_ing_ in the mixed 
     nominalizations and in _wedding_, etc.,  is the gerundival -_ing_ which 
     was part of an abstract noun which underwent a subsequent syntactic and 
     semantic lexical derivation process.}

> 3) If the _-ing_ is said to be a nominal clitic at some more abstract level,
> then DG's great theoretical advantage of limited expressive power is
> forfeited.

It's not a nominal clitic at some more abstract level - it's
a nominal clitic at the only level there is (which, roughly
speaking, is what is there on the page).

     {I'm delighted to see that Word Grammar is resisting the temptation to
     `go underlying'.}

> I think And Rosta's suggestion that gerunds are both nouns and verbs is
> closer to a viable solution.

Not surprisingly I think Stan is correct in thinking this...

> However, from a DG point of view, they have
> to be more noun than verb, since they have a crucial distributional property
> of nouns: they function as heads of noun phhe privileges
> and responsibilities accruing thereto.

In my view, 'noun' is a category useful only for determining its
members' behaviour as dependents. 
     {Of course; what more could a dependency grammarian want?  So then any
     word which a regent (`head') word regards as a noun is a noun, right?
     Some other properties shared by nouns as dependents:
     i) all nouns, including gerunds, have case forms (Nominative, Accusative,
     etc.) which may or may not be morphologicamarked.  When they *are* 
     morphologically marked for case (would that be true of Latin gerunds?), 
     then this is more than just `behaviour as a dependent'.
     ii) All nouns, including gerunds, either bear a case relation (`thematic
     relation' in Chomskyan terms) or function as predicates; no non-noun 
     bears a case relation.
     iii) All English nouns are grammatically either singular or plural, and
     either first, second, or third person.  (Again, to the extent that this
     category is morphologically marked (as in Latin?), this is more than 
     `mere' behavior as a dependent.)  Note in this connection that 
     agreement facts are unambiguous in revealing English gerunds to be 
     third person singular:
          c) Mildred's behaving like an idiot shocks/*shock her mother.
          d) This/*These teasing your sister must stop instantly!}

For example, the object of some verbs must be a noun, but can be any kind of 
noun. But as heads there are no common properties that all nouns have. 
     {This may be right; I wonder if the situation is any different for 
     verbs, especially if we consider auxiliary verbs, which I believe count 
     as verbs in Word Grammar (as they should).}

All common nouns share certain properties as heads (e.g. modification
by relative clause) but as this property shows, gerunds are not
common nouns:
    *him teasing her that upset her surprised me

     {I can use them with non-restrictive clauses, I think:
          e) His teasing her, which was getting on everyone's nerves, 
          finally went to far.
          f) I finally tried teasing her, which I hadn't attempted before.
     The fact that they don't take restrictive relative clauses indicates 
     something about their semantics (maybe they are lexically specific or
     or something like that) rather than about their syntax, I suspect.}
 
> I analyze them as abstract mass nouns which acquire some verbal valence 
slots in the process of lexical derivation:
From stanley@uhunix.uhcc.Hawaii.Edu Thu Jul 29 08:50:54 1993
Received: from uhunix.uhcc.Hawaii.Edu by aisun1.ai.uga.edu with SMTP id AA12607
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Thu, 29 Jul 1993 08:50:54 -0400
Received: by uhunix.uhcc.Hawaii.Edu (4.1/Sun690)
	id AA06311; Thu, 29 Jul 93 02:50:03 HST
Date: Thu, 29 Jul 93 02:50:03 HST
From: Stanley Starosta  <stanley@uhunix.uhcc.Hawaii.Edu>
Message-Id: <9307291250.AA06311@uhunix.uhcc.Hawaii.Edu>
To: dg@ai.uga.edu
Subject: more English gerunds
Cc: .@uhunix.uhcc.Hawaii.Edu

Key: 
> Stan Starosta's previous mailing
un-angled and unindented: And Rosta's reactions
     {Indented: Stan Starosta's re-actions}
--
> 2) _ing_ must be a suffix rather than a syntax-level clitic, since it carries
> over in  derivation:
> Gerund
>       a) Snapping twigs alerted the hunter to the approach of the bear.
> `Mixed nominalizations'
>       b) The snapping of twigs alerted the hunter to the approach of the
>            bear.
>       c) The campers were kept awake by the snappings of twigs and other
>            sounds made by the nocturnal warthogs.
>         d) The shootings of the hunters were claimed to be justified.
> (Cf. meeting, wedding, christening, opening, ending.)

Two different ings, in Dick's view (I may be wrong on this); it's
coincidental that the nominalizing morpheme is phonologically
identical to the gerund morpheme.
     {But if the similarity is coincidental, then it is unexplained.  The
     analysis which claims that the gerundival -_ing_ is an affix then has the
     advantage that it explains the similarity: the -_ing_'s are identical in
     form because they are etymologically the same; the -_ing_ in the mixed 
     nominalizations and in _wedding_, etc.,  is the gerundival -_ing_ which 
     was part of an abstract noun which underwent a subsequent syntactic and 
     semantic lexical derivation process.}

> 3) If the _-ing_ is said to be a nominal clitic at some more abstract level,
> then DG's great theoretical advantage of limited expressive power is
> forfeited.

It's not a nominal clitic at some more abstract level - it's
a nominal clitic at the only level there is (which, roughly
speaking, is what is there on the page).

     {I'm delighted to see that Word Grammar is resisting the temptation to
     `go underlying'.}

> I think And Rosta's suggestion that gerunds are both nouns and verbs is
> closer to a viable solution.

Not surprisingly I think Stan is correct in thinking this...

> However, from a DG point of view, they have
> to be more noun than verb, since they have a crucial distributional property
> of nouns: they function as heads of noun phrases, with all the privileges
> and responsibilities accruing thereto.

In my view, 'noun' is a category useful only for determining its
members' behaviour as dependents. 
     {Of course; what more could a dependency grammarian want?  So then any
     word which a regent (`head') word regards as a noun is a noun, right?
     Some other properties shared by nouns as dependents:
     i) all nouns, including gerunds, have case forms (Nominative, Accusative,
     etc.) which may or may not be morphologically marked.  When they *are* 
     morphologically marked for case (would that be true of Latin gerunds?), 
     then this is more than just `behaviour as a dependent'.
     ii) All nouns, including gerunds, either bear a case relation (`thematic
     relation' in Chomskyan terms) or function as predicates; no non-noun 
     bears a case relation.
     iii) All English nouns are grammatically either singular or plural, and
     either first, second, or third person.  (Again, to the extent that this
     category is morphologically marked (as in Latin?), this is more than 
     `mere' behavior as a dependent.)  Note in this connection that 
     agreement facts are unambiguous in revealing English gerunds to be 
     third person singular:
          c) Mildred's behaving like an idiot shocks/*shock her mother.
          d) This/*These teasing your sister must stop instantly!}

For example, the object of some verbs must be a noun, but can be any kind of 
noun. But as heads there are no common properties that all nouns have. 
     {This may be right; I wonder if the situation is any different for 
     verbs, especially if we consider auxiliary verbs, which I believe count 
     as verbs in Word Grammar (as they should).}

All common nouns share certain properties as heads (e.g. modification
by relative clause) but as this property shows, gerunds are not
common nouns:
    *him teasing her that upset her surprised me

     {I can use them with non-restrictive clauses, I think:
          e) His teasing her, which was getting on everyone's nerves, 
          finally went to far.
          f) I finally tried teasing her, which I hadn't attempted before.
     The fact that they don't take restrictive relative clauses indicates 
     something about their semantics (maybe they are lexically specific or
     or something like that) rather than about their syntax, I suspect.}
 
> I analyze them as abstract mass nouns which acquire some verbal valence 
slots in the process of lexical derivation:
From stanley@uhunix.uhcc.Hawaii.Edu Thu Jul 29 08:56:02 1993
Received: from uhunix.uhcc.Hawaii.Edu by aisun1.ai.uga.edu with SMTP id AA12674
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Thu, 29 Jul 1993 08:56:02 -0400
Received: by uhunix.uhcc.Hawaii.Edu (4.1/Sun690)
	id AA06565; Thu, 29 Jul 93 02:55:08 HST
Date: Thu, 29 Jul 93 02:55:08 HST
From: Stanley Starosta  <stanley@uhunix.uhcc.Hawaii.Edu>
Message-Id: <9307291255.AA06565@uhunix.uhcc.Hawaii.Edu>
To: dg@ai.uga.edu
Subject: more on English gerunds (full version)
Cc: .@uhunix.uhcc.Hawaii.Edu

Key: 
> Stan Starosta's previous mailing
un-angled and unindented: And Rosta's reactions
     {Indented: Stan Starosta's re-actions}
--
> 2) _ing_ must be a suffix rather than a syntax-level clitic, since it carries
> over in  derivation:
> Gerund
>       a) Snapping twigs alerted the hunter to the approach of the bear.
> `Mixed nominalizations'
>       b) The snapping of twigs alerted the hunter to the approach of the
>            bear.
>       c) The campers were kept awake by the snappings of twigs and other
>            sounds made by the nocturnal warthogs.
>         d) The shootings of the hunters were claimed to be justified.
> (Cf. meeting, wedding, christening, opening, ending.)

Two different ings, in Dick's view (I may be wrong on this); it's
coincidental that the nominalizing morpheme is phonologically
identical to the gerund morpheme.
     {But if the similarity is coincidental, then it is unexplained.  The
     analysis which claims that the gerundival -_ing_ is an affix then has the
     advantage that it explains the similarity: the -_ing_'s are identical in
     form because they are etymologically the same; the -_ing_ in the mixed 
     nominalizations and in _wedding_, etc.,  is the gerundival -_ing_ which 
     was part of an abstract noun which underwent a subsequent syntactic and 
     semantic lexical derivation process.}

> 3) If the _-ing_ is said to be a nominal clitic at some more abstract level,
> then DG's great theoretical advantage of limited expressive power is
> forfeited.

It's not a nominal clitic at some more abstract level - it's
a nominal clitic at the only level there is (which, roughly
speaking, is what is there on the page).

     {I'm delighted to see that Word Grammar is resisting the temptation to
     `go underlying'.}

> I think And Rosta's suggestion that gerunds are both nouns and verbs is
> closer to a viable solution.

Not surprisingly I think Stan is correct in thinking this...

> However, from a DG point of view, they have
> to be more noun than verb, since they have a crucial distributional property
> of nouns: they function as heads of noun phrases, with all the privileges
> and responsibilities accruing thereto.

In my view, 'noun' is a category useful only for determining its
members' behaviour as dependents. 
     {Of course; what more could a dependency grammarian want?  So then any
     word which a regent (`head') word regards as a noun is a noun, right?
     Some other properties shared by nouns as dependents:
     i) all nouns, including gerunds, have case forms (Nominative, Accusative,
     etc.) which may or may not be morphologically marked.  When they *are* 
     morphologically marked for case (would that be true of Latin gerunds?), 
     then this is more than just `behaviour as a dependent'.
     ii) All nouns, including gerunds, either bear a case relation (`thematic
     relation' in Chomskyan terms) or function as predicates; no non-noun 
     bears a case relation.
     iii) All English nouns are grammatically either singular or plural, and
     either first, second, or third person.  (Again, to the extent that this
     category is morphologically marked (as in Latin?), this is more than 
     `mere' behavior as a dependent.)  Note in this connection that 
     agreement facts are unambiguous in revealing English gerunds to be 
     third person singular:
          c) Mildred's behaving like an idiot shocks/*shock her mother.
          d) This/*These teasing your sister must stop instantly!}

For example, the objeFrom stanley@uhunix.uhcc.Hawaii.Edu Thu Jul 29 08:57:16 1993
Received: from uhunix.uhcc.Hawaii.Edu by aisun1.ai.uga.edu with SMTP id AA12685
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Thu, 29 Jul 1993 08:57:16 -0400
Received: by uhunix.uhcc.Hawaii.Edu (4.1/Sun690)
	id AA06670; Thu, 29 Jul 93 02:57:11 HST
Date: Thu, 29 Jul 93 02:57:11 HST
From: Stanley Starosta  <stanley@uhunix.uhcc.Hawaii.Edu>
Message-Id: <9307291257.AA06670@uhunix.uhcc.Hawaii.Edu>
To: dg@ai.uga.edu
Subject: truncated English gerunds file

[I'm having uploading problems; I'll try again tomorrow.]
Stan Starosta
ct of some verbs must be a noun, but can be any kind of 
noun. But as heads there are no common properties that all nouns have. 
     {This may be right; I wonder if the situation is any different for 
     verbs, especially if we consider auxiliary verbs, which I believe count 
     as verbs in Word Grammar (as they should).}

All common nouns share certain properties as heads (e.g. modification
by relative clause) but as this property shows, gerunds are not
common nouns:
    *him teasing her that upset her surprised me

     {I can use them with non-restrictive clauses, I think:
          e) His teasing her, which was getting on everyone's nerves, 
          finally went to far.
          f) I finally tried teasing her, which I hadn't attempted before.
     The fact that they don't take restrictive relative clauses indicates 
     something about their semantics (maybe they are lexically specific or
     or something like that) rather than about their syntax, I suspect.}
 
> I analyze them as abstract mass nouns which acquire some verbal valence 
slots in the process of lexical derivation:
From Bert=Peeters%GST%LW@cc3.kuleuven.ac.be Thu Jul 29 11:36:59 1993
Received: from cc3.kuleuven.ac.be by aisun1.ai.uga.edu with SMTP id AA14036
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Thu, 29 Jul 1993 11:36:59 -0400
Message-Id: <199307291536.AA14036@aisun1.ai.uga.edu>
Received: by cc3.kuleuven.ac.be with VINES ; Thu, 29 Jul 93 17:38:08 CET
Date: Thu, 29 Jul 93 17:31:19 CET
From: Bert=Peeters%GST%LW@cc3.kuleuven.ac.be
Subject: Dependency grammar in French     
To: dg@ai.uga.edu
Cc: 

A few days ago I asked whether there was literature in French and/or a French 
name for dependency grammar. Two replies were received, from Stan Starosta 
and Hugues de Mazancourt, respectively. The French term seems to be 
"grammaire de de'pendance". The following references were supplied:

1) Lucien Tesniere, Elements de syntaxe structurale, Paris, Klincksieck 1959

2) Helmut Schumacher, Valenzbibliographie, Mannheim, Institut fur deutsche
   Sprache
   -> index entries "Franzoesisch" and "Dependenzgrammatik"

3) French translations (done or at least revised by the author himself) of
   works by Igor Mel'cuk.

Thanks for the info, which I was asked quite explicitly to share with the 
entire readership of the list.
Bert Peeters

From stanley@uhunix.uhcc.Hawaii.Edu Fri Jul 30 08:16:44 1993
Received: from uhunix.uhcc.Hawaii.Edu by aisun1.ai.uga.edu with SMTP id AA18732
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Fri, 30 Jul 1993 08:16:44 -0400
Received: by uhunix.uhcc.Hawaii.Edu (4.1/Sun690)
	id AA20312; Fri, 30 Jul 93 02:16:40 HST
Date: Fri, 30 Jul 93 2:16:39 HST
From: Stanley Starosta <stanley@uhunix.uhcc.Hawaii.Edu>
To: dg@ai.uga.edu
Subject: more on English gerunds, part 2
Message-Id: <CMM.0.90.2.744034599.stanley@uhunix.uhcc.Hawaii.Edu>

> I analyze them as abstract mass nouns which acquire some verbal valence 
slots in the process of lexical derivation:
= = = = =
[Continued from my posting of yesterday]
.
.
> My analysis however so far fails to account for the following facts:
> 1) Why can't gerunds take a _the_ determiner?
>      g) *The teasing her upsets her.
> 2) If gerunds are mass nouns, why can't they take _some_?
>      h) *Some teasing her upsets her.
> 3) Why can they be preceded by an accusative NP?
>      i) Him teasing her upsets her.
> A partial explanation for this point comes from something which I would
> claim, based on looking at a lot of languages, to be universal: nouns do not
> allow nominative dependents, period.  However, I still have no way of
> explaining a pre-head slot for an accusative NP in English; it doesn't
> happen for other NPs, and it doesn't carry over from verbs (unless we want
> to posit an NP-internal topic slot, which would have some other bad
> consequences).  Any suggestions which don't increase the expressive power of
> simple DG would be welcome.

Okay:
(1) Determiners are heads in NPs. Most determiners subcategorize
for a common noun as complement. Exceptionally, possessive
determiners also select the gerund clitic _ing_ (Hudson's
analysis) or an ing-participle (my analysis). _The_ selects
only common nouns.
(2) Same reason as (1).
     {Yes, I can do that too, without assuming the `determiners as heads'
     analysis, just by marking gerunds as nouns which don't allow article 
     dependents; either way, it is just a stipulation, not an explanation, 
     and not very confidence-inspiring.}
(3) In (i) _him_ is just the subject. 
     {In my analysis (though not in Chomsky's), _him_ can't be a subject 
     because it is not a noun.  We know that it is not a noun but a determiner 
     because it alternates with _my_, _your_, _her_, _our_, and _their_, 
     which can never be the sole N of an NP in English.  (It would be 
     interesting to see how a determiner-as-noun analysis accounts
     for this distributional gap.  Stipulation?}
It's not just a characteristic of gerunds - cf:
  (ii) Him teasing her terribly, she got upset.
     {Can you really say this?  If I force myself, I can only get it as a 
     kind of `ablative absolute', where the second clause is the main finite 
     clause and the first is a subordinate participle phrase, e.g.
          g) The moon being full, Caesar's landing craft sailed in on the
          full tide.}

Verbs in general can have subjects,
     {if they are not impersonal or infinitives, of course}
and gerunds/ing-participles are no exception. 
     {I'm not yet convinced of that.}
Note that as a head, the gerund behaves as
a verb (provided we take determiners as heads of the following
noun).
     {Then would the non-restrictive relative clauses depend on the 
     determiners?}
As for 'him' rather than 'he', 'he' is only
ever subject of a tensed/finite verb. 
     {Of course; not every language has tense as a grammatical category, of 
     course, but I hereby claim as a universal generalization that only
     finite clauses can contain subjects at all, and that *every* subject must 
     be nominative.  Analyses which claim the contrary, such as Chomsky's 
     exceptional case marking analysis, can all (I claim; counterexamples
     gladly considered) be shown to be inferior to analyses which maintain
     this generalization.}

The evidence for saying that English has nominative/accusative case, as 
opposed to five personal pronouns that take a different form when subject
of a tensed verb,
     {And what label would you assign to these five pronouns to distinguish
     them from their subject-selected counterparts?  Universal considerations
     seem to dictate `Accusative'.}
is pretty slender (- to be more precise, I'm not aware of any evidence at all).
     {Nominative and Accusative are of course not lexically marked categories
     in English except for those five conservative pronouns; rather, nouns are
     assigned case by their regents, as shown in my stemma for `His teasing
     her upsets her'.  The evidence for this analysis comes from universal
     grammar; in order to make maximally simple cross-linguistic 
     generalizations about unbounded dependencies, word order, subject-
     verb agreement, morphological markedness, NP omissibility, and case 
     relation assignment, especially generalizations that apply across 
     accusative and ergative typologies, it is necessary to assume as a 
     minimum assumption that every noun in every language is grammatically
     identified as either Nominative or not Nominative.
     The above statement is of course `evidence' only in a nascent sense.  
     To make it real evidence, we need alternatives to compare it with.  We
     might start a competition on the net: one person would propose a
     universal generalization about subjects, infinitives, unbounded 
     dependencies, clause-level word order, subject-verb agreement, 
     or ergativity which did not assume universal +/-Nom marking on nouns, 
     say, and then someone else would provide data from some natural language
     demonstrating that the generalization was invalid, and/or try to 
     propose an alternative which did assume universal +/-Nom marking and 
     which was superior in terms of the number and quality of the language-
     internal and/or cross-linguistic generalizations it was able to capture. 
     Tennis, anyone?}
     
     {Stan Starosta}
From stanley@uhunix.uhcc.Hawaii.Edu Sat Jul 31 08:37:10 1993
Received: from uhunix.uhcc.Hawaii.Edu by aisun1.ai.uga.edu with SMTP id AA26656
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Sat, 31 Jul 1993 08:37:10 -0400
Received: by uhunix.uhcc.Hawaii.Edu (4.1/Sun690)
	id AA20601; Sat, 31 Jul 93 02:37:07 HST
Date: Sat, 31 Jul 93 2:37:06 HST
From: Stanley Starosta <stanley@uhunix.uhcc.Hawaii.Edu>
To: dg@ai.uga.edu
Subject: DG, PSG, and German V2s
Message-Id: <CMM.0.90.2.744122226.stanley@uhunix.uhcc.Hawaii.Edu>

reply by Stan Starosta to:
Date: Tue, 20 Jul 93 11:25:16 EDT
From: rambow@unagi.cis.upenn.edu (Owen Rambow)
Subject: DG / PSG

Key:
unindented text: Owen Rambow
     {indented and stemma: Stan Starosta}

So why have PS at all?  Let me suggest the following (in Hans Uszkoreit's
spirit of conversation starters): it seems to me that a theory of syntax
(taken very narrowly as a theory of surface word order) cannot be
formulated (in a principles-and-parameters type methodological framework)
in a DG.  Take for example verb-second in German.  In a DG, can we do
anything more than state the facts in German?
     {Let me take a shot at it.  The analysis of the German verb-second 
     phenomenon that I would like to place on the table for discussion is 
     something like the following:
     1) German finite clauses are either root or non-root (cf. Emonds 1976).
     2) Non-root clauses are verb-final.  (I have no synchronic explanation 
     for this fact or for point 3); I would assume that this has a historical 
     explanation rather than a synchronic explanation.)
     3) Root clauses are verb-initial except for allowing for a preverbal
     Theme (`topic') dependent in non-interrogative clauses [where 
     `interrogative' refers to polar interrogatives, i.e. yes-no questions].
     The Theme must be linked with some complement or adjunct valence slot 
     provided by the verb, and the subject slot (?[+Nom]) is one such valence
     slot which is available for linking.  In this respect German root 
     clauses are like clauses in verb-initial Philippine languages such as 
     Tagalog.
     4) German differs from Tagalog in the requirement that non-interrogative
     root clause requires a topic, rather than just allowing one.
     5) The explanatory force of this proposal then lies in the claim that
     external Themes (`topics') are associated with the root clause, a claim
     that can be supported with data from many languages, including e.g.
     Chinese and Japanese, I think.  Here is the formal statement of this
     analysis, somewhat simplified for our purposes:}

     {Subcategorization rules:

     SR-1.
     [+V]  --> [+/-fint]          ;verbs are finite or non-finite

     SR-2.
     [+fint] --> [+/-root]        ;finite verbs are root or non-root.

     SR-3.
     |+fint| --> [+/-ntrg]        ;finite root verbs are interrogative or 
     |+root|                      not.

     Redundancy rules:

     RR-1.
     [+fint] --> |?[+Nom]  |      ;finite verbs require a subject, and allow
                 |?(|+Nom|)|      the subject to be encoded as a dependent 
                 |  |+N  | |      noun.

     RR-2.
     |+root| --> |?[+them]    |   ;non interrogative root verbs must have a
     |-ntrg|     |?(|+them|)  |   theme; they allow this theme to be encoded
                 |  |+N   |   |   as a dependent noun.
                 |@>?(|+them|)|   ;The dependent theme noun if any must 
                 |    |+N   | |   precede the root verb.
                 |@<?(|+Nom|) |   ;The dependent subject noun if any must
                 |    |+N  |  |   follow the root verb.

     RR-3.
     [-root] --> |@>?(|+Nom|)|    ;The dependent subject noun if any must
                 |    |+N  | |    precede the non-root finite verb.

     {Here is a concrete stemma illustrating this solution.  Intended gloss:
     `Fritz knows that ice is frozen water.' 
      1     2     3    4   7  5      6
     (Please correct my German if necessary.):

        |
        |              ________________________________________
|       weiss,         |                                      |
|       |2ndex       | |                                      |
Fritz   |+V          | dass                                   |
|1ndex| |+fint       | |3ndex|                                |
|+N   | |1[+Nom]     | |+P   | |                      |        ist
        |?(|+Nom|)   |         |                      |       |7ndex      |
        |  |+N  |    |         Eis                    Wasser  |+V         |
        |+root       |         |4ndex| |              |6ndex| |+fint      |
        |-ntrg       |         |+N   | |              |+N   | |4[+Nom]    |
        |1[+them]    |                 eingrefrorenes         |4(|+Nom|)  |
        |1(|+them|)  |                 |5ndex|                |  |+N  |   |
        |  |+N   |   |                 |+Adj |                |-root      |
        |2>1(|+them|)|                                        |7>4(|+Nom|)|
        |    |+N   | |                                        |    |+N  | |
        |@<?(|+Nom|) |                                       
        |    |+N  |  |         

     Here _Fritz_ is the theme of _weiss_, not the subject.  It is linked 
     with the subject slot inside _weiss_'s lexical matrix.  _Eis_ on the 
     other hand is the subject of _ist_, which allows for no theme dependendent 
     of its own.}

The complimentary
distribution of the overt complementizer and the V2 effect in subordinate
clauses is then a mere observation.  
     {Actually it is not even that; it is by the analysis I am proposing
     here  a meaningless epiphenomenon resulting from the fact that German 
     has prepositions (where `complementizers' are a subclass of prepositions)
     and has verb-final order in non-root clauses.  What my analysis does 
     fail to explain is why German has a different basic word order in root 
     and non-root clauses.}

In a PSG, we can suggest that the
complementizer and the finite verb in second position must be in the same
position (there being no other possibilities for the finite verb).  The
complementary distribution then follows as a verified prediction.  What is
crucial in this explanation is the notion that not only can we position
dependents with respect to their head, but we can also, independently,
position heads with respect to some other structure -- in this case PS.  (I
would be interested in hearing of any DG-based analyses of V2.)
     {Can you do more than just `suggest' an analysis, Owen?  We need to see
     a PSG analysis at least as generative (formal and explicit) as the one 
     I have just sketched in order to make a meaningful comparison between 
     the two approaches to German verb-second phenomenon.  Can you give us 
     one?  We need to see in what sense the PSG analysis qualifies as a 
     prediction while a DG analysis is merely a stipulation, and look at it
     in terms of the kinds of language-specific and cross-linguistic 
     generalizations it is consistent with.}

Of course, while this very narrow area of syntax is exactly what Chomsky
considers "syntax" to be, MTT linguists (I have been told) consider it a
very small and rather unintersting problem, tackled prematurely.  This may
well be true.
     {They can't postpone paying their debts forever.  Let's see if we can 
     bring the day of reckoning closer faster.  An explicit though possibly
     flawed analysis in the hand is worth two perfect though non-existent
     analyses in the bush.}

     {Stan Starosta}
From stanley@uhunix.uhcc.Hawaii.Edu Sun Aug  1 10:27:08 1993
Received: from uhunix.uhcc.Hawaii.Edu by aisun1.ai.uga.edu with SMTP id AA01788
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Sun, 1 Aug 1993 10:27:08 -0400
Received: by uhunix.uhcc.Hawaii.Edu (4.1/Sun690)
	id AA07189; Sun, 1 Aug 93 04:27:05 HST
Date: Sun, 1 Aug 93 04:27:05 HST
From: Stanley Starosta  <stanley@uhunix.uhcc.Hawaii.Edu>
Message-Id: <9308011427.AA07189@uhunix.uhcc.Hawaii.Edu>
To: dg@ai.uga.edu
Subject: DG, PSG, and German V2s; P.S.

reply by Stan Starosta to:
Date: Tue, 20 Jul 93 11:25:16 EDT
From: rambow@unagi.cis.upenn.edu (Owen Rambow)
Subject: DG / PSG

Key:
unindented text: Owen Rambow
     {indented and stemma: Stan Starosta}

So why have PS at all?  Let me suggest the following (in Hans Uszkoreit's
spirit of conversation starters): it seems to me that a theory of syntax
(taken very narrowly as a theory of surface word order)
     {I think that is too narrow; sentences are not strings of words, but
     *structured* strings of words.  The question we are concerned with here
     is whether that structure should be represented in terms of 
     constituency or dependency.}
     {Stan Starosta}
From ellalain@leonis.nus.sg Mon Aug  2 16:46:56 1993
Received: from leonis.nus.sg by aisun1.ai.uga.edu with SMTP id AA04688
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Sun, 1 Aug 1993 20:47:01 -0400
Received: from localhost (ellalain@localhost) by leonis.nus.sg (8.2/8.2) id IAA24080; Mon, 2 Aug 1993 08:46:56 +0800
Date: Mon, 2 Aug 1993 08:46:56 +0800
From: Alain Polguere <ellalain@leonis.nus.sg>
Message-Id: <199308020046.IAA24080@leonis.nus.sg>
To: <dg@ai.uga.edu>
Subject: About MTT and linear order


DG folks,

Some comments about Meaning-Text (MTT) treatment of linear
order have been made recently in the midst of a discussion
between Rambow and Starosta. I have not much to say
about linear order and dependencies in German but I
just want to mention the following facts:
 1) MTT, as any linguistic approach to language
    description, acknowledges the need to account for
    linear order (which is ONE of the means for expressing
    syntactic dependencies);
 2) For a tentatively exhaustive account of the correspondence
    between surface-syntactic dependencies and linear order in
                                               ------------
    English, see one of the very few DG grammars ever published:
      Mel'cuk and Pertsov. 1987. _Surface_Syntax_of_English_,
           Benjamins (ISBN 90 272 1515 4).
    MTT grammars of French and Russian also exist as technical
    reports of the machine translation project ETAP-2 (in Russian).
Thanks for this interesting discussion anyway, and sorry to
interrupt :-).

A. Polguere.


From uclyrah@ucl.ac.uk Mon Aug  2 18:53:35 1993
Received: from mail-a.bcc.ac.uk by aisun1.ai.uga.edu with SMTP id AA08089
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Mon, 2 Aug 1993 12:53:55 -0400
Received: from link-1.ts.bcc.ac.uk by mail-a.bcc.ac.uk with SMTP (PP) 
          id <15541-0@mail-a.bcc.ac.uk>; Mon, 2 Aug 1993 17:53:39 +0100
Received: by link-1.ts.bcc.ac.uk (AIX 3.2/UCB 5.64/4.03) id AA30303;
          Mon, 2 Aug 1993 17:53:35 +0100
Message-Id: <9308021653.AA30303@link-1.ts.bcc.ac.uk>
To: dg@ai.uga.edu
Subject: V2, surface syntax and the need for constituent structure
Date: Mon, 02 Aug 93 17:53:35 +0100
From: RichardHudson50 <uclyrah@ucl.ac.uk>


Having missed two weeks of DG fun through being away, I'd like
to take up a couple of the issues that have been raised here.

1. Verb-second in Germanic

Owen Rambow suggests that DG can't explain why complementisers
and V2 are alternatives in German, e.g. (I assume) why (1) and
(2) are ok, but (3) and (4) aren't.

(1) Ich weiss, er trinkt Kaffee. `I know he drinks coffee'
(2) Ich weiss, dass er Kaffee trinkt.
(3) *Ich weiss, dass er trinkt Kaffee.
(4) *Ich weiss, er Kaffee trinkt.

In contrast, he says that PSG can explain this by saying that
the complementiser "dass" is in the same position as "trinkt"
when the latter is V2 (i.e. as in (1)). This is a very odd
claim, which at least needs some explanation; at first sight
it seems blatantly untrue. 

He also asks how V2 might be handled in a DG-based analysis.
This is surely easy, provided that we have some mechanism
which will allow just one if the verb's dependents (any old
dependent) to precede it. It can be done in Word Grammar by
allowing just one `predependent' arc, which then combines with
some other kind of dependent, which may be subject, object,
extractee, etc.; e.g. in "er trinkt Kaffee" the word "er" is
both subject and predependent, while in "Kaffee trinkt er" it
is just subject, and "Kaffee" is now dependent+object, and in
"Kaffee glaube ich, dass er trinkt", `Coffee I think that he
drinks', "Kaffee" is predependent of "glaube" and object of
"trinkt". The V2 effect follows from the fact that no finite
verb can have more (or less) than one predependent. 

This corresponds to a transformational PSG analysis in which
each verb has an empty specifier which has to be filled by
some phrase moved into it from a later position; but it can be
expressed as a monostratal analysis in DG whereas so far as I
know this is impossible if phrase-structure is assumed. 

The real problems arise in explaining examples like (5):

(5) Trinken wird er das Kaffee.
     drink  will he the coffee.

Hans Uszkoreit has discussed these facts very interestingly,
and the world is still worrying about them. The problem being,
of course, that one (or more) of the dependents of "trinken",
the subordinate verbs, positions itself as though it was a
dependent of "wird". 

2. The supposed need to combine PSG with DG

More generally, I see no reason to accept Owen's assumption
that DG is inherently less suitable for describing surface
word order facts than PSG is. On the contrary, for some
grammarians the big attraction of DG is precisely that it's a
**better** basis for surface syntax, including word-order
facts, than PSG is. Like Stan Starosta, I think we're short of
evidence that DG on its own is inadequate. (My own view is
that PSG is indeed needed for coordination, while all other
parts of grammar are left to DG, but that's not what people
like Hans mean when they claim that DG alone won't do.) And
And Rosta is right to bring up gerunds as a problem for DG,
but as Stan says, they're a problem for X-bar PSG as well; and
I assume that's really the kind of PSG that Hans has in mind
when he calls for composite theories.




Dick Hudson
Dept of Phonetics and Linguistics,
University College London,
Gower Street,
London WC1E 6BT
(071) 387 7050 ext 3152

From ellalain@leonis.nus.sg Wed Aug  4 22:51:28 1993
Received: from leonis.nus.sg by aisun1.ai.uga.edu with SMTP id AA19749
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Wed, 4 Aug 1993 02:51:33 -0400
Received: from localhost (ellalain@localhost) by leonis.nus.sg (8.2/8.2) id OAA17148; Wed, 4 Aug 1993 14:51:28 +0800
Date: Wed, 4 Aug 1993 14:51:28 +0800
From: Alain Polguere <ellalain@leonis.nus.sg>
Message-Id: <199308040651.OAA17148@leonis.nus.sg>
To: <dg@ai.uga.edu>
Subject: ground level questions


Hi DG,

As there is not so many things happening on this list I was
re-reading with more attention some previous exchanges and
I came across the following:

   [And Rosta]
   It's not a nominal clitic at some more abstract level - it's
   a nominal clitic at the only level there is (which, roughly
   speaking, is what is there on the page).

     [Stan Starosta]
     {I'm delighted to see that Word Grammar is resisting the temptation to
     `go underlying'.}

Well, I realize that I just no longer know what a "level" is.
My problem:

1) I believe that a "level" is a "level of something". In the
   context of grammatical studies I thought that it was a level
   of representation of utterances. Am I wrong? If
   I am not, how can the text be the "only level of
   representation of itself". If DG grammarians
   represent the structure of an utterance with something
   (anything) like a set of connections between words,
   this representation is not the text itself and functions
   at another level -- a deeper or more hidden one.
   But I must have missed the point somewhere.
2) Whether I am right or wrong in 1), I would still like
   to know what is so bad with levels. The only objection I can
   see is that language is probably not used in a very
   stratificational way -- i.e. producing utterances out of
   meanings by going through intermediate steps (syntax,
   morphology...). Is that right? What else?

Thanks for your help,

Alain.

From SGALL%CSPGUK11.BITNET@UGA.CC.UGA.EDU Wed Aug  4 11:40:45 1993
Received: from uga.cc.uga.edu by aisun1.ai.uga.edu with SMTP id AA22113
  (5.65c/IDA-1.5 for <dg@AI.UGA.EDU>); Wed, 4 Aug 1993 11:40:45 -0400
Message-Id: <199308041540.AA22113@aisun1.ai.uga.edu>
Received: from UGA.CC.UGA.EDU by uga.cc.uga.edu (IBM VM SMTP V2R2)
   with BSMTP id 8089; Wed, 04 Aug 93 11:41:32 EDT
Received: from CSPGUK11 (NJE origin MAILER@CSPGUK11) by UGA.CC.UGA.EDU (LMail V1.1d/1.7f) with BSMTP id 3017; Wed, 4 Aug 1993 11:41:32 -0400
Received: from CSPGUK11.BITNET (SGALL) by CSPGUK11RSCSV2 .BITNET (Mailer R2.07)
 with BSMTP id 6146; Wed, 04 Aug 93 17:39:44 MET
Date:         Wed, 04 Aug 93 17:39:03 MET
From: Petr Sgall <SGALL@CSPGUK11.BITNET>
To: dg@ai.uga.edu


Dear DG Colleagues,
                   At last I have found a free while to join
your discussion  again. I cannot  help you much  with the E.
gerunds,   (although   I   believe   that  your  discussion,
especially Stan's and Dick's arguments, show that DG des not
encountermore  dangerous difficulties  in this  point tha PS
does).  It is  similar with  the G.  V2 issue: yes, probably
only a historical explanation can be found for the fact that
G. behaves in this way, thus differing from other languages;
and  a  description  of  the  fact  is  well possible in DG.
Moreover, the word order  issues are intrinsically connected
with the  topic-focus articulation, for which  DG appears to
offer a  more adequate description framework  than any other
approach (note  especially that neither  topic nor focus  is
a single  constitutent  in  the  general  case,  as has been
stated in several published texts);  it is also necessary to
take into account the placment  of the sentence stress (e.g.
if  the preverbal  sentence part  in G.  is stressed,  it is
(a part of) the focus, not the theme).
I would like to comment briefly on 'dg and typology', on the
issue of  levels, and, of  course, add a  remark on function
words.
     (a)  It  is  no  mere  chance  that  the  old  European
tradition    of   syntactic    dependency   (combined   with
constituency as for the  dichotomy of subject and predicate)
was interrupted (and abolished  in favor of constituency) in
English speaking countries; E. differs from most Continental
languages in it does not render the syntactic  relations  by
endings  (with few  exceptions), so  that such  phenomena as
agreement  and government  (in the  traditional sense  of G.
Rektion) are severely restricted  here, and prepositions and
the word  order express most syntactic  relations. This is a
specific situation -  I realize that E. is  not alone, since
French, the Scandinavian languages and some others belong to
the same  'analytic' type, and  also the type  to which e.g.
Chinese or Vietnamese belong exhibits a grammaticalized word
order. However,  eben so the prototypical  situation is that
word   order   renders   first   of   all   the  topic-focus
("given"-"new")  articulation  and  grammar  is expressed by
function   morphs   (function   words,   affixes,   endings,
alternations).  This situation  makes it  natural that every
language  can be  described  by  a dependency  based grammar
(with coordination  having its own  dimension in a  network)
and that  the usefulness of  constituency is rather  limited
(no  language seems  to be  fully 'configurational',  but E.
comes close  to this and thus  is a good candidate  for a PS
description,  although the  PS  framework  even here  is not
sufficient - it was no mere  chance that such dg elements as
X-bar or theta roles have emerged also with Chomsky).
     Also the  fact that Categorial Grammar is used first of
all by semanticians is easy to explain: it was formulated ba
logicians aiming at a description of the structure of formal
languages,  and  the  research  in  semantics  nowadays is a
domain of  symbiosis of linguists  and logicians; especially
the  latter tend  to use  the apparatus  useful in their own
domain,  without  much  looking  around  if other approaches
might not be more adequate for natural language.
     (b)  As for  the levels,  I believe  now that  a single
level of syntactic representations of sentences is necessary
(neither surface  nor underlying, but an  interface level of
language and the domain  of cognition). The dependnecy based
networks  of  this  level  contrast  substantially  with the
string  shape  of  morphemic  representations (the 'surface'
word order being present only on the latter level), and then
comes the difference between this morphemic level (with such
items    as   Accusative,    Plural,   Feminine,   Preterit,
Comparative,...) and  the outer (phonetic,  graphemic) shape
of  sentences (with  individual morphs  corresponding to the
just  quoted  items).  What  was  assumed  to  be a level of
surface syntax  or of phonemics,  probably can be  dispensed
with.
     (c) Function  words appear as words  in the outer shape
of sentences, but this does  not tell us immediately if they
are  to  be  understood  as  nodes  of  a  syntactic tree or
network.  The outer  shape cannot  be accepted  as the  only
decisive criterion (even if one does not distinguish between
surface and  underlying syntax); already  such items as  the
French  au,  du,  des,  or  the  German  im, beim, where the
article merges with a preposition, show that when describing
the structure of  the language, we cannot rely  on the outer
shape. Similar  are the E.  contracted forms with  negation,
and other phenomena in other languages. It is then necessary
to look for a deeper criterion,  and this can be seen in the
dichotomy  of lexicon  and grammar,  which is  necessary for
every  linguistic  despription.  Grammatical  functions  are
expressed in  various ways in languages  of different types,
but lexical  units always have  the shape of  words (lexical
morphs).  There  is  no  necessary  or  intrinsic difference
between  the  functions  of  affixes,  endings  and function
words. In fact,  in the Princ & Param.  approach all of them
are  handled  in  the  same   way,  having  nodes  as  their
counterparts  (Aux, Compl,  Infl, Agr).  This means  that an
analytic metalanguage is being used. I still believe that an
agglutinative metalanguage is to  be preferred, with complex
labels  at  the  nodes,   where  the  grammatical  functions
(morphological meanings such as Plural, Preterite, etc., and
syntactic relations) appear in the shape of indices attached
to the  lexical items. This  means the dependency  tree does
not  get overburdened  by nodes  for items  which cannot  be
freely expanded  syntactically: an article  or a preposition
are always accompanying a noun, auxiliaries and conjunctions
accompany  verbs, etc.  (I  know  that there  are exceptions
there, but these are  present everywhere in natural language
and  we  can  only  describe   them  as  such,  as  marginal
phenomena).
     Hoping  that these  remarks may  be useful  for further
discussions,
                                   Petr Sgall
From KPUBA%HUJIVM1.BITNET@UGA.CC.UGA.EDU Wed Aug  4 16:19:00 1993
Received: from uga.cc.uga.edu by aisun1.ai.uga.edu with SMTP id AA23730
  (5.65c/IDA-1.5 for <DG@AI.UGA.EDU>); Wed, 4 Aug 1993 16:19:00 -0400
Message-Id: <199308042019.AA23730@aisun1.ai.uga.edu>
Received: from UGA.CC.UGA.EDU by uga.cc.uga.edu (IBM VM SMTP V2R2)
   with BSMTP id 0706; Wed, 04 Aug 93 16:19:47 EDT
Received: from VM1.HUJI.AC.IL (NJE origin MAILER@HUJIVM1) by UGA.CC.UGA.EDU (LMail V1.1d/1.7f) with BSMTP id 4372; Wed, 4 Aug 1993 16:19:47 -0400
Received: by HUJIVM1 (Mailer R2.07) id 2817; Wed, 04 Aug 93 23:18:52 IST
Date:         Wed, 4 Aug 1993 23:16 IST
From: Anat Ninio <KPUBA@HUJIVM1.BITNET>
Subject:      Typology-specificity indeed..
To: <DG@AI.UGA.EDU>


Re the supposed typology-specificity of DG: Stanley Starosta could have
added Arab and Hebrew linguistics of several hundred years, all in
the Dependency tradition, to his list of Slavic and German languages
that according to critics, "naturally" lend themselves to DG.  I'd love
to see anyone explain -that- away on the grounds of similarity.
From ucleaar@ucl.ac.uk Wed Aug  4 22:23:59 1993
Received: from mail-a.bcc.ac.uk by aisun1.ai.uga.edu with SMTP id AA23756
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Wed, 4 Aug 1993 16:24:08 -0400
Received: from link-1.ts.bcc.ac.uk by mail-a.bcc.ac.uk with SMTP (PP) 
          id <05939-0@mail-a.bcc.ac.uk>; Wed, 4 Aug 1993 21:24:04 +0100
Received: by link-1.ts.bcc.ac.uk (AIX 3.2/UCB 5.64/4.03) id AA60790;
          Wed, 4 Aug 1993 21:24:00 +0100
From: ucleaar@ucl.ac.uk (Mr Andrew Rosta)
Message-Id: <9308042024.AA60790@link-1.ts.bcc.ac.uk>
To: dg@ai.uga.edu, Alain Polguere <ellalain@leonis.nus.sg>
Subject: Re: ground level questions
In-Reply-To: (Your message of Wed, 04 Aug 93 14:51:28 U.) <199308040651.OAA17148@leonis.nus.sg>
Date: Wed, 04 Aug 93 21:23:59 +0100


Alain Polguere:
> Well, I realize that I just no longer know what a "level" is.
> My problem:
> 
> 1) I believe that a "level" is a "level of something". In the
>    context of grammatical studies I thought that it was a level
>    of representation of utterances. Am I wrong? If
>    I am not, how can the text be the "only level of
>    representation of itself". If DG grammarians
>    represent the structure of an utterance with something
>    (anything) like a set of connections between words,
>    this representation is not the text itself and functions
>    at another level -- a deeper or more hidden one.
>    But I must have missed the point somewhere.

I don't think I know what a level is either. But I do think
the set of connections between words is part of the text
itself. If I say "She is", the first bit of the utterance
belongs to the categories "pronoun" and "subject of the second
bit of the utterance" (as well as to other categories of
course). This is just part of the nature of these bits of
the utterance. This doesn't follow from a dependency analysis;
one could equally say that the first bit of that utterance
consitutes the whole of an NP and the second bit the whole
of a VP, & together they form an S, & this is part of their
nature, just as bricks in a wall are both bricks & part of
a wall.

When I said that what-you-see-is-what-you-get I meant that
grammatical structures don't have a derivational history, 
so one can't say "X is the subject of Y & it used to be
the object of Y, but it is no longer the object of Y", or
"at one level X is the subject but not the object of Y
and at another level X is the object but not the subject
of Y". I.e. I was asserting monostratalism. 

--------
And Rosta
From uclyrah@ucl.ac.uk Wed Aug  4 22:44:36 1993
Received: from mail-a.bcc.ac.uk by aisun1.ai.uga.edu with SMTP id AA23907
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Wed, 4 Aug 1993 16:44:46 -0400
Received: from link-1.ts.bcc.ac.uk by mail-a.bcc.ac.uk with SMTP (PP) 
          id <06185-0@mail-a.bcc.ac.uk>; Wed, 4 Aug 1993 21:44:39 +0100
Received: by link-1.ts.bcc.ac.uk (AIX 3.2/UCB 5.64/4.03) id AA78922;
          Wed, 4 Aug 1993 21:44:37 +0100
Message-Id: <9308042044.AA78922@link-1.ts.bcc.ac.uk>
To: dg@ai.uga.edu
Subject: function words
Date: Wed, 04 Aug 93 21:44:36 +0100
From: RichardHudson <uclyrah@ucl.ac.uk>


I'd be interested to hear more discussion about the status of function-words
after the interesting contribution from Petr Sgall, putting a view which I
think is very widely shared in Europe but much less attractive to those of us
who have had a foot firmly in the American tradition. Petr says that 
auxiliary verbs are function words which depend on main verbs.
Because of this, they're not shown as independent words in the syntactic
dependency structure, but as features attached to the main verb. I'd be
interested to know what evidence there is for this analysis. I gather from
his remarks that one piece of evidence is that auxiliary verbs have a `function'
which is similar to that of an affix in other languages (where presumably
`function' means something like `meaning'). But say we accept that English
"have" has the same function, in this sense, as the Latin perfect inflections.
How can we then explain e.g. why "have" itself has a tense (past or present),
why the main verb appears in a participial inflection, without any subject
agreement, why "have" does have subject agreement, why "have" may itself
be in a non-finite form when preceded by another auxiliary such as "will",
and so on? I've always assumed that by far the simplest and most elegant
analysis of a sequence like "will have gone" is to recognise each word as
a separate word in every respect, with dependency relations between them.
Why is this wrong?


Dick Hudson
Dept of Phonetics and Linguistics,
University College London,
Gower Street,
London WC1E 6BT
(071) 387 7050 ext 3152

From stanley@uhunix.uhcc.Hawaii.Edu Thu Aug  5 08:36:18 1993
Received: from uhunix.uhcc.Hawaii.Edu by aisun1.ai.uga.edu with SMTP id AA27218
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Thu, 5 Aug 1993 08:36:18 -0400
Received: by uhunix.uhcc.Hawaii.Edu (4.1/Sun690)
	id AA22657; Thu, 5 Aug 93 02:36:15 HST
Date: Thu, 5 Aug 93 02:36:15 HST
From: Stanley Starosta  <stanley@uhunix.uhcc.Hawaii.Edu>
Message-Id: <9308051236.AA22657@uhunix.uhcc.Hawaii.Edu>
To: dg@ai.uga.edu
Subject: Re: Alain Polguere's ground level questions


Key:

Alain Polguere (unindented text
   and hanging paragraphs)

     {Stan Starosta (indented, bracketed)}

As there is not so many things happening on this list I was
re-reading with more attention some previous exchanges and
I came across the following:

   [And Rosta]
   It's not a nominal clitic at some more abstract level - it's
   a nominal clitic at the only level there is (which, roughly
   speaking, is what is there on the page).

     [Stan Starosta]
     {I'm delighted to see that Word Grammar is resisting the temptation to
     `go underlying'.}

Well, I realize that I just no longer know what a "level" is.
My problem:

1) I believe that a "level" is a "level of something". In the
   context of grammatical studies I thought that it was a level
   of representation of utterances. Am I wrong? If
   I am not, how can the text be the "only level of
   representation of itself". If DG grammarians
   represent the structure of an utterance with something
   (anything) like a set of connections between words,
   this representation is not the text itself and functions
   at another level -- a deeper or more hidden one.
   But I must have missed the point somewhere.
    {I think And and I were not expressing ourselves clearly enough.  I
     agree with Alain that a grammatical representation of a sentence has to 
     be more than just a string of words (though dividing an utterance into
     words is not a trivial task in itself).  What
     I meant, and what I assume that And meant as well, is that a dependency
     representation should not include empty categories or other invisible
     elements, and that its terminal elements should be unanalyzed chunks
     which are in a one-to-one correspondence with the words in the 
     sentence, and in the same order.  I would go farther (and probably so
     would And?) in saying, following usual dependency practice (cf. e.g.
     John Anderson, as I remember), that the number of `nodes' in a 
     dependency representation should also not exceed the number of words.
     (In the stemmae I use, the `mast' I use above each word, plus the 
     word below it and the upper end where other edges are attached, count
     as a single node.)
     Thinking of such a representation in X-bar terms, a DG representation 
     should allow only one bar level.}


2) Whether I am right or wrong in 1), I would still like
   to know what is so bad with levels. The only objection I can
   see is that language is probably not used in a very
   stratificational way -- i.e. producing utterances out of
   meanings by going through intermediate steps (syntax,
   morphology...). Is that right? What else?

     {In my view, the number of levels allowed in a grammatical 
     representation is part of the determination of whether the associated 
     theory has any empirical content or not.  A grammar which allows 
     abstract devices such as underlying structures and empty categories
     allows many different kinds of representations to be associated with a
     given sentence, and makes it difficult or impossible to choose between
     them in a principled way.  Thus all representations are equally good;
     there is no `right' representation, and so the grammar cannot be tested;
     that is, it has no empirical content.  The content of a grammatical
     theory are its constraints, e.g. the limitation to a single level of
     representation, and a grammar which allows everything says nothing.
     Any comments from those of you who utilize multiple levels?}
     {Stan S.}
From ellalain@leonis.nus.sg Fri Aug  6 17:28:53 1993
Received: from leonis.nus.sg by aisun1.ai.uga.edu with SMTP id AA02052
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Thu, 5 Aug 1993 21:29:32 -0400
Received: from localhost (ellalain@localhost) by leonis.nus.sg (8.2/8.2) id JAA06013; Fri, 6 Aug 1993 09:28:53 +0800
Date: Fri, 6 Aug 1993 09:28:53 +0800
From: Alain Polguere <ellalain@leonis.nus.sg>
Message-Id: <199308060128.JAA06013@leonis.nus.sg>
To: <dg@ai.uga.edu>
Subject: levels + aux. verbs


Dear DGs:

Thanks a million for your answers about levels of representation.
Some more stuff on it + something about auxiliary verbs (following
R. Hudson's last message).
Starting with levels:

     S. Starosta says:
     {In my view, the number of levels allowed in a grammatical 
     representation is part of the determination of whether the associated 
     theory has any empirical content or not.  A grammar which allows 
     abstract devices such as underlying structures and empty categories
     allows many different kinds of representations to be associated with a
     given sentence, and makes it difficult or impossible to choose between
     them in a principled way.  Thus all representations are equally good;
     there is no `right' representation, and so the grammar cannot be tested;
     that is, it has no empirical content.  The content of a grammatical
     theory are its constraints, e.g. the limitation to a single level of
     representation, and a grammar which allows everything says nothing.
     Any comments from those of you who utilize multiple levels?}

I don't think that there is a necessary correlation between using
so-called underlying levels and introducing abstract categories.
You can use deeper levels of syntactic representation in order to
do exactly the opposite: you can trim the tree, so to speak. In
that sense, the deeper you go, the less nodes you have. May I
illustrate this with the particular case of the auxiliary verbs in
English? If you allow yourself several levels of syntactic repre-
sentation, there is no problem to consider that
(1)   "Joe is jumping the gun."
has the following superficial structure (names between
parenthesis indicate dependency relations -- I simplify)
(2)
               BE[pres.ind.]
              /   \
(subjective) /     \ (auxiliary)
            /       \
          JOE      JUMP[pres.part]
                    |
                    | (dir.objective)
                    |
                   GUN[sing]
                    |
                    | (determinative)
                    |
                   THE
This structure accounts for all syntactic dependencies holding
between word-forms in (1). Of course, I do not pretend that this
is the only way of modeling the internal syntactic structure
of (1). Beside (1), I may want to consider a deeper structure
NOT IN ORDER TO INTRODUCE ADDITIONAL ABSTRACT NODES, but precisely
in order to get rid of everything which is not to the actual central
semantic content of (1). For instance, I will consider the
following:
  - the present progressive form is an inflectional variation of
    of the verb; the auxiliary in (1) is nothing else but
    a component of the grammatical tool I have to use
    in English in order to express a given grammatical meaning;
  - 'JUMP THE GUN' is one single lexical element -- a dictionary
    entry in my English lexicon --; it happens to be an idiomatic
    expression (in other words, it is a phraseme of English).
Therefore, my deeper representation, will be simpler than (2):
(3)
             'JUMP THE GUN'[pres.,prog.asp.]
              / 
             / (1st.actant)
            /
          JOE
I am sure someone is going to tell me that (3) is not needed
at all. But what can I do? Personally it helps me a lot in
making the connection between (1) and the meaning of (1). Must
be because I am not a native speaker; but shouldn't a grammatical
modeling make things clearer for outsiders?

Cheers,

Alain.
From LARSSON@ntcclu.ntc.nokia.com Fri Aug  6 16:01:13 1993
Received: from ntc02.tele.nokia.fi by aisun1.ai.uga.edu with SMTP id AA03847
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Fri, 6 Aug 1993 04:59:42 -0400
Date:    Fri, 6 Aug 1993 12:01:13 +0400 (EET-DST)
From: LARSSON@ntcclu.ntc.nokia.com
Message-Id: <930806120113.22203965@ntcclu.ntc.nokia.com>
Subject: Function words and logical form
To: dg@ai.uga.edu
X-Vmsmail-To: INET::"dg@ai.uga.edu"

This is a posting I misplaced by sending it to owner-dg@ai.uga.edu instead of
dg@ai.uga.edu. My apologies for the delay.
-----------------------------------------------------------------------------

	In a recent posting RichardHudson <uclyrah@ucl.ac.uk>
	said:

I'd be interested to hear more discussion about the status of function-words
after the interesting contribution from Petr Sgall, putting a view which I
think is very widely shared in Europe but much less attractive to those of us
who have had a foot firmly in the American tradition. Petr says that 
auxiliary verbs are function words which depend on main verbs.
Because of this, they're not shown as independent words in the syntactic
dependency structure, but as features attached to the main verb. I'd be
interested to know what evidence there is for this analysis. I gather from
his remarks that one piece of evidence is that auxiliary verbs have a `function'
which is similar to that of an affix in other languages (where presumably
`function' means something like `meaning'). But say we accept that English
"have" has the same function, in this sense, as the Latin perfect inflections.

	Let us compare an utterly trivial Finnish sentence

	(1)	T"am"a olisi selv"a.

	with the equally trivial English 'equivalent'

	(2)	This would have been obvious.

	(where "a is used in a non-standard way for a with diaeresis,
	which can not be rendered on the Internet), we will find
	the following correspondances:

	(3) 	T"am"a	ol|COND-isi		selv"a.
		This	be|COND-would-have	obvious.

	(not providing any analysis for the initial and final words
	in the sample sentences)

How can we then explain e.g. why "have" itself has a tense (past or present),
why the main verb appears in a participial inflection, without any subject
agreement, why "have" does have subject agreement, why "have" may itself
be in a non-finite form when preceded by another auxiliary such as "will",
and so on? 

	From the rudimentary analysis in (3), we might propose
	the following logical forms for the verbs 

	(4)	cond(isi(aux(olla)))
		cond(would-have(aux(be)))

	The forms in (4) could additionally be decorated with more
	intricate feature structures, and such a representation could
	be very practical, e.g. for the purpose of some specific
	text processing task.

I've always assumed that by far the simplest and most elegant
analysis of a sequence like "will have gone" is to recognise each word as
a separate word in every respect, with dependency relations between them.
Why is this wrong?

	However, we migth feel that the verbal structures in 
	sentences (1) and (2) would call for different logical
	forms, somewhat along the lines of (5) for the Finnish
	and (6) for the English.

	(5)	cond(isi(aux(olla)))

	(6)	aux(would), aux(have), aux(be)

	Here I have refrained from any deeper thoughts as to the
	status of the pair 'be' and 'olla' themselves, for the sake 
	of simplicity (or simplemindedness).

	My questions are: 1) How can we avoid imposing differences (or
	similarities) between languages due more to our choice of
	formalism than to real evidence (and what would count as
	REAL EVIDENCE in this context). 2) Do we not have to assume
	the existence of non-surface level(s), if we want to assert
	that the 'function' or 'meaning' of some items in different
	languages are the 'same' or 'equivalent'.

	For example, a substition test would tell us that the
	Finnish suffix '-isi' in (1) have the same function as
	the English 'would have' in (2). But, since the actual
	morphemes themselves are different, our judgement of
	sameness must be guided by a mental comparison on a different
	level (i.e. a non-surface level). Traditionally, the 
	criteria for this kind of comparison seem to have had 
	something to do with the distribution of items in discourse
	(i.e. we would expect 'would have' in approximately the
	same positions as '-isi' in English and Finnish discourse,
	respectively).

	Arne Larsson
*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*
Arne Larsson			Nokia Telecommunications
Translator			Transmission Systems, Customer Services
larsson@ntc02.tele.nokia.fi	P.O. Box 12, SF-02611 Espoo, Finland
larsson@ntcclu.ntc.nokia.com	Phone +358 0 5117476, Fax +358 0 51044287
*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*
From SGALL%CSPGUK11.BITNET@UGA.CC.UGA.EDU Fri Aug  6 10:02:17 1993
Received: from uga.cc.uga.edu by aisun1.ai.uga.edu with SMTP id AA04782
  (5.65c/IDA-1.5 for <dg@AI.UGA.EDU>); Fri, 6 Aug 1993 10:02:17 -0400
Message-Id: <199308061402.AA04782@aisun1.ai.uga.edu>
Received: from UGA.CC.UGA.EDU by uga.cc.uga.edu (IBM VM SMTP V2R2)
   with BSMTP id 4782; Fri, 06 Aug 93 10:03:01 EDT
Received: from CSPGUK11 (NJE origin MAILER@CSPGUK11) by UGA.CC.UGA.EDU (LMail V1.1d/1.7f) with BSMTP id 8044; Fri, 6 Aug 1993 10:02:58 -0400
Received: from CSPGUK11.BITNET (SGALL) by CSPGUK11RSCSV2 .BITNET (Mailer R2.07)
 with BSMTP id 7010; Fri, 06 Aug 93 16:01:40 MET
Date:         Fri, 06 Aug 93 15:55:48 MET
From: Petr Sgall <SGALL@CSPGUK11.BITNET>
Subject:      function words and levels revisited
To: dg@ai.uga.edu

I can only agree with Allain and Arne. I would not say that Dick's
analysis is wrong, but we need one the one hand something more complex
(to cope with the relationships beteween languages and between sound &
meaning) and on the other side something more economical (the relation
between a lexical verb and an auxiliary is not that of syntactic depend-
ency: no other complementations of the aux are possible; in this E. have
is similar to E. -ed, and Perfect to Preterite; with other kinds of
function words it is even clearer that no nodes in a dependency tree are
needed for them).
Nice to see how the discussion proceeds; I am sorry to have to leave now;
I'll be back on e-mail in 10 days from now, for a while.
Petr Sgall
From uclyrah@ucl.ac.uk Fri Aug  6 20:27:14 1993
Received: from mail-a.bcc.ac.uk by aisun1.ai.uga.edu with SMTP id AA06771
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Fri, 6 Aug 1993 14:27:22 -0400
Received: from link-1.ts.bcc.ac.uk by mail-a.bcc.ac.uk with SMTP (PP) 
          id <20647-0@mail-a.bcc.ac.uk>; Fri, 6 Aug 1993 19:27:17 +0100
Received: by link-1.ts.bcc.ac.uk (AIX 3.2/UCB 5.64/4.03) id AA112474;
          Fri, 6 Aug 1993 19:27:15 +0100
Message-Id: <9308061827.AA112474@link-1.ts.bcc.ac.uk>
To: dg@ai.uga.edu
Subject: auxiliaries and levels
Date: Fri, 06 Aug 93 19:27:14 +0100
From: RichardHudson <uclyrah@ucl.ac.uk>


I welcome Alain Polguere's surface analysis of "John is jumping the gun",
with "is" as the root of the sentence. Let me also satisfy his prediction
that someone will tell him he doesn't need the deeper one, with "jump the
gun" as the root. He doesn't need it. Either it is the semantic analysis,
in which case the issue is simply what to call it (semantics or deep
syntax); or it isn't, in which case the issue is whether it is easier to
relate the surface syntax to the semantic structure in a single step or in
two, via his deeper level. That's a technical question without any obvious
answer, and it depends very much on the nature of the semantic structure
we're assuming; but all my money is on the simpler answer, namely that it's
simpler to go in one step than in two. 


Dick Hudson
Dept of Phonetics and Linguistics,
University College London,
Gower Street,
London WC1E 6BT
(071) 387 7050 ext 3152

From uclyrah@ucl.ac.uk Fri Aug  6 23:17:30 1993
Received: from mail-a.bcc.ac.uk by aisun1.ai.uga.edu with SMTP id AA07550
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Fri, 6 Aug 1993 17:17:39 -0400
Received: from link-1.ts.bcc.ac.uk by mail-a.bcc.ac.uk with SMTP (PP) 
          id <23001-0@mail-a.bcc.ac.uk>; Fri, 6 Aug 1993 22:17:33 +0100
Received: by link-1.ts.bcc.ac.uk (AIX 3.2/UCB 5.64/4.03) id AA27319;
          Fri, 6 Aug 1993 22:17:30 +0100
Message-Id: <9308062117.AA27319@link-1.ts.bcc.ac.uk>
To: dg@ai.uga.edu
Subject: Function words and levels
Date: Fri, 06 Aug 93 22:17:30 +0100
From: RichardHudson50 <uclyrah@ucl.ac.uk>


In reply to Arne Larsson:

I certainly wouldn't deny the need for a non-surface level of
analysis at which semantic functions can be shown. The issue is
simply what this level should be called. I would call it the
level of semantics (`logical form', if you prefer). If you agree
on this, then we're left with just a single level of syntax,
namely the surface level where (as Stan Starosta put it), the
number of nodes is precisely the same as the number of words.
(Well, I think it may be a bit more complicated than that because
of clitics, compounds and coordination, but I agree with the
spirit of what Stan says; the main point is that there are *at
least* as many nodes in syntax as there are words.) In order to
make comparisons between languages, as Arne wants to, you then
compare the (surface) syntactic structures in the different
languages which correspond to the same semantic structures.
Whatever the semantic structure for "would have" is, it must also
be the structure for Finnish "-isi" if this is a good translation
of "would have" as Arne says it is.

In reply to Petr Sgall:

If the relation between "have" and "gone" in "have eaten" is not
dependency, what about the relation between "get" and "eaten" in
"get eaten"? The essential point is that "get" is not an
auxiliary verb (at least not by any of the standard syntactic
tests - e.g. it doesn't invert with the subject or take "-n't" as
a negator, unlike "have"). Moreover, if "have" does not count as
a separate node in the syntax, what is it that carries its tense
("has" or "had")? Is the answer different from the one you'd give
for non-auxiliary "have", as in "He has a bicycle"? My claim is
that the simplest way to relate the morphology of the individual
words (e.g. the difference between "has" and "had", and between
"eat" and "eaten") to the semantic structure of the whole
sentence is by respecting the integrity of the words in the
syntactic analysis, whilst allowing two words to contribute to a
single semantic structure. If Petr is right, I must be wrong -
and vice versa! 



Dick Hudson
Dept of Phonetics and Linguistics,
University College London,
Gower Street,
London WC1E 6BT
(071) 387 7050 ext 3152

From stanley@uhunix.uhcc.Hawaii.Edu Sun Aug  8 10:23:07 1993
Received: from uhunix.uhcc.Hawaii.Edu by aisun1.ai.uga.edu with SMTP id AA15273
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Sun, 8 Aug 1993 10:23:07 -0400
Received: by uhunix.uhcc.Hawaii.Edu (4.1/Sun690)
	id AA25288; Sun, 8 Aug 93 04:23:05 HST
Date: Sun, 8 Aug 93 04:23:05 HST
From: Stanley Starosta  <stanley@uhunix.uhcc.Hawaii.Edu>
Message-Id: <9308081423.AA25288@uhunix.uhcc.Hawaii.Edu>
To: dg@ai.uga.edu
Subject: Sgall on multistrata and PSG

Comments on Petr Sgall's posting of Wed, 04 Aug 93 17:39:03 MET
Key: 
Sgall: unindented
     {Starosta: indented and bracketed}

     (a)  It  is  no  mere  chance  that  the  old  European
tradition    of   syntactic    dependency   (combined   with
constituency as for the  dichotomy of subject and predicate)
was interrupted (and abolished  in favor of constituency) in
English speaking countries; E. differs from most Continental
languages in it does not render the syntactic  relations  by
endings  (with few  exceptions), so  that such  phenomena as
agreement  and government  (in the  traditional sense  of G.
Rektion) are severely restricted  here, and prepositions and
the word  order express most syntactic  relations. 
     {Again I'm having trouble interpreting a `no mere chance' statement.  
     I'm not sure whether this is meant as a comment on the vicissitudes of
     European intellectual history or on intrinsic capabilities of 
     dependency representation; Petr's subsequent comments suggest the 
     former.  At any rate, DG grammars of `isolating' languages are still
     superior to PS grammars of such languages in terms of capturing 
     generalizations and limiting expressive power.  (I'm still waiting for
     someone to challenge this assertion and back up the challenge with a
     formal and explicit bit of PSG we can compare with a DG counterparet.)}

This situation  makes it  natural that every
language  can be  described  by  a dependency  based grammar
(with coordination  having its own  dimension in a  network)
     {I'm not prepared to concede this point yet.  It can be disproven by a
     dependency analaysis of coordination which does not require a resort to
     an additional dimension and/or PS mechanisms.  I have some ideas about
     how this may be done, and I think Peter Hellwig and Henning Lobin have
     gone well beyond me in working out the analysis.}

and that  the usefulness of  constituency is rather  limited
(no  language seems  to be  fully 'configurational',  but E.
comes close  to this and thus  is a good candidate  for a PS
description,  although the  PS  framework  even here  is not
sufficient 
     {Here I wish Petr had been more concrete.  What does it mean for a
     language to be `configurational'?  Does it mean anything more than a
     claim that a la VP node?  And can anyone on this net give 
     me any evidence at all for supposing that a VP node is any more 
     appropriate for English than it is for German or Chinese?}

it was no mere  chance that such dg elements as X-bar or theta roles have 
emerged also with Chomsky).
     {Again, why is it `no mere chance?  I am having trouble following the
     logic in these statements.  Is Petr just saying that Chomsky 
     `borrowed' these elements from DG?  Who can doubt it?  But if so, what 
     does this have to do with the claim that PSG is more suitable for 
     English than DG is?}

     (b)  As for  the levels,  I believe  now that  a single
level of syntactic representations of sentences is necessary
(neither surface  nor underlying, but an  interface level of
language and the domain  of cognition).  The dependnecy based
networks  of  this  level  contrast  substantially  with the
string  shape  of  morphemic  representations (the 'surface'
word order being present only on the latter level), and then
comes the difference between this morphemic level (with such
items    as   Accusative,    Plural,   Feminine,   Preterit,
Comparative,...) and  the outer (phonetic,  graphemic) shape
of  sentences (with  individual morphs  corresponding to the
just  quoted  items).  What  was  assumed  to  be a level of
surface syntax  or of phonemics,  probably can be  dispensed
with.
     {I claim that it is possible to provide all of this information in a 
     single `surface' level of dependency representation.  If I can in fact
     do this, then it is the `intermediate structure' which is redundant.}

     (c) Function  words appear as words  in the outer shape
of sentences, but this does  not tell us immediately if they
are  to  be  understood  as  nodes  of  a  syntactic tree or
network.  
     {This is only a meaningful question if you have already assumed that 
     there should be more than one level.  If you don't make this a priori 
     assumption, then of course function words can and must be nodes in the 
     dependency representation.}

The outer  shape cannot  be accepted  as the  only
decisive criterion (even if one does not distinguish between
surface and  underlying syntax); already  such items as  the
French  au,  du,  des,  or  the  German  im, beim, where the
article merges with a preposition, show that when describing
the structure of  the language, we cannot rely  on the outer
shape.  
     {I take Petr's point here; has anyone else found a way to account for
     these in a constrained monostratal representation?}

Similar  are the E.  contracted forms with  negation,
     {I tried to show in my 1977 `Affix Hopping' paper that English negative
     contraction' is synchronically negative inflection, and therefore 
     does not support a polystratal representaiton.  I think Geoff Pullum
     and possibly others have reached the same conclusion.  I also have a
     monostratal analysis for English `contracted' auxiliaries.}

and other phenomena in other languages.
     {Phenomena different in kind from the German, French, and English
     examples?  If not, can we conclude that the only motivation for 
     assuming a polystratal representaion is the existence of such two-
     word fusions, and that if someone finds a way to account for them
     monostratally, there will be no more reason for assuming more than one
     level of representation?}

It is then necessary
to look for a deeper criterion,  and this can be seen in the
dichotomy  of lexicon  and grammar,  which is  necessary for
every  linguistic  despription. 
     {As a devotee of lexicon-driven dependency grammar, I must of course
     take exception to this statement.  I would claim exactly the opposite:
     the lexicon *is* the grammar.  Once the valence features of each word
     have been adequately specified, the sentences of the language have 
     been generated: each word is a `node admissibility condition' on a
     dependency representation, and a well-formed sentence is any string of
     words which has a corresponding dependency representation in which each
     node is admissible.  We of course need some kind of conventions as to
     how to draw the dependency trees projected from the lexical entries, 
     but no additional grammatical rules as such are necessary.}

Grammatical  functions  are
expressed in  various ways in languages  of different types,
but lexical  units always have  the shape of  words (lexical
morphs).  There  is  no  necessary  or  intrinsic difference
between  the  functions  of  affixes,  endings  and function
words. In fact,  in the Princ & Param.  approach all of them
are  handled  in  the  same   way,  having  nodes  as  their
counterparts  (Aux, Compl,  Infl, Agr).  This means  that an
analytic metalanguage is being used. 
     {Is this necessary or desirable?  I would claim not.}

I still believe that an
agglutinative metalanguage is to  be preferred, with complex
labels  at  the  nodes,   where  the  grammatical  functions
(morphological meanings such as Plural, Preterite, etc., and
syntactic relations) appear in the shape of indices attached
to the  lexical items.
     {Agreed; in a constrained dependency representation, a node is a word,
     so this statement is a proposal for marking all grammatical information
     on words, which I heartily endorse.}

 This  means the dependency  tree does
not  get overburdened  by nodes  for items  which cannot  be
freely expanded  syntactically: an article  or a preposition
are always accompanying a noun, auxiliaries and conjunctions
accompany  verbs, etc.  
     {If people like Haj Ross, Dick Hudson, Geoff Pullum, and myself are
     correct in claiming that `auxiliares' are the main verbs of their
     clauses, then by the criterion of expandability, it would be a serious
     error to leave them out of the dependency representation.}

(I  know  that there  are exceptions
there, but these are  present everywhere in natural language
and  we  can  only  describe   them  as  such,  as  marginal
phenomena).
     {So in the absence of any *syntactic* motivation other than word fusion 
     for the extra levels, is the only motivation then for encoding words 
     then an esthetic one?  A representation for a sentence which 
     provides a node for each word, including `function words', is no more 
     `overburdened' than the sentence it represents.  I personally find this 
     isomorphy more esthetically pleasing than assigning one or more non-
     isomorphic representations to a sentence in addition to the isomorphic
     one that every adequate syntactic theory must provide.}
     
     {Stan Starosta}
From ucleaar@ucl.ac.uk Sun Aug  8 21:53:18 1993
Received: from mail-a.bcc.ac.uk by aisun1.ai.uga.edu with SMTP id AA16356
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Sun, 8 Aug 1993 15:53:27 -0400
Received: from link-1.ts.bcc.ac.uk by mail-a.bcc.ac.uk with SMTP (PP) 
          id <09944-0@mail-a.bcc.ac.uk>; Sun, 8 Aug 1993 20:53:21 +0100
Received: by link-1.ts.bcc.ac.uk (AIX 3.2/UCB 5.64/4.03) id AA121771;
          Sun, 8 Aug 1993 20:53:18 +0100
From: ucleaar@ucl.ac.uk (Mr Andrew Rosta)
Message-Id: <9308081953.AA121771@link-1.ts.bcc.ac.uk>
To: dg@ai.uga.edu, Stanley Starosta <stanley@uhunix.uhcc.Hawaii.Edu>
Subject: Re: Sgall on multistrata and PSG
In-Reply-To: (Your message of Sun, 08 Aug 93 04:23:05 K.) <9308081423.AA25288@uhunix.uhcc.Hawaii.Edu>
Date: Sun, 08 Aug 93 20:53:18 +0100


> comments on Petr Sgall's posting of Wed, 04 Aug 93 17:39:03 MET
> Key: 
> Sgall: unindented
>      {Starosta: indented and bracketed}
> 
> The outer  shape cannot  be accepted  as the  only
> decisive criterion (even if one does not distinguish between
> surface and  underlying syntax); already  such items as  the
> French  au,  du,  des,  or  the  German  im, beim, where the
> article merges with a preposition, show that when describing
> the structure of  the language, we cannot rely  on the outer
> shape.  
>      {I take Petr's point here; has anyone else found a way to account for
>      these in a constrained monostratal representation?}
 
What is striking about them is the way they do seem to transgress
normal constraints, so any theory that can accommodate them is
probably going to be less constrained than one that can't.

I was thinking recently about _au, du, des, aux_ etc., & also
English _my_, _mine_ (which I should like to take as _me_ +
_'s_). I reckoned that for a monostratal theory the best
approach is to take them as two words (DE & LE, ME & 'S),
such that the head word is a special version of DE, 'S, or 
whatever & has a different orthographic/phonological structure 
(<du> rather than <de>, <my> rather than <'s>). Then the
sound/spelling of the dependent word (LE, ME) is either 
nothing or identical with that of its head. The constraint
that this analysis transgresses is either allowing a word
to have zero sound/spelling or allowing the sound/spelling
of one word to be the sound/spelling of another word. (I
prefer the latter transgression.)

NB when I say De & LE have the same sound/spelling, I mean
that one set of actual marks on paper or sounds coming from
the mouth represent both words simultaneously.

(I'm not sure why it's _de l'hopital_ rather than _du hopital_,
but an easy solution is to say that _le_ is a different article
from _l'_ and the complement of the DE spelt <du> must be
LE rather than L'.)

>  This  means the dependency  tree does
> not  get overburdened  by nodes  for items  which cannot  be
> freely expanded  syntactically: an article  or a preposition
> are always accompanying a noun, auxiliaries and conjunctions
> accompany  verbs, etc.  
>      {If people like Haj Ross, Dick Hudson, Geoff Pullum, and myself are
>      correct in claiming that `auxiliares' are the main verbs of their
>      clauses, then by the criterion of expandability, it would be a serious
>      error to leave them out of the dependency representation.}

How widespread is the notion that expandability is a necessary
condition for dependency? 

----
And Rosta
From ellalain@leonis.nus.sg Mon Aug  9 19:40:26 1993
Received: from leonis.nus.sg by aisun1.ai.uga.edu with SMTP id AA17679
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Sun, 8 Aug 1993 23:40:31 -0400
Received: from localhost (ellalain@localhost) by leonis.nus.sg (8.2/8.2) id LAA26936; Mon, 9 Aug 1993 11:40:26 +0800
Date: Mon, 9 Aug 1993 11:40:26 +0800
From: ellalain@leonis.nus.sg (ellalain)
Message-Id: <199308090340.LAA26936@leonis.nus.sg>
To: <dg@ai.uga.edu>
Subject: Semantics vs Deep-Syntax


DGs,

From Dick Hudson:
> I welcome Alain Polguere's surface analysis of "John is jumping the gun",
> with "is" as the root of the sentence. Let me also satisfy his prediction
> that someone will tell him he doesn't need the deeper one, with "jump the
> gun" as the root. He doesn't need it. Either it is the semantic analysis,
> in which case the issue is simply what to call it (semantics or deep
> syntax); or it isn't, in which case the issue is whether it is easier to
> relate the surface syntax to the semantic structure in a single step or in
> two, via his deeper level. That's a technical question without any obvious
> answer, and it depends very much on the nature of the semantic structure
> we're assuming; but all my money is on the simpler answer, namely that it's
> simpler to go in one step than in two. 

Well, I don't think that a one-step modeling is necessarily "simpler" than
a two-step one. In fact, I know that  there is a bunch a badly wired
human beings -- among them, myself -- who consider that things become
simpler when they are decomposed. But this is all about psychology and
I really don't understand much about that. Therefore, I cannot justify
the psychological/simplicity-based relevance of the axiom:

   "separate whatever seems to be autonomous [I didn't say "independent"]
    and seems to possess some unique characteristics."

If you accept this axiom -- at least for the five minutes that it will
take you to read my message -- I can proceed with an explanation of why
the deep-syntactic structure proposed earlier is not, and cannot be,
a semantic representation.

I agree with Richard Hudson's remark about the deep-syntactic
representation of "Joe is jumping the gun.", which seems to be
useless as it is pretty close to a semantic representation,
something like:

  'jump the gun'( 'joe' )

But this example was given to illustrate the need for a distinction
between what I have presented as surface vs deep syntax. (Nothing
original here, I am just serving you with pre-cooked Meaning-Text stuff.)
As Richard Hudson doesn't reject the validity of this dichotomy, but (as 
expected :-) ) attack the syntactic nature of the deeper representation,
it remains to be explained why deep-syntactic dependency structures
(which do not feature "grammatical words", which are based on very
general universal dependencies, etc.) are not semantic representations.

Firstly, predicates do not necessarily realize their valence in a direct
way. Which means that a semantic structure such as:

    'p'( 'a1' , 'a2' )

is not necessarily expressed with the following dependencies:

            P
           / \
        I /   \ II
         /     \
        A1      A2

Take the semantic structure:

   (1) 'speak3'( 'Joe' , 'Tom' )

where 'speak3' is the meaning of "speak" expressed in sentences such
as:

   (2) "Don't disturb Joe now; he's speaking with his students."

The diathesis of the lexeme SPEAK3 presents at least the following
options:


   (3)
       (3')  'speak3'( X , Y )  ====>   (3'')     SPEAK3
                                                    / \
                                                 I /   \ II
                                                  /     \
                                                 X       Y


                                ====>   (3''')    SPEAK3
                                                     |
                                                   I |
                                                     |
                                                     X
                                                     |
                                               COORD |
                                                     |
                                                    AND
                                                     |
                                                  II |
                                                     |
                                                     Y

[You may prefer a different representation of coordinated structures,
 where the coordinating lexeme (here, AND) is the head.]

(3) shows that there are at least two different deep-SYNTACTIC
structures which can express the English meaning (3'). (3'') will
give rise to sentences such as:

(4) "Joe is speaking with Tom."

and (3'') to sentences such as:

(5) "Joe and Tom are speaking."

(4) and (5) have the same "propositional" meaning. Only the
communicative structure of the message they express may vary.
Therefore, the representation of this propositional meaning
should be unique, even though (4) and (5) present very different
systems of syntactic dependencies. I therefore NEED this distinction
between semantic and deep-syntactic structures in order to
encode properly in a dictionary of English the diathesis of
SPEAK3 (this is just one example among millions which can be
found in natural languages). Notice that SPEAK4 (roughly, 'to
give a speech'), for instance, does not have the same diathesis:

(6) "Listen! The President is speaking to his employees now."
    * "Listen! The President and his employees are speaking now."
     \
      \(another meaning!)

Secondly, the following "texts" are also cases of a identical
propositional meanings expressed with different communicative
perspectives:

(7) a. "...the red blackbird..."
    b. "The blackbird is red."

The common representation of the propositional meaning of (7a) and (7b)
                                 -------------
could be something like (I simplify):

(8) 'red'( 'blackbird' )

But of course, the deep-syntactic structures are very different; something
like:

(9) a.     BLACKBIRD        b.           BE   <-- [Notice that the copula is
               |                         / \       not considered here as a
          ATTR |                      I /   \ II   real gram. word as it appears
               |                       /     \     in deep-syntax -- different
              RED                BLACKBIRD   RED   from the auxiliary BE]

Again, this is an example among others; but I believe it can
demonstrate why deep-syntactic structures -- as presented in my
previous message -- cannot be regarded as semantic structures.
If I return to Richard Hudson's message, I could therefore reject
deep-syntactic structures only on the basis that they make things
more "complicated". But, using And Rosta's rhetoric (sorry :-) ), I
feel that deep syntactic phenomena, as opposed to superficial ones, DO
exist somewhere between the speaker's two ears. I find it simpler
to describe them separately.

All the best to the DG group,

Alain Polguere.

From stanley@uhunix.uhcc.Hawaii.Edu Mon Aug  9 00:07:33 1993
Received: from uhunix.uhcc.Hawaii.Edu by aisun1.ai.uga.edu with SMTP id AA17771
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Mon, 9 Aug 1993 00:07:33 -0400
Received: by uhunix.uhcc.Hawaii.Edu (4.1/Sun690)
	id AA08486; Sun, 8 Aug 93 18:07:31 HST
Date: Sun, 8 Aug 93 18:07:30 HST
From: Stanley Starosta <stanley@uhunix.uhcc.Hawaii.Edu>
To: dg@ai.uga.edu
Subject: Lots more on function words and levels
Message-Id: <CMM.0.90.2.744869250.stanley@uhunix.uhcc.Hawaii.Edu>

Lots more on function words and levels
Key:
Unindented, unbracketed: authors of postings
     {Stan Starosta's comments}
==
Comments on Alain Polguere, `levels + aux. verbs'


I don't think that there is a necessary correlation between using
so-called underlying levels and introducing abstract categories.
You can use deeper levels of syntactic representation in order to
do exactly the opposite: you can trim the tree, so to speak. In
that sense, the deeper you go, the less nodes you have. May I
illustrate this with the particular case of the auxiliary verbs in
English? If you allow yourself several levels of syntactic repre-
sentation, there is no problem to consider that
(1)   "Joe is jumping the gun."
has the following superficial structure (names between
parenthesis indicate dependency relations -- I simplify)
(2)
               BE[pres.ind.]
              /   \
(subjective) /     \ (auxiliary)
            /       \
          JOE      JUMP[pres.part]
                    |
                    | (dir.objective)
                    |
                   GUN[sing]
                    |
                    | (determinative)
                    |
                   THE
This structure accounts for all syntactic dependencies holding
between word-forms in (1). 
     {Except linear precedence of course.  If you don't account for that in
     `superficial structure', where do you account for it?  Or is word order
     not part of syntax??}
Of course, I do not pretend that this
is the only way of modeling the internal syntactic structure
of (1). Beside (1), I may want to consider a deeper structure
NOT IN ORDER TO INTRODUCE ADDITIONAL ABSTRACT NODES, but precisely
in order to get rid of everything which is not to the actual central
semantic content of (1). 
     {Why would anyone want to do that?  Such a level is completely 
     redundant, since it contains nothing that is not present in the 
     `superficial representation', and adds nothing to it.}

For instance, I will consider the following:
  - the present progressive form is an inflectional variation of
    of the verb; 
     {What evidence is there for such a claim?  Are we going back to the
     classical European tradition of regarding all languages as deviations
     from Latin?  Would it be legitimate to state that `the Latin present
     participle is a portmanteau variation of the English copula plus
     present participle'?}

    the auxiliary in (1) is nothing else but
    a component of the grammatical tool I have to use
    in English in order to express a given grammatical meaning;
     {That is true of every element of grammar, be it a `content word', a 
     `function word', an affix, or a grammatically significant feature of 
     word order.  What does that tell us about levels of representation?}

  - 'JUMP THE GUN' is one single lexical element -- a dictionary
    entry in my English lexicon --; it happens to be an idiomatic
    expression (in other words, it is a phraseme of English).
     {In some sense that is of course true, but it is still a phrase and
     still requires representaion as three words.  The special status of
     this collocation can be accounted for in a monostratal representation 
     by treating JUMP as a word JUMP2 which is different in distribution and
     meaning from the ordinary JUMP1.  In particular, JUMP2 requires as its
     dependent the unique and semantically empty word GUN2, which in turn 
     requires THE as a dependent.  The difference in meaning between JUMP1 
     and JUMP2 of course resides in the lexical matrices of these two words, 
     and it is not necessary to create a separate level of structure to 
     account for it.}

Therefore, my deeper representation, will be simpler than (2):
     {That is cheating; the fair way to reckon relative complexity is to 
     compare i) a monostratal analysis (e.g. (2) with linear precedence
     incorporated) with ii) a combination of (2) and (3) plus whatever 
     mechanism is adduced to accommodate linear precedence.}

(3)
             'JUMP THE GUN'[pres.,prog.asp.]
              /
             / (1st.actant)
            /
          JOE

I am sure someone is going to tell me that (3) is not needed
at all. 
     {Quite right.}
But what can I do? Personally it helps me a lot in
making the connection between (1) and the meaning of (1). Must
be because I am not a native speaker; but shouldn't a grammatical
modeling make things clearer for outsiders?
     {The first responsibility of a scientific model is to model the 
     phenomenon under investigation in a maximally economical way (Occam's
     Razor).  If you think you need to create extra alternative 
     representations to make things clearer, that is a matter of 
     practical pedagogy, but I personally think that if the ultimate goal is
     to impart a fundamental and correct understanding to `outsiders', then
     it is better to teach something the way it is rather than to teach an 
     alternative view with no scientific support.}

     {Stan S.}
==
Comments on Arne Larsson, `Function words and logical form'

        Let us compare an utterly trivial Finnish sentence

        (1)     T"am"a olisi selv"a.

        with the equally trivial English 'equivalent'

        (2)     This would have been obvious.

        (where "a is used in a non-standard way for a with diaeresis,
        which can not be rendered on the Internet), we will find
        the following correspondances:

        (3)     T"am"a  ol|COND-isi             selv"a.
                This    be|COND-would-have      obvious.

        (not providing any analysis for the initial and final words
        in the sample sentences)

        From the rudimentary analysis in (3), we might propose
        the following logical forms for the verbs
     {For what purpose?  Basic rationale not clear.}

        (4)     cond(isi(aux(olla)))
                cond(would-have(aux(be)))

        The forms in (4) could additionally be decorated with more
        intricate feature structures, and such a representation could
        be very practical, e.g. for the purpose of some specific
        text processing task.

     {Would this be more practical than Dick's `a word is a word is a word' 
     approach?  If so, how, and for which specific text processing task?  
     What is the evidence for this claim?  I would like to see some logical
     motivation for such `logical form' paraphrases.}

. . .
        However, we might feel that the verbal structures in
        sentences (1) and (2) would call for different logical
        forms, somewhat along the lines of (5) for the Finnish
        and (6) for the English.

        (5)     cond(isi(aux(olla)))

        (6)     aux(would), aux(have), aux(be)

     {Feelings are fine as a starter, but more than feelings are needed to
     justify an analysis and an alternative theory.  Precisely what do we
     gain by adding this `logical form' paraphrase to a monostratal 
     syntactic representation?}

. . .
        My questions are: 1) How can we avoid imposing differences (or
        similarities) between languages due more to our choice of
        formalism than to real evidence (and what would count as
        REAL EVIDENCE in this context). 
     {The representations should be as similar to and as different from each
     other as the languages themselves are.  Real evidence is relatively 
     easy to recognize when we confine ourselves to linear precedence and 
     word-word dependency relations, and much less so when we move away from
     the physical facts and intersubjectively verifiable native speaker 
     reactions.}

        2) Do we not have to assume
        the existence of non-surface level(s), if we want to assert
        that the 'function' or 'meaning' of some items in different
        languages are the 'same' or 'equivalent'.

        For example, a substition test would tell us that the
        Finnish suffix '-isi' in (1) have the same function as
        the English 'would have' in (2). But, since the actual
        morphemes themselves are different, our judgement of
        sameness must be guided by a mental comparison on a different
        level (i.e. a non-surface level). 
     {Why?  If lexical meaning can be represented in terms of features on
     lexical items, then I see no reason why `would have' might not share
     the same semantic features as the set of Finnish words affixed with
     `-isi'.}

     {Stan S.}

==
Comments on Petr Sgall's `function words and levels revisited'

I can only agree with Allain and Arne. I would not say that Dick's
analysis is wrong, but we need one the one hand something more complex
(to cope with the relationships between languages and between sound &
meaning)
     {For example?  If the question is whether some particular phenomenon 
     can or cannot be accounted for in a monostratal analysis, then the
     best way to answer it convincingly is to look as some of these 
     phenomena and see whether the monostratalists on the net can handle 
     them monostratally.  
     I think we need to be much more concrete in our
     discussions if we want to avoid vacuity.  Arne Larsson's Finnish data
     have set us a good example in this respect.  The people on this net
     know a lot of languages, and I personally would love to see more of
     them (languages, not people) scrolling down my screen.}

and on the other side something more economical (the relation
between a lexical verb and an auxiliary is not that of syntactic depend-
ency: 
     {I claim that it is *exactly* that of syntactic dependency.  Now we have
     two contrasting positions, and the next step in the debate is for Petr 
     and others to give us supporting evidence and arguments for this claim, 
     and for others of us to attempt to rebut it.}

no other complementations of the aux are possible; in this E. have
is similar to E. -ed, and Perfect to Preterite; 
     {This looks like evidence, but I'm afraid I don't follow it.}

with other kinds of
function words it is even clearer that no nodes in a dependency tree are
needed for them).
     {It's not clear to me.  I think in cases like this where some of us are
     coming to DG from different directions, we will need to make our 
     assumptions and reasoning more explicit.}

     {Stan S.}
From uclyrah@ucl.ac.uk Mon Aug  9 09:14:39 1993
Received: from mail-a.bcc.ac.uk by aisun1.ai.uga.edu with SMTP id AA18470
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Mon, 9 Aug 1993 03:14:48 -0400
Received: from link-1.ts.bcc.ac.uk by mail-a.bcc.ac.uk with SMTP (PP) 
          id <00359-0@mail-a.bcc.ac.uk>; Mon, 9 Aug 1993 08:14:42 +0100
Received: by link-1.ts.bcc.ac.uk (AIX 3.2/UCB 5.64/4.03) id AA74462;
          Mon, 9 Aug 1993 08:14:40 +0100
Message-Id: <9308090714.AA74462@link-1.ts.bcc.ac.uk>
To: dg@ai.uga.edu
Subject: French <au> and <du>
Date: Mon, 09 Aug 93 08:14:39 +0100
From: RichardHudson50 <uclyrah@ucl.ac.uk>


Following up the interesting discussion triggered by Petr Sgall, there's
a very easy way to handle these `portmanteau words', where a single 
spelling-word (and phonological word) corresponds to two syntactic ones.
Like And Rosta, I think <au village> consists of three syntactic nodes,
A + LE + VILLAGE, where the capital letters are meant to show that we are
talking about syntactic nodes plus lexical and inflectional analysis. If
we assume that each word is actually described by a collection of feature-
equations, one of these for VILLAGE will be "spelling of VILLAGE is 
<village>". Now we introduce into the grammar a spelling rule for the
sequence A + LE: "spelling of A + LE is <au>". By the general principles
of default inheritance, or of phonology, or any other system for handling
exceptions, this takes priority over the rules for just A and just LE,
because the sequence A + LE is more specific than either A or LE alone.
I've toyed with the idea of doing the same for English lexical suppletive
examples like THIS + DAY = TODAY. 

If this analysis is successful, it means that <au> etc are irrelevant to
the number of levels needed for syntax.


Dick Hudson
Dept of Phonetics and Linguistics,
University College London,
Gower Street,
London WC1E 6BT
(071) 387 7050 ext 3152

From uclyrah@ucl.ac.uk Mon Aug  9 10:06:27 1993
Received: from mail-a.bcc.ac.uk by aisun1.ai.uga.edu with SMTP id AA18610
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Mon, 9 Aug 1993 04:06:35 -0400
Received: from link-1.ts.bcc.ac.uk by mail-a.bcc.ac.uk with SMTP (PP) 
          id <00877-0@mail-a.bcc.ac.uk>; Mon, 9 Aug 1993 09:06:30 +0100
Received: by link-1.ts.bcc.ac.uk (AIX 3.2/UCB 5.64/4.03) id AA126484;
          Mon, 9 Aug 1993 09:06:27 +0100
Message-Id: <9308090806.AA126484@link-1.ts.bcc.ac.uk>
To: dg@ai.uga.edu
Subject: Levels
Date: Mon, 09 Aug 93 09:06:27 +0100
From: RichardHudson50 <uclyrah@ucl.ac.uk>


Re Alain Polguere on deep syntax, I'm still not convinced that
you need a level of deep syntax, in addition to surfac% syntax
and semanti#s. I agree that a one-step relation between the two
isn't necessarily simpler than a two-step one; taking an extra
step can in some circumstances simplify everything considerably.
But this has to be demonstrated, and the burden of proof lies on
those who recommend the extra level. So it's good to have some
specific examples (provided by Alain) to talk about. 

1. SPEAK/3 = x speak with y  has same meaning as
     or    = x and y speak

Alain says these differences show the need for two different deep
structures corresponding to one semantic structure, therefore his
deep structures can't also be semantic structures (contrary to
what I had suggested for his example "Joe jumped the gun"). Ok -
but why do you need deep structures when the surface structures
will make precisely the same distinction? (Incidentally, the
second valency of SPEAK/3 shouldn't mention coordination, but
plurality - a coordinated subject is just a particular case of a
subject with a set as referent, e.g. "they".)


2. "the red blackbird" vs "the blackbird is red". 

Alain says these have the same propositional meaning but
different deep syntax; so again deep syntax must be different
from meaning. But again I can't see why you need deep syntax in
addition to surface syntax, since these phrases will certainly be
distinguished in the surface. (Incidentally the examples must
surely also have different propositional meanings, though their
meanings have several features in common?)

******************************************************

By the way, I'm assuming that valency, as defined in the lexical
entry for a verb, is just an **extract** of surface structure -
e.g. it just mentions grammatical functions of dependents without
saying more about their word order, inflections etc. E.g. SPEAK/3
takes a subject and a dependent "with". 

I think this may be a source of misunderstanding among us. I
wonder if those who advocate a distinction between deep and
surface syntax are working with a model in which surface syntax
is by definition fully specified for all surface features?


Dick Hudson
Dept of Phonetics and Linguistics,
University College London,
Gower Street,
London WC1E 6BT
(071) 387 7050 ext 3152

From ucleaar@ucl.ac.uk Tue Aug 10 01:36:53 1993
Received: from mail-a.bcc.ac.uk by aisun1.ai.uga.edu with SMTP id AA24360
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Mon, 9 Aug 1993 19:44:28 -0400
Received: from link-1.ts.bcc.ac.uk by mail-a.bcc.ac.uk with SMTP (PP) 
          id <17323-0@mail-a.bcc.ac.uk>; Tue, 10 Aug 1993 00:36:55 +0100
Received: by link-1.ts.bcc.ac.uk (AIX 3.2/UCB 5.64/4.03) id AA89830;
          Tue, 10 Aug 1993 00:36:53 +0100
From: ucleaar@ucl.ac.uk (Mr Andrew Rosta)
Message-Id: <9308092336.AA89830@link-1.ts.bcc.ac.uk>
To: dg@ai.uga.edu
Subject: Re: Levels
In-Reply-To: (Your message of Mon, 09 Aug 93 09:06:27 N.) <9308090806.AA126484@link-1.ts.bcc.ac.uk>
Date: Tue, 10 Aug 93 00:36:53 +0100


Dick Hudson says:
> 2. "the red blackbird" vs "the blackbird is red". 
> 
> Alain says these have the same propositional meaning but
> different deep syntax; so again deep syntax must be different
> from meaning. But again I can't see why you need deep syntax in
> addition to surface syntax, since these phrases will certainly be
> distinguished in the surface. (Incidentally the examples must
> surely also have different propositional meanings, though their
> meanings have several features in common?)

I agree that the examples have different meanings:
   red(the blackbird) [at least assuming non-restrictive modification]
   is(red(the blackbird))
But I cannot see that the semantic relation between the blackbird
& the red is any different in the 2 examples.

----
And Rosta
From ucleaar@ucl.ac.uk Tue Aug 10 01:50:31 1993
Received: from mail-a.bcc.ac.uk by aisun1.ai.uga.edu with SMTP id AA24383
  (5.65c/IDA-1.5 for <dg@ai.uga.edu>); Mon, 9 Aug 1993 19:52:28 -0400
Received: from link-1.ts.bcc.ac.uk by mail-a.bcc.ac.uk with SMTP (PP) 
          id <17408-0@mail-a.bcc.ac.uk>; Tue, 10 Aug 1993 00:50:34 +0100
Received: by link-1.ts.bcc.ac.uk (AIX 3.2/UCB 5.64/4.03) id AA108170;
          Tue, 10 Aug 1993 00:50:32 +0100
From: ucleaar@ucl.ac.uk (Mr Andrew Rosta)
Message-Id: <9308092350.AA108170@link-1.ts.bcc.ac.uk>
To: dg@ai.uga.edu, RichardHudson50 <uclyrah@ucl.ac.uk>
Subject: Re: French <au> and <du>
In-Reply-To: (Your message of Mon, 09 Aug 93 08:14:39 N.) <9308090714.AA74462@link-1.ts.bcc.ac.uk>
Date: Tue, 10 Aug 93 00:50:31 +0100


Dick Hudson writes: 
> Following up the interesting discussion triggered by Petr Sgall, there's
> a very easy way to handle these `portmanteau words', where a single 
> spelling-word (and phonological word) corresponds to two syntactic ones.
> Like And Rosta, I think <au village> consists of three syntactic nodes,
> A + LE + VILLAGE, where the capital letters are meant to show that we are
> talking about syntactic nodes plus lexical and inflectional analysis. If
> we assume that each word is actually described by a collection of feature-
> equations, one of these for VILLAGE will be "spelling of VILLAGE is 
> <village>". Now we introduce into the grammar a spelling rule for the
> sequence A + LE: "spelling of A + LE is <au>". By the general principles
> of default inheritance, or of phonology, or any other system for handling
> exceptions, this takes priority over the rules for just A and just LE,
> because the sequence A + LE is more specific than either A or LE alone.

For "A + LE" to be an exception, we would need a category "linear
sequence of two words". Then "A + LE" would be an exceptional
member because its phonology is not a mere concatenation of its
parts. But it is preferable to avoid such an alien category, &
instead use an exceptional instance of A or LE (or of both) as
the means to state the A + LE = <au> rule.

> I've toyed with the idea of doing the same for English lexical suppletive
> examples like THIS + DAY = TODAY. 

If _today_ suppletes _this day_, it is only when _this day_
is adverbial & refers to today. Even then, is it actually 
*ungrammatical* to say "This day I shall visit the museum"?
Is this not just a matter of usage? - We all naturally
conform to the prevailing usage patterns, & in this case 
_today_ prevails.

----
And Rosta
